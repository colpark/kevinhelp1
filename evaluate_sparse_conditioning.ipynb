{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Conditioning Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the sparse conditioning capabilities of a trained PixNerDiT model.\n",
    "\n",
    "**Important Note**: If the model was trained with the buggy code (where `cond_mask` was ignored),\n",
    "sparse conditioning will have limited effectiveness. The model needs to be retrained with the\n",
    "fixed code to properly learn sparse pixel conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration - UPDATE THESE PATHS\n",
    "CHECKPOINT_PATH = \"/home/idies/workspace/Temporary/dpark1/scratch/kevinhelp1/workdirs/exp_sfc_both/checkpoints/last-v1.ckpt\"\n",
    "# Alternative for local testing:\n",
    "# CHECKPOINT_PATH = \"./workdirs/exp_sfc_both/checkpoints/last-v1.ckpt\"\n",
    "\n",
    "# Add project to path\n",
    "PROJECT_ROOT = Path(\".\").absolute()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from functools import partial\n",
    "\n",
    "# Disable torch.compile for compatibility\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "# Set precision for tensor cores\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model components\n",
    "from src.models.autoencoder.pixel import PixelAE\n",
    "from src.models.conditioner.class_label import LabelConditioner\n",
    "from src.models.transformer.pixnerd_c2i_heavydecoder import PixNerDiT\n",
    "from src.diffusion.flow_matching.scheduling import LinearScheduler\n",
    "from src.diffusion.flow_matching.sampling import EulerSampler, ode_step_fn\n",
    "from src.diffusion.base.guidance import simple_guidance_fn\n",
    "from src.diffusion.flow_matching.training import FlowMatchingTrainer\n",
    "from src.callbacks.simple_ema import SimpleEMA\n",
    "from src.lightning_model import LightningModel\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Configuration\n",
    "\n",
    "These settings should match the training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (must match training config)\n",
    "MODEL_CONFIG = {\n",
    "    \"in_channels\": 3,\n",
    "    \"patch_size\": 4,\n",
    "    \"num_groups\": 4,\n",
    "    \"hidden_size\": 256,\n",
    "    \"decoder_hidden_size\": 64,\n",
    "    \"num_encoder_blocks\": 4,\n",
    "    \"num_decoder_blocks\": 2,\n",
    "    \"num_classes\": 10,\n",
    "    \"encoder_type\": \"sfc\",\n",
    "    # SFC settings\n",
    "    \"sfc_curve\": \"hilbert\",\n",
    "    \"sfc_group_size\": 8,\n",
    "    \"sfc_cross_depth\": 2,\n",
    "    # Ablation flags (both enabled for exp_sfc_both)\n",
    "    \"sfc_unified_coords\": True,\n",
    "    \"sfc_spatial_bias\": True,\n",
    "}\n",
    "\n",
    "# Sampling configuration\n",
    "SAMPLING_CONFIG = {\n",
    "    \"num_steps\": 50,  # Fewer steps for faster evaluation\n",
    "    \"guidance\": 2.0,\n",
    "}\n",
    "\n",
    "# Sparsity configuration\n",
    "SPARSITY = 0.4  # 40% of pixels observed\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"Encoder type: {MODEL_CONFIG['encoder_type']}\")\n",
    "print(f\"SFC unified coords: {MODEL_CONFIG['sfc_unified_coords']}\")\n",
    "print(f\"SFC spatial bias: {MODEL_CONFIG['sfc_spatial_bias']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model and Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, sampling_config):\n",
    "    \"\"\"Build the model architecture.\"\"\"\n",
    "    main_scheduler = LinearScheduler()\n",
    "    \n",
    "    vae = PixelAE(scale=1.0)\n",
    "    conditioner = LabelConditioner(num_classes=config[\"num_classes\"])\n",
    "    \n",
    "    denoiser = PixNerDiT(\n",
    "        in_channels=config[\"in_channels\"],\n",
    "        patch_size=config[\"patch_size\"],\n",
    "        num_groups=config[\"num_groups\"],\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        decoder_hidden_size=config[\"decoder_hidden_size\"],\n",
    "        num_encoder_blocks=config[\"num_encoder_blocks\"],\n",
    "        num_decoder_blocks=config[\"num_decoder_blocks\"],\n",
    "        num_classes=config[\"num_classes\"],\n",
    "        encoder_type=config[\"encoder_type\"],\n",
    "        sfc_curve=config[\"sfc_curve\"],\n",
    "        sfc_group_size=config[\"sfc_group_size\"],\n",
    "        sfc_cross_depth=config[\"sfc_cross_depth\"],\n",
    "        sfc_unified_coords=config[\"sfc_unified_coords\"],\n",
    "        sfc_spatial_bias=config[\"sfc_spatial_bias\"],\n",
    "    )\n",
    "    \n",
    "    sampler = EulerSampler(\n",
    "        num_steps=sampling_config[\"num_steps\"],\n",
    "        guidance=sampling_config[\"guidance\"],\n",
    "        guidance_interval_min=0.0,\n",
    "        guidance_interval_max=1.0,\n",
    "        scheduler=main_scheduler,\n",
    "        w_scheduler=LinearScheduler(),\n",
    "        guidance_fn=simple_guidance_fn,\n",
    "        step_fn=ode_step_fn,\n",
    "    )\n",
    "    \n",
    "    trainer = FlowMatchingTrainer(\n",
    "        scheduler=main_scheduler,\n",
    "        lognorm_t=True,\n",
    "        timeshift=1.0,\n",
    "    )\n",
    "    \n",
    "    ema_tracker = SimpleEMA(decay=0.9999)\n",
    "    optimizer = partial(torch.optim.AdamW, lr=1e-4, weight_decay=0.0)\n",
    "    \n",
    "    model = LightningModel(\n",
    "        vae=vae,\n",
    "        conditioner=conditioner,\n",
    "        denoiser=denoiser,\n",
    "        diffusion_trainer=trainer,\n",
    "        diffusion_sampler=sampler,\n",
    "        ema_tracker=ema_tracker,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=None,\n",
    "        eval_original_model=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_model(MODEL_CONFIG, SAMPLING_CONFIG)\n",
    "print(f\"Model built. Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "print(f\"Loading checkpoint from: {CHECKPOINT_PATH}\")\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {CHECKPOINT_PATH}\")\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "print(f\"Checkpoint keys: {checkpoint.keys()}\")\n",
    "\n",
    "# Load state dict\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "print(f\"\\nMissing keys: {len(missing)}\")\n",
    "if missing:\n",
    "    print(f\"  Examples: {missing[:5]}\")\n",
    "print(f\"Unexpected keys: {len(unexpected)}\")\n",
    "if unexpected:\n",
    "    print(f\"  Examples: {unexpected[:5]}\")\n",
    "\n",
    "# Get training step info if available\n",
    "if \"global_step\" in checkpoint:\n",
    "    print(f\"\\nCheckpoint from step: {checkpoint['global_step']}\")\n",
    "if \"epoch\" in checkpoint:\n",
    "    print(f\"Checkpoint from epoch: {checkpoint['epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPU and set to eval mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Convert EMA denoiser to float32 for inference\n",
    "model.ema_denoiser.to(torch.float32)\n",
    "\n",
    "print(f\"Model moved to: {device}\")\n",
    "print(\"Model set to eval mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 class names\n",
    "CIFAR10_CLASSES = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Load test dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "test_dataset = CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_batch(dataset, indices, device):\n",
    "    \"\"\"Get a batch of test images.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for idx in indices:\n",
    "        img, label = dataset[idx]\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    \n",
    "    images = torch.stack(images).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "    return images, labels\n",
    "\n",
    "# Get a batch of test images (one per class)\n",
    "test_indices = [i * 1000 for i in range(10)]  # Spread across dataset\n",
    "test_images, test_labels = get_test_batch(test_dataset, test_indices, device)\n",
    "\n",
    "print(f\"Test batch shape: {test_images.shape}\")\n",
    "print(f\"Test labels: {[CIFAR10_CLASSES[l] for l in test_labels.tolist()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sparse_mask(x, sparsity=0.4):\n",
    "    \"\"\"\n",
    "    Generate a sparse conditioning mask.\n",
    "    \n",
    "    Args:\n",
    "        x: (B, C, H, W) input tensor\n",
    "        sparsity: fraction of pixels to observe\n",
    "    \n",
    "    Returns:\n",
    "        cond_mask: (B, 1, H, W) binary mask where 1 = observed pixel\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    device = x.device\n",
    "    \n",
    "    total_keep = int(sparsity * H * W)\n",
    "    cond_mask = torch.zeros(B, 1, H, W, device=device)\n",
    "    \n",
    "    for b in range(B):\n",
    "        indices = torch.randperm(H * W, device=device)[:total_keep]\n",
    "        mask_flat = torch.zeros(H * W, device=device)\n",
    "        mask_flat[indices] = 1.0\n",
    "        cond_mask[b, 0] = mask_flat.view(H, W)\n",
    "    \n",
    "    return cond_mask\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\"Convert tensor to displayable image.\"\"\"\n",
    "    img = tensor.detach().cpu()\n",
    "    img = (img.clamp(-1, 1) + 1) / 2  # [-1, 1] -> [0, 1]\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    return img\n",
    "\n",
    "def visualize_batch(images, titles=None, figsize=(15, 3)):\n",
    "    \"\"\"Visualize a batch of images.\"\"\"\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (img, ax) in enumerate(zip(images, axes)):\n",
    "        ax.imshow(tensor_to_image(img))\n",
    "        ax.axis(\"off\")\n",
    "        if titles:\n",
    "            ax.set_title(titles[i], fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Class-Conditional Generation (Baseline)\n",
    "\n",
    "First, test that the model can generate class-conditional images without sparse hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_class_conditional(model, labels, num_samples=None):\n",
    "    \"\"\"\n",
    "    Generate images conditioned only on class labels.\n",
    "    \"\"\"\n",
    "    if num_samples is None:\n",
    "        num_samples = len(labels)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Get conditioning\n",
    "    condition, uncondition = model.conditioner(labels)\n",
    "    \n",
    "    # Generate noise\n",
    "    noise = torch.randn(num_samples, 3, 32, 32, device=device)\n",
    "    \n",
    "    # Sample\n",
    "    samples = model.diffusion_sampler(\n",
    "        model.ema_denoiser,\n",
    "        noise,\n",
    "        condition,\n",
    "        uncondition,\n",
    "    )\n",
    "    \n",
    "    # Handle tuple return\n",
    "    if isinstance(samples, tuple):\n",
    "        samples = samples[0][-1] if isinstance(samples[0], list) else samples[0]\n",
    "    \n",
    "    # Decode (PixelAE is identity)\n",
    "    samples = model.vae.decode(samples)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "print(\"Generating class-conditional samples...\")\n",
    "class_samples = generate_class_conditional(model, test_labels)\n",
    "print(f\"Generated {len(class_samples)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class-conditional samples\n",
    "titles = [f\"{CIFAR10_CLASSES[l]}\" for l in test_labels.tolist()]\n",
    "fig = visualize_batch(class_samples, titles, figsize=(20, 2.5))\n",
    "plt.suptitle(\"Class-Conditional Generation (No Sparse Hints)\", fontsize=14, y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sparse-Conditioned Generation\n",
    "\n",
    "Now test generation with sparse pixel hints.\n",
    "\n",
    "**Note**: If the model was trained with the buggy code, sparse conditioning will have limited effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sparse_conditioned(model, images, labels, sparsity=0.4):\n",
    "    \"\"\"\n",
    "    Generate images conditioned on class labels AND sparse pixel hints.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    B = len(images)\n",
    "    \n",
    "    # Encode images to latent space\n",
    "    x_latent = model.vae.encode(images)\n",
    "    \n",
    "    # Generate sparse mask\n",
    "    cond_mask = generate_sparse_mask(x_latent, sparsity=sparsity)\n",
    "    \n",
    "    # Get conditioning\n",
    "    condition, uncondition = model.conditioner(labels)\n",
    "    \n",
    "    # Generate noise\n",
    "    noise = torch.randn_like(x_latent)\n",
    "    \n",
    "    # Sample with sparse conditioning\n",
    "    samples = model.diffusion_sampler(\n",
    "        model.ema_denoiser,\n",
    "        noise,\n",
    "        condition,\n",
    "        uncondition,\n",
    "        cond_mask=cond_mask,\n",
    "        x_cond=x_latent,\n",
    "    )\n",
    "    \n",
    "    # Handle tuple return\n",
    "    if isinstance(samples, tuple):\n",
    "        samples = samples[0][-1] if isinstance(samples[0], list) else samples[0]\n",
    "    \n",
    "    # Decode\n",
    "    samples = model.vae.decode(samples)\n",
    "    \n",
    "    # Create sparse input visualization\n",
    "    sparse_input = images * cond_mask\n",
    "    \n",
    "    return samples, sparse_input, cond_mask\n",
    "\n",
    "print(f\"Generating sparse-conditioned samples (sparsity={SPARSITY})...\")\n",
    "sparse_samples, sparse_inputs, masks = generate_sparse_conditioned(\n",
    "    model, test_images, test_labels, sparsity=SPARSITY\n",
    ")\n",
    "print(f\"Generated {len(sparse_samples)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison: Ground Truth vs Sparse Input vs Sparse-Conditioned Output\n",
    "n_show = min(5, len(test_images))\n",
    "\n",
    "fig, axes = plt.subplots(3, n_show, figsize=(3*n_show, 9))\n",
    "\n",
    "for i in range(n_show):\n",
    "    # Ground truth\n",
    "    axes[0, i].imshow(tensor_to_image(test_images[i]))\n",
    "    axes[0, i].set_title(f\"GT: {CIFAR10_CLASSES[test_labels[i]]}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    \n",
    "    # Sparse input\n",
    "    axes[1, i].imshow(tensor_to_image(sparse_inputs[i]))\n",
    "    axes[1, i].set_title(f\"Sparse ({SPARSITY*100:.0f}%)\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "    \n",
    "    # Sparse-conditioned output\n",
    "    axes[2, i].imshow(tensor_to_image(sparse_samples[i]))\n",
    "    axes[2, i].set_title(\"Reconstructed\")\n",
    "    axes[2, i].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Ground Truth\", fontsize=12)\n",
    "axes[1, 0].set_ylabel(\"Sparse Input\", fontsize=12)\n",
    "axes[2, 0].set_ylabel(\"Reconstruction\", fontsize=12)\n",
    "\n",
    "plt.suptitle(\"Sparse-Conditioned Reconstruction\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison: Class-Only vs Sparse-Conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate class-only samples with same noise for fair comparison\n",
    "torch.manual_seed(42)\n",
    "noise = torch.randn(len(test_images), 3, 32, 32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_latent = model.vae.encode(test_images)\n",
    "    cond_mask = generate_sparse_mask(x_latent, sparsity=SPARSITY)\n",
    "    condition, uncondition = model.conditioner(test_labels)\n",
    "    \n",
    "    # Class-only (same noise)\n",
    "    torch.manual_seed(42)\n",
    "    noise = torch.randn_like(x_latent)\n",
    "    class_only = model.diffusion_sampler(\n",
    "        model.ema_denoiser, noise, condition, uncondition\n",
    "    )\n",
    "    if isinstance(class_only, tuple):\n",
    "        class_only = class_only[0][-1] if isinstance(class_only[0], list) else class_only[0]\n",
    "    class_only = model.vae.decode(class_only)\n",
    "    \n",
    "    # Sparse-conditioned (same noise)\n",
    "    torch.manual_seed(42)\n",
    "    noise = torch.randn_like(x_latent)\n",
    "    sparse_cond = model.diffusion_sampler(\n",
    "        model.ema_denoiser, noise, condition, uncondition,\n",
    "        cond_mask=cond_mask, x_cond=x_latent\n",
    "    )\n",
    "    if isinstance(sparse_cond, tuple):\n",
    "        sparse_cond = sparse_cond[0][-1] if isinstance(sparse_cond[0], list) else sparse_cond[0]\n",
    "    sparse_cond = model.vae.decode(sparse_cond)\n",
    "    \n",
    "    sparse_input_vis = test_images * cond_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 4-way comparison\n",
    "n_show = min(5, len(test_images))\n",
    "\n",
    "fig, axes = plt.subplots(4, n_show, figsize=(3*n_show, 12))\n",
    "\n",
    "for i in range(n_show):\n",
    "    # Ground truth\n",
    "    axes[0, i].imshow(tensor_to_image(test_images[i]))\n",
    "    axes[0, i].set_title(f\"{CIFAR10_CLASSES[test_labels[i]]}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    \n",
    "    # Sparse input\n",
    "    axes[1, i].imshow(tensor_to_image(sparse_input_vis[i]))\n",
    "    axes[1, i].axis(\"off\")\n",
    "    \n",
    "    # Class-only\n",
    "    axes[2, i].imshow(tensor_to_image(class_only[i]))\n",
    "    axes[2, i].axis(\"off\")\n",
    "    \n",
    "    # Sparse-conditioned\n",
    "    axes[3, i].imshow(tensor_to_image(sparse_cond[i]))\n",
    "    axes[3, i].axis(\"off\")\n",
    "\n",
    "row_labels = [\"Ground Truth\", f\"Sparse Input ({SPARSITY*100:.0f}%)\", \n",
    "              \"Class-Only\", \"Sparse-Conditioned\"]\n",
    "for i, label in enumerate(row_labels):\n",
    "    axes[i, 0].set_ylabel(label, fontsize=11)\n",
    "\n",
    "plt.suptitle(\"Comparison: Class-Only vs Sparse-Conditioned Generation\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quantitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(gt, pred, mask=None):\n",
    "    \"\"\"\n",
    "    Compute reconstruction metrics.\n",
    "    \n",
    "    Args:\n",
    "        gt: Ground truth images (B, C, H, W)\n",
    "        pred: Predicted images (B, C, H, W)\n",
    "        mask: Optional mask (B, 1, H, W) - metrics computed at mask=1 locations\n",
    "    \n",
    "    Returns:\n",
    "        dict with MSE, PSNR\n",
    "    \"\"\"\n",
    "    # Normalize to [0, 1]\n",
    "    gt = (gt.clamp(-1, 1) + 1) / 2\n",
    "    pred = (pred.clamp(-1, 1) + 1) / 2\n",
    "    \n",
    "    if mask is not None:\n",
    "        # Only compute at masked locations\n",
    "        mask = mask.expand_as(gt)\n",
    "        n_pixels = mask.sum()\n",
    "        mse = ((gt - pred) ** 2 * mask).sum() / n_pixels\n",
    "    else:\n",
    "        mse = ((gt - pred) ** 2).mean()\n",
    "    \n",
    "    psnr = 10 * torch.log10(1.0 / mse)\n",
    "    \n",
    "    return {\n",
    "        \"mse\": mse.item(),\n",
    "        \"psnr\": psnr.item(),\n",
    "    }\n",
    "\n",
    "# Compute metrics\n",
    "metrics_class_only = compute_metrics(test_images, class_only)\n",
    "metrics_sparse_full = compute_metrics(test_images, sparse_cond)\n",
    "metrics_sparse_masked = compute_metrics(test_images, sparse_cond, cond_mask)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Reconstruction Metrics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nClass-Only Generation (no hints):\")\n",
    "print(f\"  MSE:  {metrics_class_only['mse']:.6f}\")\n",
    "print(f\"  PSNR: {metrics_class_only['psnr']:.2f} dB\")\n",
    "\n",
    "print(f\"\\nSparse-Conditioned (full image):\")\n",
    "print(f\"  MSE:  {metrics_sparse_full['mse']:.6f}\")\n",
    "print(f\"  PSNR: {metrics_sparse_full['psnr']:.2f} dB\")\n",
    "\n",
    "print(f\"\\nSparse-Conditioned (at hint locations only):\")\n",
    "print(f\"  MSE:  {metrics_sparse_masked['mse']:.6f}\")\n",
    "print(f\"  PSNR: {metrics_sparse_masked['psnr']:.2f} dB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if metrics_sparse_masked['psnr'] > 30:\n",
    "    print(\"Hint fidelity is HIGH - sparse conditioning is working!\")\n",
    "elif metrics_sparse_masked['psnr'] > 20:\n",
    "    print(\"Hint fidelity is MODERATE - sparse conditioning partially working.\")\n",
    "else:\n",
    "    print(\"Hint fidelity is LOW - model may need retraining with fixed code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Different Sparsity Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_levels = [0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "results = []\n",
    "\n",
    "print(\"Testing different sparsity levels...\")\n",
    "for sparsity in sparsity_levels:\n",
    "    with torch.no_grad():\n",
    "        x_latent = model.vae.encode(test_images)\n",
    "        cond_mask = generate_sparse_mask(x_latent, sparsity=sparsity)\n",
    "        condition, uncondition = model.conditioner(test_labels)\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "        noise = torch.randn_like(x_latent)\n",
    "        samples = model.diffusion_sampler(\n",
    "            model.ema_denoiser, noise, condition, uncondition,\n",
    "            cond_mask=cond_mask, x_cond=x_latent\n",
    "        )\n",
    "        if isinstance(samples, tuple):\n",
    "            samples = samples[0][-1] if isinstance(samples[0], list) else samples[0]\n",
    "        samples = model.vae.decode(samples)\n",
    "        \n",
    "        metrics = compute_metrics(test_images, samples)\n",
    "        metrics_hints = compute_metrics(test_images, samples, cond_mask)\n",
    "        \n",
    "        results.append({\n",
    "            \"sparsity\": sparsity,\n",
    "            \"psnr_full\": metrics[\"psnr\"],\n",
    "            \"psnr_hints\": metrics_hints[\"psnr\"],\n",
    "        })\n",
    "        print(f\"  Sparsity {sparsity*100:.0f}%: Full PSNR={metrics['psnr']:.2f}, Hint PSNR={metrics_hints['psnr']:.2f}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PSNR vs Sparsity\n",
    "sparsities = [r[\"sparsity\"] * 100 for r in results]\n",
    "psnr_full = [r[\"psnr_full\"] for r in results]\n",
    "psnr_hints = [r[\"psnr_hints\"] for r in results]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(sparsities, psnr_full, 'b-o', label='Full Image PSNR', linewidth=2, markersize=8)\n",
    "ax.plot(sparsities, psnr_hints, 'r-s', label='Hint Locations PSNR', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Sparsity (%)', fontsize=12)\n",
    "ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "ax.set_title('Reconstruction Quality vs Sparsity Level', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "### Interpretation Guide:\n",
    "\n",
    "**If the model was trained with the FIXED code:**\n",
    "- Sparse-conditioned samples should closely match ground truth at hint locations\n",
    "- Hint PSNR should be high (>30 dB)\n",
    "- Full image PSNR should improve with more hints\n",
    "\n",
    "**If the model was trained with the BUGGY code:**\n",
    "- Sparse conditioning will have minimal effect\n",
    "- Class-only and sparse-conditioned will look similar\n",
    "- Hint PSNR will be low\n",
    "- **Solution**: Retrain with the fixed code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCheckpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"Encoder type: {MODEL_CONFIG['encoder_type']}\")\n",
    "print(f\"SFC unified coords: {MODEL_CONFIG['sfc_unified_coords']}\")\n",
    "print(f\"SFC spatial bias: {MODEL_CONFIG['sfc_spatial_bias']}\")\n",
    "print(f\"\\nTest sparsity: {SPARSITY*100:.0f}%\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Class-only PSNR: {metrics_class_only['psnr']:.2f} dB\")\n",
    "print(f\"  Sparse-cond PSNR (full): {metrics_sparse_full['psnr']:.2f} dB\")\n",
    "print(f\"  Sparse-cond PSNR (hints): {metrics_sparse_masked['psnr']:.2f} dB\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "improvement = metrics_sparse_full['psnr'] - metrics_class_only['psnr']\n",
    "if improvement > 3:\n",
    "    print(f\"Sparse conditioning provides +{improvement:.1f} dB improvement!\")\n",
    "    print(\"The model appears to have learned sparse conditioning.\")\n",
    "elif improvement > 0:\n",
    "    print(f\"Sparse conditioning provides modest +{improvement:.1f} dB improvement.\")\n",
    "    print(\"Consider retraining with the fixed code for better results.\")\n",
    "else:\n",
    "    print(f\"Sparse conditioning shows no improvement ({improvement:.1f} dB).\")\n",
    "    print(\"The model was likely trained with buggy code - RETRAIN REQUIRED.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
