{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Sparse Conditioning Evaluation Notebook\n\nThis notebook evaluates a PixNerDiT model trained with **sparse pixel conditioning**.\n\n## Training Setup Recap\n- **40% of pixels** are randomly observed during training (`sparsity=0.4`)\n- The model receives these sparse pixel hints via `cond_mask` and `x_cond`\n- The SFC encoder builds tokens from these sparse observations\n\n## Expected Behavior\nSince the model was trained WITH sparse conditioning:\n- **Class-only generation (no hints)**: Will NOT work well - the model expects sparse pixel hints\n- **Sparse-conditioned generation**: Should work well - this is what the model was trained for\n\nThe model learned to reconstruct full images from sparse observations, not to generate from scratch."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration - UPDATE THESE PATHS\n",
    "CHECKPOINT_PATH = \"/home/idies/workspace/Temporary/dpark1/scratch/kevinhelp1/workdirs/exp_sfc_both/checkpoints/last-v1.ckpt\"\n",
    "# Alternative for local testing:\n",
    "# CHECKPOINT_PATH = \"./workdirs/exp_sfc_both/checkpoints/last-v1.ckpt\"\n",
    "\n",
    "# Add project to path\n",
    "PROJECT_ROOT = Path(\".\").absolute()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from functools import partial\n",
    "\n",
    "# Disable torch.compile for compatibility\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "# Set precision for tensor cores\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model components\n",
    "from src.models.autoencoder.pixel import PixelAE\n",
    "from src.models.conditioner.class_label import LabelConditioner\n",
    "from src.models.transformer.pixnerd_c2i_heavydecoder import PixNerDiT\n",
    "from src.diffusion.flow_matching.scheduling import LinearScheduler\n",
    "from src.diffusion.flow_matching.sampling import EulerSampler, ode_step_fn\n",
    "from src.diffusion.base.guidance import simple_guidance_fn\n",
    "from src.diffusion.flow_matching.training import FlowMatchingTrainer\n",
    "from src.callbacks.simple_ema import SimpleEMA\n",
    "from src.lightning_model import LightningModel\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Configuration\n",
    "\n",
    "These settings should match the training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (must match training config)\n",
    "MODEL_CONFIG = {\n",
    "    \"in_channels\": 3,\n",
    "    \"patch_size\": 4,\n",
    "    \"num_groups\": 4,\n",
    "    \"hidden_size\": 256,\n",
    "    \"decoder_hidden_size\": 64,\n",
    "    \"num_encoder_blocks\": 4,\n",
    "    \"num_decoder_blocks\": 2,\n",
    "    \"num_classes\": 10,\n",
    "    \"encoder_type\": \"sfc\",\n",
    "    # SFC settings\n",
    "    \"sfc_curve\": \"hilbert\",\n",
    "    \"sfc_group_size\": 8,\n",
    "    \"sfc_cross_depth\": 2,\n",
    "    # Ablation flags (both enabled for exp_sfc_both)\n",
    "    \"sfc_unified_coords\": True,\n",
    "    \"sfc_spatial_bias\": True,\n",
    "}\n",
    "\n",
    "# Sampling configuration\n",
    "SAMPLING_CONFIG = {\n",
    "    \"num_steps\": 50,  # Fewer steps for faster evaluation\n",
    "    \"guidance\": 2.0,\n",
    "}\n",
    "\n",
    "# Sparsity configuration\n",
    "SPARSITY = 0.4  # 40% of pixels observed\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"Encoder type: {MODEL_CONFIG['encoder_type']}\")\n",
    "print(f\"SFC unified coords: {MODEL_CONFIG['sfc_unified_coords']}\")\n",
    "print(f\"SFC spatial bias: {MODEL_CONFIG['sfc_spatial_bias']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model and Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, sampling_config):\n",
    "    \"\"\"Build the model architecture.\"\"\"\n",
    "    main_scheduler = LinearScheduler()\n",
    "    \n",
    "    vae = PixelAE(scale=1.0)\n",
    "    conditioner = LabelConditioner(num_classes=config[\"num_classes\"])\n",
    "    \n",
    "    denoiser = PixNerDiT(\n",
    "        in_channels=config[\"in_channels\"],\n",
    "        patch_size=config[\"patch_size\"],\n",
    "        num_groups=config[\"num_groups\"],\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        decoder_hidden_size=config[\"decoder_hidden_size\"],\n",
    "        num_encoder_blocks=config[\"num_encoder_blocks\"],\n",
    "        num_decoder_blocks=config[\"num_decoder_blocks\"],\n",
    "        num_classes=config[\"num_classes\"],\n",
    "        encoder_type=config[\"encoder_type\"],\n",
    "        sfc_curve=config[\"sfc_curve\"],\n",
    "        sfc_group_size=config[\"sfc_group_size\"],\n",
    "        sfc_cross_depth=config[\"sfc_cross_depth\"],\n",
    "        sfc_unified_coords=config[\"sfc_unified_coords\"],\n",
    "        sfc_spatial_bias=config[\"sfc_spatial_bias\"],\n",
    "    )\n",
    "    \n",
    "    sampler = EulerSampler(\n",
    "        num_steps=sampling_config[\"num_steps\"],\n",
    "        guidance=sampling_config[\"guidance\"],\n",
    "        guidance_interval_min=0.0,\n",
    "        guidance_interval_max=1.0,\n",
    "        scheduler=main_scheduler,\n",
    "        w_scheduler=LinearScheduler(),\n",
    "        guidance_fn=simple_guidance_fn,\n",
    "        step_fn=ode_step_fn,\n",
    "    )\n",
    "    \n",
    "    trainer = FlowMatchingTrainer(\n",
    "        scheduler=main_scheduler,\n",
    "        lognorm_t=True,\n",
    "        timeshift=1.0,\n",
    "    )\n",
    "    \n",
    "    ema_tracker = SimpleEMA(decay=0.9999)\n",
    "    optimizer = partial(torch.optim.AdamW, lr=1e-4, weight_decay=0.0)\n",
    "    \n",
    "    model = LightningModel(\n",
    "        vae=vae,\n",
    "        conditioner=conditioner,\n",
    "        denoiser=denoiser,\n",
    "        diffusion_trainer=trainer,\n",
    "        diffusion_sampler=sampler,\n",
    "        ema_tracker=ema_tracker,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=None,\n",
    "        eval_original_model=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_model(MODEL_CONFIG, SAMPLING_CONFIG)\n",
    "print(f\"Model built. Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "print(f\"Loading checkpoint from: {CHECKPOINT_PATH}\")\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {CHECKPOINT_PATH}\")\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "print(f\"Checkpoint keys: {checkpoint.keys()}\")\n",
    "\n",
    "# Load state dict\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "print(f\"\\nMissing keys: {len(missing)}\")\n",
    "if missing:\n",
    "    print(f\"  Examples: {missing[:5]}\")\n",
    "print(f\"Unexpected keys: {len(unexpected)}\")\n",
    "if unexpected:\n",
    "    print(f\"  Examples: {unexpected[:5]}\")\n",
    "\n",
    "# Get training step info if available\n",
    "if \"global_step\" in checkpoint:\n",
    "    print(f\"\\nCheckpoint from step: {checkpoint['global_step']}\")\n",
    "if \"epoch\" in checkpoint:\n",
    "    print(f\"Checkpoint from epoch: {checkpoint['epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPU and set to eval mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Convert EMA denoiser to float32 for inference\n",
    "model.ema_denoiser.to(torch.float32)\n",
    "\n",
    "print(f\"Model moved to: {device}\")\n",
    "print(\"Model set to eval mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 class names\n",
    "CIFAR10_CLASSES = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Load test dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "test_dataset = CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_batch(dataset, indices, device):\n",
    "    \"\"\"Get a batch of test images.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for idx in indices:\n",
    "        img, label = dataset[idx]\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    \n",
    "    images = torch.stack(images).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "    return images, labels\n",
    "\n",
    "# Get a batch of test images (one per class)\n",
    "test_indices = [i * 1000 for i in range(10)]  # Spread across dataset\n",
    "test_images, test_labels = get_test_batch(test_dataset, test_indices, device)\n",
    "\n",
    "print(f\"Test batch shape: {test_images.shape}\")\n",
    "print(f\"Test labels: {[CIFAR10_CLASSES[l] for l in test_labels.tolist()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sparse_mask(x, sparsity=0.4):\n",
    "    \"\"\"\n",
    "    Generate a sparse conditioning mask.\n",
    "    \n",
    "    Args:\n",
    "        x: (B, C, H, W) input tensor\n",
    "        sparsity: fraction of pixels to observe\n",
    "    \n",
    "    Returns:\n",
    "        cond_mask: (B, 1, H, W) binary mask where 1 = observed pixel\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    device = x.device\n",
    "    \n",
    "    total_keep = int(sparsity * H * W)\n",
    "    cond_mask = torch.zeros(B, 1, H, W, device=device)\n",
    "    \n",
    "    for b in range(B):\n",
    "        indices = torch.randperm(H * W, device=device)[:total_keep]\n",
    "        mask_flat = torch.zeros(H * W, device=device)\n",
    "        mask_flat[indices] = 1.0\n",
    "        cond_mask[b, 0] = mask_flat.view(H, W)\n",
    "    \n",
    "    return cond_mask\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\"Convert tensor to displayable image.\"\"\"\n",
    "    img = tensor.detach().cpu()\n",
    "    img = (img.clamp(-1, 1) + 1) / 2  # [-1, 1] -> [0, 1]\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    return img\n",
    "\n",
    "def visualize_batch(images, titles=None, figsize=(15, 3)):\n",
    "    \"\"\"Visualize a batch of images.\"\"\"\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (img, ax) in enumerate(zip(images, axes)):\n",
    "        ax.imshow(tensor_to_image(img))\n",
    "        ax.axis(\"off\")\n",
    "        if titles:\n",
    "            ax.set_title(titles[i], fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Class-Conditional Generation (Baseline - Expected to Fail)\n\nTest generation with class labels only, **without** sparse pixel hints.\n\n**Expected Result**: Poor quality / noise-like output. The model was trained to always receive \nsparse pixel hints, so without them the SFC encoder has no meaningful input to process."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_class_conditional(model, labels, num_samples=None):\n",
    "    \"\"\"\n",
    "    Generate images conditioned only on class labels.\n",
    "    \"\"\"\n",
    "    if num_samples is None:\n",
    "        num_samples = len(labels)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Get conditioning\n",
    "    condition, uncondition = model.conditioner(labels)\n",
    "    \n",
    "    # Generate noise\n",
    "    noise = torch.randn(num_samples, 3, 32, 32, device=device)\n",
    "    \n",
    "    # Sample\n",
    "    samples = model.diffusion_sampler(\n",
    "        model.ema_denoiser,\n",
    "        noise,\n",
    "        condition,\n",
    "        uncondition,\n",
    "    )\n",
    "    \n",
    "    # Handle tuple return\n",
    "    if isinstance(samples, tuple):\n",
    "        samples = samples[0][-1] if isinstance(samples[0], list) else samples[0]\n",
    "    \n",
    "    # Decode (PixelAE is identity)\n",
    "    samples = model.vae.decode(samples)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "print(\"Generating class-conditional samples...\")\n",
    "class_samples = generate_class_conditional(model, test_labels)\n",
    "print(f\"Generated {len(class_samples)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class-conditional samples\n",
    "titles = [f\"{CIFAR10_CLASSES[l]}\" for l in test_labels.tolist()]\n",
    "fig = visualize_batch(class_samples, titles, figsize=(20, 2.5))\n",
    "plt.suptitle(\"Class-Conditional Generation (No Sparse Hints)\", fontsize=14, y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Sparse-Conditioned Reconstruction (Primary Evaluation)\n\nTest reconstruction with sparse pixel hints - **this is the model's intended use case**.\n\nGiven:\n- 40% of pixels as observations (sparse hints)\n- Class label\n\nThe model should reconstruct the full image, filling in the missing 60% of pixels coherently."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sparse_conditioned(model, images, labels, sparsity=0.4):\n",
    "    \"\"\"\n",
    "    Generate images conditioned on class labels AND sparse pixel hints.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    B = len(images)\n",
    "    \n",
    "    # Encode images to latent space\n",
    "    x_latent = model.vae.encode(images)\n",
    "    \n",
    "    # Generate sparse mask\n",
    "    cond_mask = generate_sparse_mask(x_latent, sparsity=sparsity)\n",
    "    \n",
    "    # Get conditioning\n",
    "    condition, uncondition = model.conditioner(labels)\n",
    "    \n",
    "    # Generate noise\n",
    "    noise = torch.randn_like(x_latent)\n",
    "    \n",
    "    # Sample with sparse conditioning\n",
    "    samples = model.diffusion_sampler(\n",
    "        model.ema_denoiser,\n",
    "        noise,\n",
    "        condition,\n",
    "        uncondition,\n",
    "        cond_mask=cond_mask,\n",
    "        x_cond=x_latent,\n",
    "    )\n",
    "    \n",
    "    # Handle tuple return\n",
    "    if isinstance(samples, tuple):\n",
    "        samples = samples[0][-1] if isinstance(samples[0], list) else samples[0]\n",
    "    \n",
    "    # Decode\n",
    "    samples = model.vae.decode(samples)\n",
    "    \n",
    "    # Create sparse input visualization\n",
    "    sparse_input = images * cond_mask\n",
    "    \n",
    "    return samples, sparse_input, cond_mask\n",
    "\n",
    "print(f\"Generating sparse-conditioned samples (sparsity={SPARSITY})...\")\n",
    "sparse_samples, sparse_inputs, masks = generate_sparse_conditioned(\n",
    "    model, test_images, test_labels, sparsity=SPARSITY\n",
    ")\n",
    "print(f\"Generated {len(sparse_samples)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison: Ground Truth vs Sparse Input vs Sparse-Conditioned Output\n",
    "n_show = min(5, len(test_images))\n",
    "\n",
    "fig, axes = plt.subplots(3, n_show, figsize=(3*n_show, 9))\n",
    "\n",
    "for i in range(n_show):\n",
    "    # Ground truth\n",
    "    axes[0, i].imshow(tensor_to_image(test_images[i]))\n",
    "    axes[0, i].set_title(f\"GT: {CIFAR10_CLASSES[test_labels[i]]}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    \n",
    "    # Sparse input\n",
    "    axes[1, i].imshow(tensor_to_image(sparse_inputs[i]))\n",
    "    axes[1, i].set_title(f\"Sparse ({SPARSITY*100:.0f}%)\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "    \n",
    "    # Sparse-conditioned output\n",
    "    axes[2, i].imshow(tensor_to_image(sparse_samples[i]))\n",
    "    axes[2, i].set_title(\"Reconstructed\")\n",
    "    axes[2, i].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Ground Truth\", fontsize=12)\n",
    "axes[1, 0].set_ylabel(\"Sparse Input\", fontsize=12)\n",
    "axes[2, 0].set_ylabel(\"Reconstruction\", fontsize=12)\n",
    "\n",
    "plt.suptitle(\"Sparse-Conditioned Reconstruction\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison: Class-Only vs Sparse-Conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate class-only samples with same noise for fair comparison\n",
    "torch.manual_seed(42)\n",
    "noise = torch.randn(len(test_images), 3, 32, 32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_latent = model.vae.encode(test_images)\n",
    "    cond_mask = generate_sparse_mask(x_latent, sparsity=SPARSITY)\n",
    "    condition, uncondition = model.conditioner(test_labels)\n",
    "    \n",
    "    # Class-only (same noise)\n",
    "    torch.manual_seed(42)\n",
    "    noise = torch.randn_like(x_latent)\n",
    "    class_only = model.diffusion_sampler(\n",
    "        model.ema_denoiser, noise, condition, uncondition\n",
    "    )\n",
    "    if isinstance(class_only, tuple):\n",
    "        class_only = class_only[0][-1] if isinstance(class_only[0], list) else class_only[0]\n",
    "    class_only = model.vae.decode(class_only)\n",
    "    \n",
    "    # Sparse-conditioned (same noise)\n",
    "    torch.manual_seed(42)\n",
    "    noise = torch.randn_like(x_latent)\n",
    "    sparse_cond = model.diffusion_sampler(\n",
    "        model.ema_denoiser, noise, condition, uncondition,\n",
    "        cond_mask=cond_mask, x_cond=x_latent\n",
    "    )\n",
    "    if isinstance(sparse_cond, tuple):\n",
    "        sparse_cond = sparse_cond[0][-1] if isinstance(sparse_cond[0], list) else sparse_cond[0]\n",
    "    sparse_cond = model.vae.decode(sparse_cond)\n",
    "    \n",
    "    sparse_input_vis = test_images * cond_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 4-way comparison\n",
    "n_show = min(5, len(test_images))\n",
    "\n",
    "fig, axes = plt.subplots(4, n_show, figsize=(3*n_show, 12))\n",
    "\n",
    "for i in range(n_show):\n",
    "    # Ground truth\n",
    "    axes[0, i].imshow(tensor_to_image(test_images[i]))\n",
    "    axes[0, i].set_title(f\"{CIFAR10_CLASSES[test_labels[i]]}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    \n",
    "    # Sparse input\n",
    "    axes[1, i].imshow(tensor_to_image(sparse_input_vis[i]))\n",
    "    axes[1, i].axis(\"off\")\n",
    "    \n",
    "    # Class-only\n",
    "    axes[2, i].imshow(tensor_to_image(class_only[i]))\n",
    "    axes[2, i].axis(\"off\")\n",
    "    \n",
    "    # Sparse-conditioned\n",
    "    axes[3, i].imshow(tensor_to_image(sparse_cond[i]))\n",
    "    axes[3, i].axis(\"off\")\n",
    "\n",
    "row_labels = [\"Ground Truth\", f\"Sparse Input ({SPARSITY*100:.0f}%)\", \n",
    "              \"Class-Only\", \"Sparse-Conditioned\"]\n",
    "for i, label in enumerate(row_labels):\n",
    "    axes[i, 0].set_ylabel(label, fontsize=11)\n",
    "\n",
    "plt.suptitle(\"Comparison: Class-Only vs Sparse-Conditioned Generation\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quantitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(gt, pred, mask=None):\n",
    "    \"\"\"\n",
    "    Compute reconstruction metrics.\n",
    "    \n",
    "    Args:\n",
    "        gt: Ground truth images (B, C, H, W)\n",
    "        pred: Predicted images (B, C, H, W)\n",
    "        mask: Optional mask (B, 1, H, W) - metrics computed at mask=1 locations\n",
    "    \n",
    "    Returns:\n",
    "        dict with MSE, PSNR\n",
    "    \"\"\"\n",
    "    # Normalize to [0, 1]\n",
    "    gt = (gt.clamp(-1, 1) + 1) / 2\n",
    "    pred = (pred.clamp(-1, 1) + 1) / 2\n",
    "    \n",
    "    if mask is not None:\n",
    "        # Only compute at masked locations\n",
    "        mask = mask.expand_as(gt)\n",
    "        n_pixels = mask.sum()\n",
    "        mse = ((gt - pred) ** 2 * mask).sum() / n_pixels\n",
    "    else:\n",
    "        mse = ((gt - pred) ** 2).mean()\n",
    "    \n",
    "    psnr = 10 * torch.log10(1.0 / mse)\n",
    "    \n",
    "    return {\n",
    "        \"mse\": mse.item(),\n",
    "        \"psnr\": psnr.item(),\n",
    "    }\n",
    "\n",
    "# Compute metrics\n",
    "metrics_class_only = compute_metrics(test_images, class_only)\n",
    "metrics_sparse_full = compute_metrics(test_images, sparse_cond)\n",
    "metrics_sparse_masked = compute_metrics(test_images, sparse_cond, cond_mask)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Reconstruction Metrics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nClass-Only Generation (no hints):\")\n",
    "print(f\"  MSE:  {metrics_class_only['mse']:.6f}\")\n",
    "print(f\"  PSNR: {metrics_class_only['psnr']:.2f} dB\")\n",
    "\n",
    "print(f\"\\nSparse-Conditioned (full image):\")\n",
    "print(f\"  MSE:  {metrics_sparse_full['mse']:.6f}\")\n",
    "print(f\"  PSNR: {metrics_sparse_full['psnr']:.2f} dB\")\n",
    "\n",
    "print(f\"\\nSparse-Conditioned (at hint locations only):\")\n",
    "print(f\"  MSE:  {metrics_sparse_masked['mse']:.6f}\")\n",
    "print(f\"  PSNR: {metrics_sparse_masked['psnr']:.2f} dB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if metrics_sparse_masked['psnr'] > 30:\n",
    "    print(\"Hint fidelity is HIGH - sparse conditioning is working!\")\n",
    "elif metrics_sparse_masked['psnr'] > 20:\n",
    "    print(\"Hint fidelity is MODERATE - sparse conditioning partially working.\")\n",
    "else:\n",
    "    print(\"Hint fidelity is LOW - model may need retraining with fixed code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Different Sparsity Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_levels = [0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "results = []\n",
    "\n",
    "print(\"Testing different sparsity levels...\")\n",
    "for sparsity in sparsity_levels:\n",
    "    with torch.no_grad():\n",
    "        x_latent = model.vae.encode(test_images)\n",
    "        cond_mask = generate_sparse_mask(x_latent, sparsity=sparsity)\n",
    "        condition, uncondition = model.conditioner(test_labels)\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "        noise = torch.randn_like(x_latent)\n",
    "        samples = model.diffusion_sampler(\n",
    "            model.ema_denoiser, noise, condition, uncondition,\n",
    "            cond_mask=cond_mask, x_cond=x_latent\n",
    "        )\n",
    "        if isinstance(samples, tuple):\n",
    "            samples = samples[0][-1] if isinstance(samples[0], list) else samples[0]\n",
    "        samples = model.vae.decode(samples)\n",
    "        \n",
    "        metrics = compute_metrics(test_images, samples)\n",
    "        metrics_hints = compute_metrics(test_images, samples, cond_mask)\n",
    "        \n",
    "        results.append({\n",
    "            \"sparsity\": sparsity,\n",
    "            \"psnr_full\": metrics[\"psnr\"],\n",
    "            \"psnr_hints\": metrics_hints[\"psnr\"],\n",
    "        })\n",
    "        print(f\"  Sparsity {sparsity*100:.0f}%: Full PSNR={metrics['psnr']:.2f}, Hint PSNR={metrics_hints['psnr']:.2f}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PSNR vs Sparsity\n",
    "sparsities = [r[\"sparsity\"] * 100 for r in results]\n",
    "psnr_full = [r[\"psnr_full\"] for r in results]\n",
    "psnr_hints = [r[\"psnr_hints\"] for r in results]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(sparsities, psnr_full, 'b-o', label='Full Image PSNR', linewidth=2, markersize=8)\n",
    "ax.plot(sparsities, psnr_hints, 'r-s', label='Hint Locations PSNR', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Sparsity (%)', fontsize=12)\n",
    "ax.set_ylabel('PSNR (dB)', fontsize=12)\n",
    "ax.set_title('Reconstruction Quality vs Sparsity Level', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Conclusion\n\n### Expected Results for This Model\n\nThis model was trained **with sparse conditioning** (40% observed pixels). Therefore:\n\n| Generation Mode | Expected Quality | Reason |\n|-----------------|------------------|--------|\n| **Class-only** (no hints) | ❌ Poor | Model expects sparse hints; SFC encoder has no input |\n| **Sparse-conditioned** | ✅ Good | This is what the model was trained for |\n\n### Quality Indicators\n\n**For sparse-conditioned reconstruction:**\n- **Hint PSNR > 30 dB**: Excellent - model preserves observed pixels accurately\n- **Full PSNR improves with sparsity**: Model fills in missing regions coherently\n- **Visual quality**: Reconstructions should look realistic and match the sparse input\n\n### Note on Class-Only Generation\n\nIf you need a model that can generate from class labels alone (without sparse hints),\nyou would need to train differently:\n1. Train without sparse conditioning (standard diffusion), OR\n2. Train with dropout on sparse hints (sometimes provide hints, sometimes not)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"EVALUATION SUMMARY\")\nprint(\"=\"*60)\nprint(f\"\\nCheckpoint: {CHECKPOINT_PATH}\")\nprint(f\"Encoder type: {MODEL_CONFIG['encoder_type']}\")\nprint(f\"SFC unified coords: {MODEL_CONFIG['sfc_unified_coords']}\")\nprint(f\"SFC spatial bias: {MODEL_CONFIG['sfc_spatial_bias']}\")\nprint(f\"\\nTest sparsity: {SPARSITY*100:.0f}%\")\nprint(f\"\\nMetrics:\")\nprint(f\"  Class-only PSNR: {metrics_class_only['psnr']:.2f} dB (expected: low)\")\nprint(f\"  Sparse-cond PSNR (full): {metrics_sparse_full['psnr']:.2f} dB\")\nprint(f\"  Sparse-cond PSNR (hints): {metrics_sparse_masked['psnr']:.2f} dB\")\nprint(\"\\n\" + \"=\"*60)\n\n# For a sparse-conditioning model, we expect:\n# - Class-only to be poor (model needs hints)\n# - Sparse-conditioned to be good\nprint(\"\\nINTERPRETATION:\")\nprint(\"-\" * 40)\n\nif metrics_sparse_masked['psnr'] > 30:\n    print(\"✅ Hint fidelity is HIGH (>30 dB)\")\n    print(\"   The model accurately preserves observed pixels.\")\nelse:\n    print(f\"⚠️  Hint fidelity is {metrics_sparse_masked['psnr']:.1f} dB\")\n    print(\"   Expected >30 dB for repaint-style conditioning.\")\n\nif metrics_sparse_full['psnr'] > metrics_class_only['psnr'] + 3:\n    print(f\"✅ Sparse conditioning improves PSNR by +{metrics_sparse_full['psnr'] - metrics_class_only['psnr']:.1f} dB\")\n    print(\"   The model successfully uses sparse hints for reconstruction.\")\nelse:\n    print(\"⚠️  Limited improvement from sparse conditioning.\")\n\nif metrics_class_only['psnr'] < 15:\n    print(\"✅ Class-only generation fails as expected (model needs hints).\")\nelse:\n    print(f\"ℹ️  Class-only PSNR = {metrics_class_only['psnr']:.1f} dB\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Uncertainty Estimation (100 Forward Passes)\n\nEvaluate the model's uncertainty structure by running 100 forward passes with different seeds.\nThe per-pixel variance across samples indicates model uncertainty - high variance regions are\nwhere the model is less confident about the reconstruction.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@torch.no_grad()\ndef generate_uncertainty_samples(model, images, labels, sparsity=0.4, num_samples=100):\n    \"\"\"\n    Generate multiple samples with different seeds to estimate uncertainty.\n    \n    Args:\n        model: The trained model\n        images: Ground truth images [B, C, H, W]\n        labels: Class labels [B]\n        sparsity: Fraction of observed pixels\n        num_samples: Number of forward passes with different seeds\n    \n    Returns:\n        all_samples: [num_samples, B, C, H, W] all generated samples\n        mean_sample: [B, C, H, W] mean across samples\n        variance_map: [B, C, H, W] per-pixel variance\n        cond_mask: [B, 1, H, W] the conditioning mask used\n    \"\"\"\n    device = next(model.parameters()).device\n    B = len(images)\n    \n    # Encode to latent space\n    x_latent = model.vae.encode(images)\n    \n    # Generate sparse mask (fixed across all samples)\n    torch.manual_seed(42)  # Fixed seed for consistent mask\n    cond_mask = generate_sparse_mask(x_latent, sparsity=sparsity)\n    \n    # Get conditioning\n    condition, uncondition = model.conditioner(labels)\n    \n    # Collect all samples\n    all_samples = []\n    \n    print(f\"Generating {num_samples} samples for uncertainty estimation...\")\n    for i in range(num_samples):\n        if (i + 1) % 10 == 0:\n            print(f\"  Sample {i+1}/{num_samples}\")\n        \n        # Different seed for each sample\n        torch.manual_seed(i * 12345)\n        noise = torch.randn_like(x_latent)\n        \n        # Sample with sparse conditioning\n        samples = model.diffusion_sampler(\n            model.ema_denoiser,\n            noise,\n            condition,\n            uncondition,\n            cond_mask=cond_mask,\n            x_cond=x_latent,\n        )\n        \n        # Handle tuple return\n        if isinstance(samples, tuple):\n            samples = samples[0][-1] if isinstance(samples[0], list) else samples[0]\n        \n        # Decode\n        samples = model.vae.decode(samples)\n        all_samples.append(samples.clone())\n    \n    # Stack all samples: [num_samples, B, C, H, W]\n    all_samples = torch.stack(all_samples, dim=0)\n    \n    # Compute statistics\n    mean_sample = all_samples.mean(dim=0)  # [B, C, H, W]\n    variance_map = all_samples.var(dim=0)   # [B, C, H, W]\n    \n    # Create sparse input visualization\n    sparse_input = images * cond_mask\n    \n    return all_samples, mean_sample, variance_map, cond_mask, sparse_input\n\nprint(\"Uncertainty estimation function defined.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run uncertainty estimation on a subset of test images\n# Use fewer images to reduce computation time\nuncertainty_indices = [0, 1000, 2000]  # 3 images from different classes\nuncertainty_images, uncertainty_labels = get_test_batch(test_dataset, uncertainty_indices, device)\n\nprint(f\"Running uncertainty estimation on {len(uncertainty_indices)} images...\")\nprint(f\"Classes: {[CIFAR10_CLASSES[l] for l in uncertainty_labels.tolist()]}\")\n\n# Generate 100 samples for uncertainty estimation\nall_samples, mean_sample, variance_map, unc_mask, sparse_input = generate_uncertainty_samples(\n    model, uncertainty_images, uncertainty_labels, \n    sparsity=SPARSITY, num_samples=100\n)\n\nprint(f\"\\nResults:\")\nprint(f\"  All samples shape: {all_samples.shape}\")\nprint(f\"  Mean sample shape: {mean_sample.shape}\")\nprint(f\"  Variance map shape: {variance_map.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize uncertainty maps\ndef visualize_uncertainty(gt_images, sparse_inputs, mean_samples, variance_maps, cond_mask, labels, class_names):\n    \"\"\"\n    Visualize uncertainty analysis results.\n    \n    Columns: Ground Truth | Sparse Input | Mean Reconstruction | Uncertainty Map | Mask Overlay\n    \"\"\"\n    n_images = len(gt_images)\n    fig, axes = plt.subplots(n_images, 5, figsize=(15, 3*n_images))\n    \n    if n_images == 1:\n        axes = axes.reshape(1, -1)\n    \n    for i in range(n_images):\n        # Ground truth\n        axes[i, 0].imshow(tensor_to_image(gt_images[i]))\n        axes[i, 0].set_title(f\"GT: {class_names[labels[i]]}\" if i == 0 else \"\")\n        axes[i, 0].axis(\"off\")\n        if i == 0:\n            axes[i, 0].set_ylabel(\"Ground Truth\", fontsize=10)\n        \n        # Sparse input\n        axes[i, 1].imshow(tensor_to_image(sparse_inputs[i]))\n        axes[i, 1].set_title(f\"Sparse Input ({SPARSITY*100:.0f}%)\" if i == 0 else \"\")\n        axes[i, 1].axis(\"off\")\n        \n        # Mean reconstruction\n        axes[i, 2].imshow(tensor_to_image(mean_samples[i]))\n        axes[i, 2].set_title(\"Mean (100 samples)\" if i == 0 else \"\")\n        axes[i, 2].axis(\"off\")\n        \n        # Uncertainty map (sum variance across channels)\n        var_rgb = variance_maps[i].sum(dim=0).cpu().numpy()  # [H, W]\n        var_normalized = var_rgb / (var_rgb.max() + 1e-8)  # Normalize to [0, 1]\n        im = axes[i, 3].imshow(var_normalized, cmap='hot', vmin=0, vmax=1)\n        axes[i, 3].set_title(\"Uncertainty (Variance)\" if i == 0 else \"\")\n        axes[i, 3].axis(\"off\")\n        \n        # Overlay: uncertainty with mask\n        # Show uncertainty, with observed pixels marked\n        mask_np = cond_mask[i, 0].cpu().numpy()\n        overlay = var_normalized.copy()\n        axes[i, 4].imshow(overlay, cmap='hot', vmin=0, vmax=1)\n        # Mark observed pixels in blue\n        obs_y, obs_x = np.where(mask_np > 0.5)\n        axes[i, 4].scatter(obs_x, obs_y, c='cyan', s=2, alpha=0.5)\n        axes[i, 4].set_title(\"Uncertainty + Hints (cyan)\" if i == 0 else \"\")\n        axes[i, 4].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.colorbar(im, ax=axes[:, 3], shrink=0.6, label='Normalized Variance')\n    return fig\n\nfig = visualize_uncertainty(\n    uncertainty_images, sparse_input, mean_sample, variance_map, \n    unc_mask, uncertainty_labels.tolist(), CIFAR10_CLASSES\n)\nplt.suptitle(\"Uncertainty Estimation: 100 Forward Passes\", fontsize=14, y=1.02)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Quantitative uncertainty analysis\nprint(\"=\" * 60)\nprint(\"UNCERTAINTY ANALYSIS\")\nprint(\"=\" * 60)\n\n# Compute uncertainty statistics at observed vs unobserved locations\nfor i in range(len(uncertainty_images)):\n    mask = unc_mask[i, 0]  # [H, W]\n    var = variance_map[i].sum(dim=0)  # [H, W] - sum across RGB channels\n    \n    observed_var = var[mask > 0.5].mean().item()\n    unobserved_var = var[mask < 0.5].mean().item()\n    \n    print(f\"\\nImage {i+1} ({CIFAR10_CLASSES[uncertainty_labels[i]]}):\")\n    print(f\"  Mean variance at OBSERVED pixels:   {observed_var:.6f}\")\n    print(f\"  Mean variance at UNOBSERVED pixels: {unobserved_var:.6f}\")\n    print(f\"  Ratio (unobserved/observed):        {unobserved_var/(observed_var+1e-8):.2f}x\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"INTERPRETATION:\")\nprint(\"-\" * 60)\nprint(\"If uncertainty is LOWER at observed (hint) locations:\")\nprint(\"  ✅ Model respects sparse hints and is confident there\")\nprint(\"If uncertainty is HIGHER at unobserved locations:\")\nprint(\"  ✅ Model appropriately indicates uncertainty where data is missing\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Super-Resolution to 128x128 Using Neural Field Decoder\n\nLeverage the neural field decoder to generate high-resolution outputs from 32x32 sparse inputs.\n\n**Correct Approach (from original training code):**\n1. Temporarily modify `decoder_patch_scaling_h/w = scale` on the model\n2. Create HxW tensors with 32x32 sparse hints placed at stride=scale positions\n3. Run full diffusion at HxW with this sparse conditioning\n4. Restore original `decoder_patch_scaling` values\n\n**Known Limitation - Checkerboard Artifacts:**\nThe decoder uses non-overlapping patches (patch_size × decoder_patch_scaling = 16×16 at 4x scale).\nEach patch is decoded independently by NerfBlocks with no cross-patch communication.\nThis causes visible seams at patch boundaries, especially at 4x scale.\n\n**Mitigations Implemented:**\n1. **2x scale option** - Uses 8×8 patches (less visible seams)\n2. **Overlapping inference** - Run 4 times with offset, blend results to smooth boundaries\n3. **Post-processing blur** - Apply Gaussian blur at patch boundaries",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Super-Resolution Implementation with Artifact Mitigations\n# Addresses checkerboard artifacts from non-overlapping patch decoder\n\nimport torch.nn.functional as F\n\n@torch.no_grad()\ndef generate_superres(model, images, labels, sparsity=0.4, scale=4, seed=42, disable_spatial_bias=True):\n    \"\"\"\n    Generate super-resolution from 32x32 sparse inputs (basic version).\n    \n    Args:\n        model: Trained model\n        images: [B, C, 32, 32] input images\n        labels: [B] class labels\n        sparsity: Fraction of observed pixels\n        scale: Upscaling factor (2 or 4)\n        seed: Random seed for reproducibility\n        disable_spatial_bias: If True, disable spatial attention bias during SR\n            to reduce checkerboard artifacts (default: True)\n    \n    Returns:\n        recon_32: [B, C, 32, 32] baseline reconstruction\n        recon_hr: [B, C, 32*scale, 32*scale] super-resolution output\n        sparse_input: [B, C, 32, 32] sparse input visualization\n        mask_32: [B, 1, 32, 32] conditioning mask\n    \"\"\"\n    device = next(model.parameters()).device\n    B = len(images)\n    H_hr = 32 * scale\n    W_hr = 32 * scale\n    \n    # Encode to latent space (32x32)\n    x_latent_32 = model.vae.encode(images)\n    \n    # Generate sparse mask at 32x32\n    torch.manual_seed(seed)\n    cond_mask_32 = generate_sparse_mask(x_latent_32, sparsity=sparsity)\n    \n    # Get conditioning\n    condition, uncondition = model.conditioner(labels)\n    \n    # ----- 32x32 baseline reconstruction -----\n    torch.manual_seed(seed)\n    noise_32 = torch.randn_like(x_latent_32)\n    samples_32 = model.diffusion_sampler(\n        model.ema_denoiser, noise_32, condition, uncondition,\n        cond_mask=cond_mask_32, x_cond=x_latent_32,\n    )\n    if isinstance(samples_32, tuple):\n        samples_32 = samples_32[0][-1] if isinstance(samples_32[0], list) else samples_32[0]\n    recon_32 = model.vae.decode(samples_32)\n    \n    # ----- High-res super-resolution -----\n    # Create HR tensors with 32x32 hints at stride positions\n    cond_mask_hr = torch.zeros((B, 1, H_hr, W_hr), device=device, dtype=x_latent_32.dtype)\n    cond_mask_hr[:, :, ::scale, ::scale] = cond_mask_32\n    \n    x_cond_hr = torch.zeros((B, x_latent_32.shape[1], H_hr, W_hr), device=device, dtype=x_latent_32.dtype)\n    x_cond_hr[:, :, ::scale, ::scale] = x_latent_32\n    \n    # Temporarily modify decoder_patch_scaling\n    old_scales = {}\n    for name, net in [(\"denoiser\", model.denoiser), (\"ema_denoiser\", model.ema_denoiser)]:\n        if hasattr(net, \"decoder_patch_scaling_h\"):\n            old_scales[name] = (net.decoder_patch_scaling_h, net.decoder_patch_scaling_w)\n            net.decoder_patch_scaling_h = scale\n            net.decoder_patch_scaling_w = scale\n    \n    # Run diffusion at high resolution\n    # Note: disable_spatial_bias=True reduces checkerboard artifacts at SR\n    # by preventing overly-localized attention that causes patch boundary discontinuities\n    torch.manual_seed(seed)\n    noise_hr = torch.randn_like(x_cond_hr)\n    samples_hr = model.diffusion_sampler(\n        model.ema_denoiser, noise_hr, condition, uncondition,\n        cond_mask=cond_mask_hr, x_cond=x_cond_hr,\n        disable_spatial_bias=disable_spatial_bias,\n    )\n    if isinstance(samples_hr, tuple):\n        samples_hr = samples_hr[0][-1] if isinstance(samples_hr[0], list) else samples_hr[0]\n    recon_hr = model.vae.decode(samples_hr)\n    \n    # Restore original decoder_patch_scaling values\n    for name, net in [(\"denoiser\", model.denoiser), (\"ema_denoiser\", model.ema_denoiser)]:\n        if name in old_scales:\n            h, w = old_scales[name]\n            net.decoder_patch_scaling_h = h\n            net.decoder_patch_scaling_w = w\n    \n    sparse_input = images * cond_mask_32\n    return recon_32, recon_hr, sparse_input, cond_mask_32\n\n\n@torch.no_grad()\ndef generate_superres_overlapped(model, images, labels, sparsity=0.4, scale=4, seed=42, disable_spatial_bias=True):\n    \"\"\"\n    Generate super-resolution with overlapping patch inference to reduce artifacts.\n    \n    Runs inference 4 times with different offsets and blends results.\n    This smooths the patch boundaries by averaging multiple predictions.\n    \n    Note: This is 4x slower but significantly reduces checkerboard artifacts.\n    \"\"\"\n    device = next(model.parameters()).device\n    B = len(images)\n    H_hr = 32 * scale\n    W_hr = 32 * scale\n    patch_size = 4 * scale  # Decoder patch size at this scale (16 for 4x, 8 for 2x)\n    \n    # Encode to latent space\n    x_latent_32 = model.vae.encode(images)\n    \n    # Generate sparse mask\n    torch.manual_seed(seed)\n    cond_mask_32 = generate_sparse_mask(x_latent_32, sparsity=sparsity)\n    \n    # Get conditioning\n    condition, uncondition = model.conditioner(labels)\n    \n    # Create HR tensors\n    cond_mask_hr = torch.zeros((B, 1, H_hr, W_hr), device=device, dtype=x_latent_32.dtype)\n    cond_mask_hr[:, :, ::scale, ::scale] = cond_mask_32\n    \n    x_cond_hr = torch.zeros((B, x_latent_32.shape[1], H_hr, W_hr), device=device, dtype=x_latent_32.dtype)\n    x_cond_hr[:, :, ::scale, ::scale] = x_latent_32\n    \n    # Temporarily modify decoder_patch_scaling\n    old_scales = {}\n    for name, net in [(\"denoiser\", model.denoiser), (\"ema_denoiser\", model.ema_denoiser)]:\n        if hasattr(net, \"decoder_patch_scaling_h\"):\n            old_scales[name] = (net.decoder_patch_scaling_h, net.decoder_patch_scaling_w)\n            net.decoder_patch_scaling_h = scale\n            net.decoder_patch_scaling_w = scale\n    \n    # Run inference with 4 different offsets and blend\n    offset_step = patch_size // 2  # Half patch offset\n    offsets = [(0, 0), (offset_step, 0), (0, offset_step), (offset_step, offset_step)]\n    \n    accumulated = torch.zeros((B, 3, H_hr, W_hr), device=device, dtype=x_latent_32.dtype)\n    weight_map = torch.zeros((B, 1, H_hr, W_hr), device=device, dtype=x_latent_32.dtype)\n    \n    for idx, (off_h, off_w) in enumerate(offsets):\n        print(f\"    Offset {idx+1}/4: ({off_h}, {off_w})\")\n        \n        # Pad input, run inference, then crop back\n        if off_h > 0 or off_w > 0:\n            # Pad to shift the patch grid\n            padded_mask = F.pad(cond_mask_hr, (off_w, 0, off_h, 0), mode='constant', value=0)\n            padded_cond = F.pad(x_cond_hr, (off_w, 0, off_h, 0), mode='constant', value=0)\n            \n            # Crop to original size (shift content)\n            padded_mask = padded_mask[:, :, :H_hr, :W_hr]\n            padded_cond = padded_cond[:, :, :H_hr, :W_hr]\n        else:\n            padded_mask = cond_mask_hr\n            padded_cond = x_cond_hr\n        \n        # Run diffusion with spatial bias disabled\n        torch.manual_seed(seed + idx)\n        noise_hr = torch.randn((B, 3, H_hr, W_hr), device=device, dtype=x_latent_32.dtype)\n        samples_hr = model.diffusion_sampler(\n            model.ema_denoiser, noise_hr, condition, uncondition,\n            cond_mask=padded_mask, x_cond=padded_cond,\n            disable_spatial_bias=disable_spatial_bias,\n        )\n        if isinstance(samples_hr, tuple):\n            samples_hr = samples_hr[0][-1] if isinstance(samples_hr[0], list) else samples_hr[0]\n        result = model.vae.decode(samples_hr)\n        \n        # Shift back and accumulate\n        if off_h > 0 or off_w > 0:\n            # Pad the result to shift it back\n            result = F.pad(result, (0, off_w, 0, off_h), mode='constant', value=0)\n            result = result[:, :, off_h:off_h+H_hr, off_w:off_w+W_hr]\n            \n            # Create weight mask (1 where we have valid pixels)\n            w_mask = torch.ones((B, 1, H_hr, W_hr), device=device, dtype=x_latent_32.dtype)\n            w_mask = F.pad(w_mask, (0, off_w, 0, off_h), mode='constant', value=0)\n            w_mask = w_mask[:, :, off_h:off_h+H_hr, off_w:off_w+W_hr]\n        else:\n            w_mask = torch.ones((B, 1, H_hr, W_hr), device=device, dtype=x_latent_32.dtype)\n        \n        accumulated += result * w_mask\n        weight_map += w_mask\n    \n    # Average\n    recon_hr = accumulated / (weight_map + 1e-8)\n    \n    # Restore decoder_patch_scaling\n    for name, net in [(\"denoiser\", model.denoiser), (\"ema_denoiser\", model.ema_denoiser)]:\n        if name in old_scales:\n            h, w = old_scales[name]\n            net.decoder_patch_scaling_h = h\n            net.decoder_patch_scaling_w = w\n    \n    sparse_input = images * cond_mask_32\n    return recon_hr, sparse_input, cond_mask_32\n\n\ndef apply_boundary_blur(image, patch_size=16, blur_width=3):\n    \"\"\"\n    Apply Gaussian blur along patch boundaries to smooth artifacts.\n    \n    Args:\n        image: [B, C, H, W] image tensor\n        patch_size: Size of decoder patches\n        blur_width: Width of blur kernel\n    \n    Returns:\n        Smoothed image with blurred patch boundaries\n    \"\"\"\n    B, C, H, W = image.shape\n    device = image.device\n    \n    # Create boundary mask (1 at patch boundaries, 0 elsewhere)\n    boundary_mask = torch.zeros(1, 1, H, W, device=device)\n    \n    # Horizontal boundaries\n    for y in range(patch_size, H, patch_size):\n        boundary_mask[:, :, max(0, y-blur_width):min(H, y+blur_width), :] = 1.0\n    \n    # Vertical boundaries\n    for x in range(patch_size, W, patch_size):\n        boundary_mask[:, :, :, max(0, x-blur_width):min(W, x+blur_width)] = 1.0\n    \n    # Apply Gaussian blur to entire image\n    kernel_size = blur_width * 2 + 1\n    sigma = blur_width / 2\n    \n    # Create Gaussian kernel\n    x_coord = torch.arange(kernel_size, device=device).float() - kernel_size // 2\n    gauss_1d = torch.exp(-x_coord**2 / (2 * sigma**2))\n    gauss_1d = gauss_1d / gauss_1d.sum()\n    gauss_2d = gauss_1d.view(-1, 1) @ gauss_1d.view(1, -1)\n    gauss_kernel = gauss_2d.view(1, 1, kernel_size, kernel_size).expand(C, 1, -1, -1)\n    \n    # Apply blur\n    padding = kernel_size // 2\n    blurred = F.conv2d(\n        F.pad(image, (padding, padding, padding, padding), mode='reflect'),\n        gauss_kernel.to(image.dtype),\n        groups=C\n    )\n    \n    # Blend: use blurred only at boundaries\n    boundary_mask = boundary_mask.expand(B, C, H, W)\n    result = image * (1 - boundary_mask) + blurred * boundary_mask\n    \n    return result\n\n\nprint(\"Super-resolution functions defined:\")\nprint(\"  - generate_superres(): Basic SR with disable_spatial_bias option\")\nprint(\"  - generate_superres_overlapped(): Overlapped inference (4x slower, smoother)\")\nprint(\"  - apply_boundary_blur(): Post-processing to smooth patch boundaries\")\nprint()\nprint(\"NOTE: disable_spatial_bias=True (default) reduces checkerboard artifacts\")\nprint(\"      by preventing overly-localized attention at patch boundaries.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run super-resolution evaluation\n# Use a subset of test images\nsr_indices = [0, 1000, 5000]  # 3 images from different classes\nsr_images, sr_labels = get_test_batch(test_dataset, sr_indices, device)\n\nprint(f\"Testing super-resolution on {len(sr_indices)} images...\")\nprint(f\"Classes: {[CIFAR10_CLASSES[l] for l in sr_labels.tolist()]}\")\nprint(f\"Input resolution: 32x32, Output resolution: 128x128 (4x upscale)\")\nprint()\n\n# Run super-resolution (correct implementation)\nprint(\"Running Super-Resolution:\")\nprint(\"-\" * 50)\nrecon_32, recon_128, sparse_input_sr, mask_32 = generate_superres(\n    model, sr_images, sr_labels, sparsity=SPARSITY, scale=4, seed=42\n)\nprint(f\"  32x32 reconstruction shape: {recon_32.shape}\")\nprint(f\"  128x128 super-resolution shape: {recon_128.shape}\")\nprint(\"\\nSuper-resolution generation complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize super-resolution results\ndef tensor_to_image_128(tensor):\n    \"\"\"Convert 128x128 tensor to displayable image.\"\"\"\n    img = tensor.detach().cpu()\n    img = (img.clamp(-1, 1) + 1) / 2  # [-1, 1] -> [0, 1]\n    img = img.permute(1, 2, 0).numpy()\n    return img\n\nn_show = len(sr_images)\nfig, axes = plt.subplots(n_show, 4, figsize=(16, 4*n_show))\n\nif n_show == 1:\n    axes = axes.reshape(1, -1)\n\nfor i in range(n_show):\n    # Ground truth (32x32)\n    axes[i, 0].imshow(tensor_to_image(sr_images[i]))\n    axes[i, 0].set_title(f\"GT 32x32\\n{CIFAR10_CLASSES[sr_labels[i]]}\" if i == 0 else CIFAR10_CLASSES[sr_labels[i]])\n    axes[i, 0].axis(\"off\")\n    \n    # Sparse input (32x32)\n    axes[i, 1].imshow(tensor_to_image(sparse_input_sr[i]))\n    axes[i, 1].set_title(f\"Sparse Input\\n32x32 ({SPARSITY*100:.0f}%)\" if i == 0 else \"\")\n    axes[i, 1].axis(\"off\")\n    \n    # 32x32 reconstruction\n    axes[i, 2].imshow(tensor_to_image(recon_32[i]))\n    axes[i, 2].set_title(\"Recon 32x32\" if i == 0 else \"\")\n    axes[i, 2].axis(\"off\")\n    \n    # 128x128 super-resolution\n    axes[i, 3].imshow(tensor_to_image_128(recon_128[i]))\n    axes[i, 3].set_title(\"Super-Res 128x128\" if i == 0 else \"\")\n    axes[i, 3].axis(\"off\")\n\nplt.suptitle(\"Super-Resolution: 32x32 → 128x128 (via decoder_patch_scaling)\", fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare with bilinear upscaled ground truth\ngt_128_bilinear = F.interpolate(sr_images, size=(128, 128), mode='bilinear', align_corners=False)\n\n# Also create bilinear upscale of 32x32 reconstruction for comparison\nrecon_32_upscaled = F.interpolate(recon_32, size=(128, 128), mode='bilinear', align_corners=False)\n\n# Compute PSNR at 128x128 resolution\ndef compute_psnr_128(pred, gt):\n    \"\"\"Compute PSNR between 128x128 images.\"\"\"\n    pred = (pred.clamp(-1, 1) + 1) / 2\n    gt = (gt.clamp(-1, 1) + 1) / 2\n    mse = ((pred - gt) ** 2).mean()\n    psnr = 10 * torch.log10(1.0 / mse)\n    return psnr.item()\n\nprint(\"=\" * 60)\nprint(\"SUPER-RESOLUTION ANALYSIS\")\nprint(\"=\" * 60)\n\nprint(\"\\nPSNR vs Bilinear-upscaled Ground Truth (128x128):\")\nprint(\"-\" * 50)\n\npsnr_32_upscaled = compute_psnr_128(recon_32_upscaled, gt_128_bilinear)\npsnr_128_sr = compute_psnr_128(recon_128, gt_128_bilinear)\n\nprint(f\"32x32 Recon (bilinear upscale) PSNR: {psnr_32_upscaled:.2f} dB\")\nprint(f\"128x128 Super-Resolution PSNR:       {psnr_128_sr:.2f} dB\")\nprint(f\"Improvement:                         +{psnr_128_sr - psnr_32_upscaled:.2f} dB\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"IMPLEMENTATION NOTES:\")\nprint(\"-\" * 60)\nprint(\"\"\"\nCorrect Super-Resolution Approach:\n  1. Temporarily set decoder_patch_scaling_h/w = 4 on the model\n  2. Create 128x128 tensors with 32x32 hints at stride=4 positions:\n     - cond_mask_128[:, :, ::4, ::4] = cond_mask_32\n     - x_cond_128[:, :, ::4, ::4] = x_latent_32\n  3. Run full diffusion at 128x128 resolution\n  4. Restore original decoder_patch_scaling values\n\nThis approach \"lifts\" sparse 32x32 observations to a 128x128 grid,\nwhere each pixel at (i,j) in 32x32 becomes pixel (i*4, j*4) in 128x128.\nThe neural field decoder then fills in all 128x128 pixels.\n\"\"\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Detailed comparison: zoom into patches to see detail quality\ndef visualize_sr_comparison_detailed(gt_32, sparse_input, recon_32, recon_128, label, class_name):\n    \"\"\"\n    Detailed super-resolution comparison with zoom patches.\n    \"\"\"\n    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n    \n    # Row 1: Full images\n    gt_128 = F.interpolate(gt_32.unsqueeze(0), size=(128, 128), mode='bilinear', align_corners=False).squeeze(0)\n    sparse_128 = F.interpolate(sparse_input.unsqueeze(0), size=(128, 128), mode='nearest').squeeze(0)\n    recon_32_up = F.interpolate(recon_32.unsqueeze(0), size=(128, 128), mode='bilinear', align_corners=False).squeeze(0)\n    \n    axes[0, 0].imshow(tensor_to_image_128(gt_128))\n    axes[0, 0].set_title(f\"GT (bilinear 128x128)\\n{class_name}\")\n    axes[0, 0].axis(\"off\")\n    \n    axes[0, 1].imshow(tensor_to_image_128(sparse_128))\n    axes[0, 1].set_title(f\"Sparse Input\\n({SPARSITY*100:.0f}% observed)\")\n    axes[0, 1].axis(\"off\")\n    \n    axes[0, 2].imshow(tensor_to_image_128(recon_32_up))\n    axes[0, 2].set_title(\"32x32 Recon\\n(bilinear upscale)\")\n    axes[0, 2].axis(\"off\")\n    \n    axes[0, 3].imshow(tensor_to_image_128(recon_128))\n    axes[0, 3].set_title(\"128x128 Super-Res\\n(neural field)\")\n    axes[0, 3].axis(\"off\")\n    \n    # Row 2: Zoomed center patch (64x64 crop at center)\n    crop_start = 32  # Center crop\n    crop_end = 96\n    \n    def crop_center(img):\n        return img[:, crop_start:crop_end, crop_start:crop_end]\n    \n    axes[1, 0].imshow(tensor_to_image_128(crop_center(gt_128)))\n    axes[1, 0].set_title(\"Center 64x64 crop\")\n    axes[1, 0].axis(\"off\")\n    \n    axes[1, 1].imshow(tensor_to_image_128(crop_center(sparse_128)))\n    axes[1, 1].axis(\"off\")\n    \n    axes[1, 2].imshow(tensor_to_image_128(crop_center(recon_32_up)))\n    axes[1, 2].axis(\"off\")\n    \n    axes[1, 3].imshow(tensor_to_image_128(crop_center(recon_128)))\n    axes[1, 3].axis(\"off\")\n    \n    axes[0, 0].set_ylabel(\"Full 128x128\", fontsize=12)\n    axes[1, 0].set_ylabel(\"Center crop\", fontsize=12)\n    \n    plt.tight_layout()\n    return fig\n\n# Show detailed comparison for first image\nfig = visualize_sr_comparison_detailed(\n    sr_images[0], sparse_input_sr[0], recon_32[0], recon_128[0],\n    sr_labels[0], CIFAR10_CLASSES[sr_labels[0]]\n)\nplt.suptitle(\"Detailed Super-Resolution Comparison\", fontsize=14, y=1.02)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 13. Final Summary\n\nThis notebook evaluated the sparse-conditioned PixNerDiT model with:\n\n### Key Findings\n\n| Evaluation | Result | Interpretation |\n|------------|--------|----------------|\n| **Class-only generation** | Poor | Model was trained WITH sparse hints, cannot generate from scratch |\n| **Sparse-conditioned** | Good | Model successfully reconstructs from 40% observed pixels |\n| **Hint fidelity** | High PSNR at hint locations | Repaint-style conditioning preserves input pixels |\n| **Uncertainty estimation** | Higher variance at unobserved locations | Model appropriately indicates uncertainty |\n| **Super-resolution** | 128x128 output from 32x32 input | Neural field decoder enables arbitrary resolution |\n\n### Architecture Highlights\n\n1. **SFC Tokenizer**: Converts sparse pixel observations to sequence tokens using space-filling curves\n2. **Option A (unified coords)**: Shared coordinate embedding for encoder/decoder alignment\n3. **Option B (spatial bias)**: Attention bias based on spatial proximity\n4. **Neural Field Decoder (NerfBlocks)**: Continuous coordinate queries enable arbitrary output resolution\n\n### Super-Resolution Implementation\n\nThe correct approach (from original training code):\n1. Temporarily modify `decoder_patch_scaling_h/w = scale` on the model\n2. \"Lift\" 32x32 hints to 128x128 grid at stride=4 positions\n3. Run full diffusion at 128x128 resolution\n4. Restore original decoder_patch_scaling values\n\nThis differs from using the `superres_scale` parameter, which only affects a single forward pass.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}