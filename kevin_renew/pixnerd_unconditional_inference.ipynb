{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# PixNerd Unconditional Generation\n",
    "\n",
    "This notebook demonstrates unconditional image generation using a pretrained PixNerd checkpoint.\n",
    "By setting guidance=1.0 and using only the unconditional embedding, we can generate random coherent images\n",
    "without any text or class conditioning.\n",
    "\n",
    "Super-resolution is still supported via the NF decoder patch scaling mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": "## Environment Setup\n\n- Requires a GPU runtime\n- Assumes dependencies from `requirements.txt` are installed\n- Place the checkpoint at `PixNerd/checkpoints/checkpoints/PixNerd-XXL-P16-T2I/model.ckpt`\n- Text encoder: Auto-detects local Qwen3-1.7B or downloads from HuggingFace (`Qwen/Qwen3-1.7B`)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9j0k1l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to PixNerd folder where src/ is located\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the directory containing this notebook\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "print(f\"Starting directory: {NOTEBOOK_DIR}\")\n",
    "\n",
    "# Navigate to PixNerd folder (where src/ lives)\n",
    "PIXNERD_DIR = os.path.join(NOTEBOOK_DIR, \"PixNerd\")\n",
    "if os.path.exists(PIXNERD_DIR):\n",
    "    os.chdir(PIXNERD_DIR)\n",
    "    print(f\"Changed to: {os.getcwd()}\")\n",
    "elif os.path.basename(NOTEBOOK_DIR) == \"PixNerd\":\n",
    "    print(f\"Already in PixNerd directory: {NOTEBOOK_DIR}\")\n",
    "else:\n",
    "    # Try parent directory\n",
    "    parent = os.path.dirname(NOTEBOOK_DIR)\n",
    "    pixnerd_in_parent = os.path.join(parent, \"PixNerd\")\n",
    "    if os.path.exists(pixnerd_in_parent):\n",
    "        os.chdir(pixnerd_in_parent)\n",
    "        print(f\"Changed to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(f\"WARNING: Could not find PixNerd folder. Current dir: {NOTEBOOK_DIR}\")\n",
    "\n",
    "# Verify src/ exists\n",
    "if os.path.exists(\"src\"):\n",
    "    print(\"Found src/ directory\")\n",
    "else:\n",
    "    print(\"ERROR: src/ directory not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3n4o5p6",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\nimport math\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Paths (relative to PixNerd directory)\nPIXNERD_ROOT = Path(os.getcwd())\n\n# ============================================================\n# CHECKPOINT PATH - UPDATE THIS TO YOUR CHECKPOINT LOCATION\n# ============================================================\nCKPT_PATH = PIXNERD_ROOT / \"checkpoints\" / \"checkpoints\" / \"PixNerd-XXL-P16-T2I\" / \"model.ckpt\"\n\n# Alternative: uncomment and modify if your checkpoint is elsewhere\n# CKPT_PATH = Path(\"/path/to/your/model.ckpt\")\n# ============================================================\n\n# Text encoder path - tries local paths first, falls back to HuggingFace\nLOCAL_QWEN_PATHS = [\n    \"/pscratch/sd/d/dpark1/models/Qwen3-1.7B\",  # Your scratch space\n    \"/pscratch/sd/k/kevinval/models/Qwen3-1.7B\",  # Alternative location\n    str(PIXNERD_ROOT / \"models\" / \"Qwen3-1.7B\"),  # Local to project\n]\n\nTEXT_ENCODER_PATH = None\nfor path in LOCAL_QWEN_PATHS:\n    if os.path.exists(path):\n        TEXT_ENCODER_PATH = path\n        print(f\"Found local Qwen3 at: {path}\")\n        break\n\nif TEXT_ENCODER_PATH is None:\n    # Fall back to HuggingFace (will download automatically)\n    TEXT_ENCODER_PATH = \"Qwen/Qwen3-1.7B\"\n    print(f\"No local Qwen3 found, will download from HuggingFace: {TEXT_ENCODER_PATH}\")\n\n# Output directory for generated images\nOUTPUT_DIR = PIXNERD_ROOT / \"outputs\" / \"unconditional\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif DEVICE != \"cuda\":\n    raise RuntimeError(\"PixNerd inference requires a CUDA GPU\")\n\nprint(f\"PixNerd root: {PIXNERD_ROOT}\")\nprint(f\"Checkpoint path: {CKPT_PATH}\")\nprint(f\"Checkpoint exists: {CKPT_PATH.exists()}\")\nif not CKPT_PATH.exists():\n    print(f\"\\n⚠️  CHECKPOINT NOT FOUND!\")\n    print(f\"   Please update CKPT_PATH in this cell to point to your model.ckpt\")\n    print(f\"   Looking for: {CKPT_PATH}\")\nprint(f\"Text encoder: {TEXT_ENCODER_PATH}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Device: {DEVICE}\")"
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0",
   "metadata": {},
   "source": [
    "## Build Model Components\n",
    "\n",
    "We still need the Qwen3TextEncoder to generate the unconditional embedding (empty string encoding).\n",
    "The model architecture is identical to T2I - we just use it differently during sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PixNerd components from src/\n",
    "from src.models.autoencoder.pixel import PixelAE\n",
    "from src.models.conditioner.qwen3_text_encoder import Qwen3TextEncoder\n",
    "from src.models.transformer.pixnerd_t2i_heavydecoder import PixNerDiT\n",
    "from src.diffusion.flow_matching.scheduling import LinearScheduler\n",
    "from src.diffusion.flow_matching.adam_sampling import AdamLMSampler, ode_step_fn\n",
    "from src.diffusion.base.guidance import simple_guidance_fn\n",
    "from src.diffusion.flow_matching.training_repa import REPATrainer\n",
    "from src.callbacks.simple_ema import SimpleEMA\n",
    "from src.lightning_model import LightningModel\n",
    "from src.models.encoder import IndentityMapping\n",
    "from src.models.autoencoder.base import fp2uint8\n",
    "\n",
    "# Model hyperparameters (must match checkpoint)\n",
    "HIDDEN_SIZE = 1536\n",
    "TXT_EMBED_DIM = 2048\n",
    "PATCH_SIZE = 16\n",
    "TXT_MAX_LENGTH = 128\n",
    "BASE_RES = 512  # Training resolution\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5z6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model components\n",
    "print(\"Initializing model components...\")\n",
    "\n",
    "main_scheduler = LinearScheduler()\n",
    "\n",
    "vae = PixelAE(scale=1.0)\n",
    "\n",
    "print(\"Loading Qwen3 text encoder (this may take a moment)...\")\n",
    "conditioner = Qwen3TextEncoder(\n",
    "    weight_path=TEXT_ENCODER_PATH,\n",
    "    embed_dim=TXT_EMBED_DIM,\n",
    "    max_length=TXT_MAX_LENGTH,\n",
    ")\n",
    "\n",
    "denoiser = PixNerDiT(\n",
    "    in_channels=3,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_groups=24,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    txt_embed_dim=TXT_EMBED_DIM,\n",
    "    txt_max_length=TXT_MAX_LENGTH,\n",
    "    num_text_blocks=4,\n",
    "    decoder_hidden_size=64,\n",
    "    num_encoder_blocks=16,\n",
    "    num_decoder_blocks=2,\n",
    ")\n",
    "\n",
    "# Sampler configured for unconditional generation (guidance=1.0)\n",
    "sampler = AdamLMSampler(\n",
    "    num_steps=25,\n",
    "    guidance=1.0,  # No guidance amplification for unconditional\n",
    "    timeshift=3.0,\n",
    "    order=2,\n",
    "    scheduler=main_scheduler,\n",
    "    guidance_fn=simple_guidance_fn,\n",
    "    step_fn=ode_step_fn,\n",
    ")\n",
    "\n",
    "# REPATrainer stub (needed for checkpoint loading)\n",
    "trainer_stub = REPATrainer(\n",
    "    scheduler=main_scheduler,\n",
    "    lognorm_t=True,\n",
    "    timeshift=4.0,\n",
    "    feat_loss_weight=0.5,\n",
    "    encoder=IndentityMapping(),\n",
    "    align_layer=6,\n",
    "    proj_denoiser_dim=HIDDEN_SIZE,\n",
    "    proj_hidden_dim=HIDDEN_SIZE,\n",
    "    proj_encoder_dim=768,\n",
    ")\n",
    "\n",
    "ema_tracker = SimpleEMA(decay=0.9999)\n",
    "\n",
    "model = LightningModel(\n",
    "    vae=vae,\n",
    "    conditioner=conditioner,\n",
    "    denoiser=denoiser,\n",
    "    diffusion_trainer=trainer_stub,\n",
    "    diffusion_sampler=sampler,\n",
    "    ema_tracker=ema_tracker,\n",
    "    optimizer=None,\n",
    "    lr_scheduler=None,\n",
    "    eval_original_model=False,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "print(\"Model initialized and moved to GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3h4i5j6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading checkpoint from: {CKPT_PATH}\")\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\", weights_only=False)\n",
    "missing, unexpected = model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "print(f\"Missing keys: {len(missing)} | Unexpected keys: {len(unexpected)}\")\n",
    "print(\"Checkpoint loaded. Ready for unconditional sampling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k7l8m9n0",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1p2q3r4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_decoder_scale(scale: float):\n",
    "    \"\"\"Set NF decoder patch scaling for super-resolution.\"\"\"\n",
    "    for net in [model.denoiser, getattr(model, \"ema_denoiser\", None)]:\n",
    "        if net is None:\n",
    "            continue\n",
    "        net.decoder_patch_scaling_h = scale\n",
    "        net.decoder_patch_scaling_w = scale\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_unconditional(\n",
    "    batch_size: int = 4,\n",
    "    height: int = 512,\n",
    "    width: int = 512,\n",
    "    seed: int = 42,\n",
    "    num_steps: int = 25,\n",
    "    base_res: int = BASE_RES,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate images without any conditioning.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of images to generate\n",
    "        height: Output image height (can be different from base_res for super-res)\n",
    "        width: Output image width\n",
    "        seed: Random seed for reproducibility\n",
    "        num_steps: Number of ODE solver steps\n",
    "        base_res: Training resolution (used to compute scaling factor)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Generated images as uint8 [B, 3, H, W]\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Compute and set decoder scale for super-resolution\n",
    "    if height == base_res and width == base_res:\n",
    "        set_decoder_scale(1.0)\n",
    "        print(f\"Generating at native {base_res}x{base_res}\")\n",
    "    else:\n",
    "        scale_h = height / float(base_res)\n",
    "        scale_w = width / float(base_res)\n",
    "        assert scale_h == scale_w, \"Only square scaling supported\"\n",
    "        set_decoder_scale(scale_h)\n",
    "        print(f\"Generating at {height}x{width} (scale={scale_h:.2f}x)\")\n",
    "    \n",
    "    # Configure sampler for unconditional generation\n",
    "    model.diffusion_sampler.guidance = 1.0  # No CFG amplification\n",
    "    model.diffusion_sampler.num_steps = num_steps\n",
    "    \n",
    "    # Start from Gaussian noise at target resolution\n",
    "    noise = torch.randn(batch_size, 3, height, width, device=DEVICE)\n",
    "    \n",
    "    # Get unconditional embedding (empty string encoded by Qwen3)\n",
    "    # We pass uncondition for BOTH condition and uncondition arguments\n",
    "    dummy_prompts = [\"\"] * batch_size\n",
    "    _, uncondition = model.conditioner(dummy_prompts)\n",
    "    uncondition = uncondition.to(DEVICE)\n",
    "    \n",
    "    # Run sampling with uncondition for both slots\n",
    "    # CFG formula: uncond + guidance * (cond - uncond)\n",
    "    # With cond = uncond and guidance = 1.0: output = uncond (pure unconditional)\n",
    "    samples = model.diffusion_sampler(\n",
    "        model.ema_denoiser,\n",
    "        noise,\n",
    "        uncondition,  # condition slot (using uncond)\n",
    "        uncondition,  # uncondition slot\n",
    "    )\n",
    "    \n",
    "    # Decode to pixel space\n",
    "    images = model.vae.decode(samples)\n",
    "    images = torch.clamp(images, -1.0, 1.0)\n",
    "    images_uint8 = fp2uint8(images)\n",
    "    \n",
    "    return images_uint8.cpu()\n",
    "\n",
    "\n",
    "def show_images(images_uint8, title=\"\", cols=None):\n",
    "    \"\"\"Display a batch of images.\"\"\"\n",
    "    if isinstance(images_uint8, torch.Tensor):\n",
    "        imgs_np = images_uint8.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    else:\n",
    "        imgs_np = np.transpose(images_uint8, (0, 2, 3, 1))\n",
    "    \n",
    "    n = len(imgs_np)\n",
    "    if cols is None:\n",
    "        cols = min(n, 4)\n",
    "    rows = math.ceil(n / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs_np)):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"{title} #{i}\")\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for ax in axes[n:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_grid(images_uint8, filename, cols=None):\n",
    "    \"\"\"Save images as a grid.\"\"\"\n",
    "    if isinstance(images_uint8, torch.Tensor):\n",
    "        imgs_np = images_uint8.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    else:\n",
    "        imgs_np = np.transpose(images_uint8, (0, 2, 3, 1))\n",
    "    \n",
    "    imgs = [Image.fromarray(img) for img in imgs_np]\n",
    "    \n",
    "    n = len(imgs)\n",
    "    if cols is None:\n",
    "        cols = min(n, 4)\n",
    "    rows = math.ceil(n / cols)\n",
    "    \n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", (cols * w, rows * h))\n",
    "    for idx, img in enumerate(imgs):\n",
    "        r, c = divmod(idx, cols)\n",
    "        grid.paste(img, (c * w, r * h))\n",
    "    \n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    grid.save(out_path)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5t6u7v8",
   "metadata": {},
   "source": [
    "## Unconditional Generation at Base Resolution (512x512)\n",
    "\n",
    "Generate random images without any text or class conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9x0y1z2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 4 unconditional images at 512x512\n",
    "print(\"=\" * 50)\n",
    "print(\"Unconditional Generation: 512x512 (native)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "images_512 = sample_unconditional(\n",
    "    batch_size=4,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    seed=42,\n",
    "    num_steps=25,\n",
    ")\n",
    "\n",
    "print(f\"Output shape: {images_512.shape}\")\n",
    "show_images(images_512, title=\"Unconditional 512x512\")\n",
    "save_grid(images_512, \"unconditional_512x512_grid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## Unconditional Super-Resolution (1024x1024)\n",
    "\n",
    "The NF decoder can upsample to higher resolutions even in unconditional mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8g9h0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unconditional images at 1024x1024 (2x super-res)\n",
    "print(\"=\" * 50)\n",
    "print(\"Unconditional Generation: 1024x1024 (2x super-res)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "images_1024 = sample_unconditional(\n",
    "    batch_size=4,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    seed=42,  # Same seed as 512 for comparison\n",
    "    num_steps=25,\n",
    ")\n",
    "\n",
    "print(f\"Output shape: {images_1024.shape}\")\n",
    "show_images(images_1024, title=\"Unconditional 1024x1024\")\n",
    "save_grid(images_1024, \"unconditional_1024x1024_grid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1j2k3l4",
   "metadata": {},
   "source": [
    "## Unconditional Super-Resolution (768x768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m5n6o7p8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate at 768x768 (1.5x super-res)\n",
    "print(\"=\" * 50)\n",
    "print(\"Unconditional Generation: 768x768 (1.5x super-res)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "images_768 = sample_unconditional(\n",
    "    batch_size=4,\n",
    "    height=768,\n",
    "    width=768,\n",
    "    seed=123,\n",
    "    num_steps=25,\n",
    ")\n",
    "\n",
    "print(f\"Output shape: {images_768.shape}\")\n",
    "show_images(images_768, title=\"Unconditional 768x768\")\n",
    "save_grid(images_768, \"unconditional_768x768_grid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q9r0s1t2",
   "metadata": {},
   "source": [
    "## Seed Variation\n",
    "\n",
    "Different seeds produce different random images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u3v4w5x6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with different seeds\n",
    "print(\"=\" * 50)\n",
    "print(\"Seed Variation at 512x512\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_images = []\n",
    "seeds = [0, 42, 123, 456, 789, 1000, 2024, 9999]\n",
    "\n",
    "for seed in seeds:\n",
    "    img = sample_unconditional(\n",
    "        batch_size=1,\n",
    "        height=512,\n",
    "        width=512,\n",
    "        seed=seed,\n",
    "        num_steps=25,\n",
    "    )\n",
    "    all_images.append(img)\n",
    "\n",
    "# Stack all images\n",
    "all_images = torch.cat(all_images, dim=0)\n",
    "print(f\"Generated {len(seeds)} images with different seeds\")\n",
    "show_images(all_images, title=\"Seed\", cols=4)\n",
    "save_grid(all_images, \"unconditional_seed_variation.png\", cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y7z8a9b0",
   "metadata": {},
   "source": [
    "## Compare: Base vs Super-Resolution (Same Seed)\n",
    "\n",
    "Using the same seed, compare 512x512 native vs 1024x1024 super-res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Comparison: Same seed, different resolutions\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "COMPARE_SEED = 777\n",
    "\n",
    "# 512x512\n",
    "img_512 = sample_unconditional(\n",
    "    batch_size=1,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    seed=COMPARE_SEED,\n",
    "    num_steps=25,\n",
    ")\n",
    "\n",
    "# 1024x1024\n",
    "img_1024 = sample_unconditional(\n",
    "    batch_size=1,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    seed=COMPARE_SEED,\n",
    "    num_steps=25,\n",
    ")\n",
    "\n",
    "# Bilinear upscale 512 to 1024 for comparison\n",
    "img_512_tensor = img_512.float() / 255.0\n",
    "img_512_upscaled = F.interpolate(\n",
    "    img_512_tensor,\n",
    "    size=(1024, 1024),\n",
    "    mode='bilinear',\n",
    "    align_corners=False,\n",
    ")\n",
    "img_512_upscaled = (img_512_upscaled * 255).to(torch.uint8)\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(img_512[0].permute(1, 2, 0).numpy())\n",
    "axes[0].set_title(\"512x512 Native\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img_512_upscaled[0].permute(1, 2, 0).numpy())\n",
    "axes[1].set_title(\"512 -> Bilinear 1024\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(img_1024[0].permute(1, 2, 0).numpy())\n",
    "axes[2].set_title(\"1024x1024 NF Super-Res\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Seed {COMPARE_SEED}: Native vs Bilinear vs NF Super-Resolution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5h6i7j8",
   "metadata": {},
   "source": [
    "## High-Resolution Unconditional (1920x1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9l0m1n2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate at very high resolution\n",
    "print(\"=\" * 50)\n",
    "print(\"Unconditional Generation: 1920x1920 (3.75x super-res)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "images_1920 = sample_unconditional(\n",
    "    batch_size=2,  # Fewer images due to GPU memory\n",
    "    height=1920,\n",
    "    width=1920,\n",
    "    seed=42,\n",
    "    num_steps=25,\n",
    ")\n",
    "\n",
    "print(f\"Output shape: {images_1920.shape}\")\n",
    "show_images(images_1920, title=\"Unconditional 1920x1920\", cols=2)\n",
    "save_grid(images_1920, \"unconditional_1920x1920_grid.png\", cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o3p4q5r6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated **unconditional generation** with PixNerd:\n",
    "\n",
    "### How It Works\n",
    "- Set `guidance=1.0` (no CFG amplification)\n",
    "- Use the unconditional embedding (empty string `\"\"` encoded by Qwen3) for **both** condition and uncondition slots\n",
    "- CFG formula becomes: `uncond + 1.0 * (uncond - uncond) = uncond`\n",
    "\n",
    "### Super-Resolution Still Works\n",
    "- The NF decoder patch scaling mechanism is independent of conditioning\n",
    "- Can generate at arbitrary resolutions: 512, 768, 1024, 1920, etc.\n",
    "\n",
    "### Trade-offs\n",
    "| Aspect | Unconditional | Conditional (T2I) |\n",
    "|--------|---------------|-------------------|\n",
    "| Control | None | Full text control |\n",
    "| Output | Random from training distribution | Specified content |\n",
    "| Quality | Good | Slightly better with CFG |\n",
    "\n",
    "### Use Cases\n",
    "- Exploring the model's learned image distribution\n",
    "- Generating diverse random images\n",
    "- Baseline comparison for conditional generation\n",
    "- Testing super-resolution without conditioning effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7t8u9v0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}