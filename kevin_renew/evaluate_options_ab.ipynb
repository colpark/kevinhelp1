{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options A/B Ablation Study - Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the impact of:\n",
    "- **Option A** (sfc_unified_coords): Shared coordinate embedder for tokens & queries\n",
    "- **Option B** (sfc_spatial_bias): Spatial attention bias in cross-attention\n",
    "\n",
    "We also explore:\n",
    "- Different **sampling steps** (50, 100, 200, 500)\n",
    "- Different **guidance scales** (1.0, 1.5, 2.0, 3.0)\n",
    "- Different **sparsity rates** (10%, 20%, 40%, 60%, 80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nfrom pathlib import Path\n\n# Setup paths\nNOTEBOOK_DIR = Path(os.getcwd())\nPIXNERD_DIR = NOTEBOOK_DIR / \"PixNerd\"\nif PIXNERD_DIR.exists():\n    os.chdir(PIXNERD_DIR)\n    sys.path.insert(0, str(PIXNERD_DIR))\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nfrom torchvision.datasets import CIFAR10\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom tqdm import tqdm  # Use regular tqdm instead of tqdm.auto\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Disable torch compile\ntorch._dynamo.config.disable = True\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model components\n",
    "from src.models.autoencoder.pixel import PixelAE\n",
    "from src.models.conditioner.class_label import LabelConditioner\n",
    "from src.models.transformer.pixnerd_c2i_heavydecoder import PixNerDiT\n",
    "from src.diffusion.flow_matching.scheduling import LinearScheduler\n",
    "from src.diffusion.flow_matching.sampling import EulerSampler, ode_step_fn\n",
    "from src.diffusion.base.guidance import simple_guidance_fn\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Device configuration\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n\n# Model configuration (should match training)\nMODEL_CONFIG = {\n    \"in_channels\": 3,\n    \"hidden_size\": 512,\n    \"decoder_hidden_size\": 64,\n    \"num_encoder_blocks\": 8,\n    \"num_decoder_blocks\": 2,\n    \"num_classes\": 10,\n    \"patch_size\": 8,\n    \"num_groups\": 8,\n    \"encoder_type\": \"sfc\",\n    \"sfc_curve\": \"hilbert\",\n    \"sfc_group_size\": 8,\n    \"sfc_cross_depth\": 2,\n    \"sfc_self_depth\": 2,\n}\n\n# Auto-discover checkpoints\ndef find_checkpoints():\n    \"\"\"Scan workdirs for available checkpoints.\"\"\"\n    import glob\n    \n    # Common checkpoint locations to search\n    search_paths = [\n        \"./workdirs/*/checkpoints/*.ckpt\",\n        \"./workdirs/*/checkpoints/last.ckpt\",\n        \"../workdirs/*/checkpoints/*.ckpt\",\n        \"/home/*/workspace/*/workdirs/*/checkpoints/*.ckpt\",  # SciServer pattern\n    ]\n    \n    found = {}\n    all_ckpts = []\n    \n    for pattern in search_paths:\n        all_ckpts.extend(glob.glob(pattern, recursive=True))\n    \n    # Deduplicate\n    all_ckpts = list(set(all_ckpts))\n    \n    print(\"Found checkpoints:\")\n    for ckpt in sorted(all_ckpts):\n        print(f\"  {ckpt}\")\n        \n        # Try to match to model variant based on path\n        path_lower = ckpt.lower()\n        if \"baseline\" in path_lower or (\"no_option\" in path_lower and \"a\" in path_lower and \"b\" in path_lower):\n            found.setdefault(\"baseline\", ckpt)\n        elif \"a+b\" in path_lower or \"ab\" in path_lower or (\"option_a\" in path_lower and \"option_b\" in path_lower):\n            found.setdefault(\"A+B\", ckpt)\n        elif \"a_only\" in path_lower or (\"option_a\" in path_lower and \"no_option_b\" in path_lower):\n            found.setdefault(\"A_only\", ckpt)\n        elif \"b_only\" in path_lower or (\"option_b\" in path_lower and \"no_option_a\" in path_lower):\n            found.setdefault(\"B_only\", ckpt)\n        # Also check exp_name patterns\n        elif \"cifar10_ab\" in path_lower and \"only\" not in path_lower:\n            found.setdefault(\"A+B\", ckpt)\n        elif \"cifar10_a_only\" in path_lower or \"cifar10_sfc_a_only\" in path_lower:\n            found.setdefault(\"A_only\", ckpt)\n        elif \"cifar10_b_only\" in path_lower or \"cifar10_sfc_b_only\" in path_lower:\n            found.setdefault(\"B_only\", ckpt)\n        elif \"cifar10_baseline\" in path_lower or \"cifar10_sfc_baseline\" in path_lower:\n            found.setdefault(\"baseline\", ckpt)\n    \n    return found, all_ckpts\n\ndiscovered_ckpts, all_ckpt_list = find_checkpoints()\n\n# Default checkpoint paths - will be overridden by discovered ones\nCHECKPOINT_PATHS = {\n    \"A+B\": \"./workdirs/exp_cifar10_ab/checkpoints/last.ckpt\",\n    \"A_only\": \"./workdirs/exp_cifar10_a_only/checkpoints/last.ckpt\",\n    \"B_only\": \"./workdirs/exp_cifar10_b_only/checkpoints/last.ckpt\",\n    \"baseline\": \"./workdirs/exp_cifar10_baseline/checkpoints/last.ckpt\",\n}\n\n# Override with discovered checkpoints\nCHECKPOINT_PATHS.update(discovered_ckpts)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CHECKPOINT CONFIGURATION\")\nprint(\"=\"*60)\nfor name, path in CHECKPOINT_PATHS.items():\n    exists = os.path.exists(path)\n    status = \"✓ FOUND\" if exists else \"✗ NOT FOUND\"\n    print(f\"  {name:12s}: {status}\")\n    print(f\"               {path}\")\n\n# If checkpoints not auto-discovered, user can manually set them here:\n# CHECKPOINT_PATHS[\"A+B\"] = \"/path/to/your/checkpoint.ckpt\"\n\n# Evaluation parameters\nEVAL_BATCH_SIZE = 8\nNUM_EVAL_BATCHES = 4  # Total samples = EVAL_BATCH_SIZE * NUM_EVAL_BATCHES\n\nprint(f\"\\nDevice: {DEVICE}\")\nprint(f\"Dtype: {DTYPE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_cifar10_test(num_samples=32, seed=42):\n    \"\"\"Load fixed test samples from CIFAR-10.\"\"\"\n    torch.manual_seed(seed)\n    \n    tfm = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n    \n    dataset = CIFAR10(root=\"./data\", train=False, transform=tfm, download=True)\n    \n    # Get fixed samples\n    indices = torch.randperm(len(dataset))[:num_samples]\n    images = torch.stack([dataset[i][0] for i in indices])\n    labels = torch.tensor([dataset[i][1] for i in indices])\n    \n    return images, labels\n\n\ndef generate_sparsity_mask(batch_size, height, width, sparsity, device, dtype=torch.float32, seed=None):\n    \"\"\"\n    Generate conditioning mask with given sparsity.\n    \n    Args:\n        sparsity: fraction of pixels to use as conditioning (0.0 to 1.0)\n    \n    Returns:\n        cond_mask: (B, 1, H, W) with 1s at conditioning pixels\n    \"\"\"\n    if seed is not None:\n        torch.manual_seed(seed)\n    \n    B, H, W = batch_size, height, width\n    total = H * W\n    k_cond = int(round(sparsity * total))\n    k_cond = max(1, min(total, k_cond))\n    \n    cond_mask = torch.zeros(B, 1, H, W, device=device, dtype=dtype)\n    flat_idx = torch.arange(total, device=device)\n    \n    for b in range(B):\n        perm = flat_idx[torch.randperm(total, device=device)]\n        cond_idx = perm[:k_cond]\n        cond_mask[b].view(-1)[cond_idx] = 1.0\n    \n    return cond_mask\n\n\ndef tensor_to_image(tensor, nrow=8):\n    \"\"\"Convert tensor to displayable image grid.\"\"\"\n    tensor = tensor.detach().cpu().float()  # Convert to float32 for numpy compatibility\n    tensor = (tensor.clamp(-1, 1) + 1) / 2  # [-1,1] -> [0,1]\n    grid = make_grid(tensor, nrow=nrow, padding=2, normalize=False)\n    return grid.permute(1, 2, 0).numpy()\n\n\ndef compute_metrics(pred, target, mask=None):\n    \"\"\"Compute reconstruction metrics.\"\"\"\n    pred = pred.float()\n    target = target.float()\n    \n    if mask is not None:\n        mask = mask.float().expand_as(pred)\n        mse = ((pred - target) ** 2 * mask).sum() / mask.sum()\n    else:\n        mse = F.mse_loss(pred, target)\n    \n    psnr = 10 * torch.log10(4.0 / (mse + 1e-8))  # max range is 2 for [-1,1]\n    \n    return {\n        \"mse\": mse.item(),\n        \"psnr\": psnr.item(),\n    }\n\n\nprint(\"Helper functions defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(option_a: bool, option_b: bool):\n",
    "    \"\"\"Build model with specified options.\"\"\"\n",
    "    model = PixNerDiT(\n",
    "        **MODEL_CONFIG,\n",
    "        sfc_unified_coords=option_a,\n",
    "        sfc_spatial_bias=option_b,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_checkpoint(model, checkpoint_path):\n",
    "    \"\"\"Load checkpoint into model.\"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Warning: Checkpoint not found: {checkpoint_path}\")\n",
    "        return False\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    \n",
    "    # Handle Lightning checkpoint format\n",
    "    if \"state_dict\" in checkpoint:\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Extract EMA denoiser weights (preferred) or regular denoiser\n",
    "    model_state = {}\n",
    "    prefix = \"ema_denoiser.\"  # Use EMA weights\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(prefix):\n",
    "            new_key = k[len(prefix):]\n",
    "            model_state[new_key] = v\n",
    "    \n",
    "    if not model_state:\n",
    "        # Fallback to regular denoiser\n",
    "        prefix = \"denoiser.\"\n",
    "        for k, v in state_dict.items():\n",
    "            if k.startswith(prefix):\n",
    "                new_key = k[len(prefix):]\n",
    "                model_state[new_key] = v\n",
    "    \n",
    "    if model_state:\n",
    "        model.load_state_dict(model_state, strict=False)\n",
    "        print(f\"Loaded {len(model_state)} parameters from {checkpoint_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Warning: Could not find model weights in checkpoint\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def build_sampler(scheduler, num_steps=200, guidance=2.0):\n",
    "    \"\"\"Build sampler with specified parameters.\"\"\"\n",
    "    return EulerSampler(\n",
    "        num_steps=num_steps,\n",
    "        guidance=guidance,\n",
    "        guidance_interval_min=0.0,\n",
    "        guidance_interval_max=1.0,\n",
    "        scheduler=scheduler,\n",
    "        w_scheduler=LinearScheduler(),\n",
    "        guidance_fn=simple_guidance_fn,\n",
    "        step_fn=ode_step_fn,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Model loading functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images\n",
    "test_images, test_labels = load_cifar10_test(num_samples=EVAL_BATCH_SIZE * NUM_EVAL_BATCHES)\n",
    "print(f\"Loaded {len(test_images)} test images\")\n",
    "\n",
    "# Display some test images\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 3))\n",
    "ax.imshow(tensor_to_image(test_images[:16], nrow=16))\n",
    "ax.set_title(\"Test Images (first 16)\")\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load All Model Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model variants\n",
    "MODEL_VARIANTS = {\n",
    "    \"A+B\": {\"option_a\": True, \"option_b\": True},\n",
    "    \"A_only\": {\"option_a\": True, \"option_b\": False},\n",
    "    \"B_only\": {\"option_a\": False, \"option_b\": True},\n",
    "    \"baseline\": {\"option_a\": False, \"option_b\": False},\n",
    "}\n",
    "\n",
    "# Build conditioner\n",
    "conditioner = LabelConditioner(num_classes=MODEL_CONFIG[\"num_classes\"]).to(DEVICE)\n",
    "conditioner.eval()\n",
    "\n",
    "# Build VAE (identity for pixel space)\n",
    "vae = PixelAE(scale=1.0)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = LinearScheduler()\n",
    "\n",
    "# Load models\n",
    "models = {}\n",
    "for name, config in MODEL_VARIANTS.items():\n",
    "    print(f\"\\nLoading {name}...\")\n",
    "    model = build_model(**config)\n",
    "    \n",
    "    ckpt_path = CHECKPOINT_PATHS.get(name)\n",
    "    if ckpt_path and os.path.exists(ckpt_path):\n",
    "        load_checkpoint(model, ckpt_path)\n",
    "        model = model.to(DEVICE).to(DTYPE)\n",
    "        model.eval()\n",
    "        models[name] = model\n",
    "        print(f\"  Loaded successfully!\")\n",
    "    else:\n",
    "        print(f\"  Checkpoint not found, skipping.\")\n",
    "\n",
    "print(f\"\\nLoaded {len(models)} model variants: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_reconstruction(\n",
    "    model,\n",
    "    images,\n",
    "    labels,\n",
    "    sparsity=0.2,\n",
    "    num_steps=200,\n",
    "    guidance=2.0,\n",
    "    disable_spatial_bias=False,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run sparse-conditioned reconstruction.\n",
    "    \n",
    "    Args:\n",
    "        model: PixNerDiT model\n",
    "        images: (B, C, H, W) ground truth images\n",
    "        labels: (B,) class labels\n",
    "        sparsity: fraction of pixels to condition on\n",
    "        num_steps: number of sampling steps\n",
    "        guidance: classifier-free guidance scale\n",
    "        disable_spatial_bias: whether to disable Option B during sampling\n",
    "        seed: random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        samples: (B, C, H, W) reconstructed images\n",
    "        cond_mask: (B, 1, H, W) conditioning mask used\n",
    "    \"\"\"\n",
    "    B, C, H, W = images.shape\n",
    "    device = images.device\n",
    "    dtype = images.dtype\n",
    "    \n",
    "    # Generate conditioning mask\n",
    "    cond_mask = generate_sparsity_mask(B, H, W, sparsity, device, dtype, seed=seed)\n",
    "    \n",
    "    # Get class conditioning\n",
    "    condition, uncondition = conditioner(labels)\n",
    "    \n",
    "    # Build sampler\n",
    "    sampler = build_sampler(scheduler, num_steps=num_steps, guidance=guidance)\n",
    "    \n",
    "    # Start from noise\n",
    "    noise = torch.randn_like(images)\n",
    "    \n",
    "    # Wrapper to pass disable_spatial_bias\n",
    "    def model_fn(x, t, y, cond_mask=None):\n",
    "        return model(x, t, y, cond_mask=cond_mask, disable_spatial_bias=disable_spatial_bias)\n",
    "    \n",
    "    # Sample\n",
    "    samples = sampler(\n",
    "        model_fn,\n",
    "        noise,\n",
    "        condition,\n",
    "        uncondition,\n",
    "        cond_mask=cond_mask,\n",
    "        x_cond=images,\n",
    "    )\n",
    "    \n",
    "    return samples, cond_mask\n",
    "\n",
    "\n",
    "print(\"Sampling function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ablation: Options A/B Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_comparison(\n",
    "    models_dict,\n",
    "    images,\n",
    "    labels,\n",
    "    sparsity=0.2,\n",
    "    num_steps=200,\n",
    "    guidance=2.0,\n",
    "):\n",
    "    \"\"\"Run comparison across all model variants.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    images = images.to(DEVICE).to(DTYPE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    for name, model in tqdm(models_dict.items(), desc=\"Models\"):\n",
    "        samples, cond_mask = sample_reconstruction(\n",
    "            model, images, labels,\n",
    "            sparsity=sparsity,\n",
    "            num_steps=num_steps,\n",
    "            guidance=guidance,\n",
    "        )\n",
    "        \n",
    "        # Compute metrics on non-conditioned pixels\n",
    "        target_mask = 1.0 - cond_mask\n",
    "        metrics = compute_metrics(samples, images, mask=target_mask)\n",
    "        \n",
    "        results[name] = {\n",
    "            \"samples\": samples.cpu(),\n",
    "            \"cond_mask\": cond_mask.cpu(),\n",
    "            \"metrics\": metrics,\n",
    "        }\n",
    "        \n",
    "        print(f\"{name}: PSNR={metrics['psnr']:.2f} dB, MSE={metrics['mse']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run ablation if models are loaded\n",
    "if models:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Options A/B Ablation (sparsity=0.2, steps=200, guidance=2.0)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    ablation_results = run_ablation_comparison(\n",
    "        models,\n",
    "        test_images[:EVAL_BATCH_SIZE],\n",
    "        test_labels[:EVAL_BATCH_SIZE],\n",
    "        sparsity=0.2,\n",
    "        num_steps=200,\n",
    "        guidance=2.0,\n",
    "    )\n",
    "else:\n",
    "    print(\"No models loaded - please check checkpoint paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ablation_results(results, gt_images, num_show=8):\n",
    "    \"\"\"Visualize ablation results.\"\"\"\n",
    "    num_variants = len(results)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_variants + 2, 1, figsize=(16, 3 * (num_variants + 2)))\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[0].imshow(tensor_to_image(gt_images[:num_show], nrow=num_show))\n",
    "    axes[0].set_title(\"Ground Truth\", fontsize=14)\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Conditioning mask (from first result)\n",
    "    first_result = list(results.values())[0]\n",
    "    cond_vis = gt_images[:num_show].clone()\n",
    "    mask = first_result[\"cond_mask\"][:num_show]\n",
    "    cond_vis = cond_vis * mask + (1 - mask) * 0.5  # Gray out non-conditioned\n",
    "    axes[1].imshow(tensor_to_image(cond_vis, nrow=num_show))\n",
    "    axes[1].set_title(f\"Conditioning Mask (sparsity={mask.mean():.1%})\", fontsize=14)\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    # Each variant\n",
    "    for idx, (name, data) in enumerate(results.items()):\n",
    "        samples = data[\"samples\"][:num_show]\n",
    "        metrics = data[\"metrics\"]\n",
    "        \n",
    "        axes[idx + 2].imshow(tensor_to_image(samples, nrow=num_show))\n",
    "        axes[idx + 2].set_title(\n",
    "            f\"{name}: PSNR={metrics['psnr']:.2f} dB\",\n",
    "            fontsize=14\n",
    "        )\n",
    "        axes[idx + 2].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"ablation_options_ab.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if models and 'ablation_results' in dir():\n",
    "    visualize_ablation_results(ablation_results, test_images[:EVAL_BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ablation: Sampling Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steps_ablation(model, images, labels, steps_list=[50, 100, 200, 500], sparsity=0.2, guidance=2.0):\n",
    "    \"\"\"Evaluate effect of sampling steps.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    images = images.to(DEVICE).to(DTYPE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    for num_steps in tqdm(steps_list, desc=\"Steps\"):\n",
    "        samples, cond_mask = sample_reconstruction(\n",
    "            model, images, labels,\n",
    "            sparsity=sparsity,\n",
    "            num_steps=num_steps,\n",
    "            guidance=guidance,\n",
    "        )\n",
    "        \n",
    "        target_mask = 1.0 - cond_mask\n",
    "        metrics = compute_metrics(samples, images, mask=target_mask)\n",
    "        \n",
    "        results[num_steps] = {\n",
    "            \"samples\": samples.cpu(),\n",
    "            \"cond_mask\": cond_mask.cpu(),\n",
    "            \"metrics\": metrics,\n",
    "        }\n",
    "        \n",
    "        print(f\"Steps={num_steps}: PSNR={metrics['psnr']:.2f} dB\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run if we have the A+B model\n",
    "if \"A+B\" in models:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sampling Steps Ablation (A+B model)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    steps_results = run_steps_ablation(\n",
    "        models[\"A+B\"],\n",
    "        test_images[:EVAL_BATCH_SIZE],\n",
    "        test_labels[:EVAL_BATCH_SIZE],\n",
    "        steps_list=[50, 100, 200, 500],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_steps_ablation(results, gt_images, num_show=8):\n",
    "    \"\"\"Visualize steps ablation.\"\"\"\n",
    "    num_steps_variants = len(results)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_steps_variants + 1, 1, figsize=(16, 3 * (num_steps_variants + 1)))\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[0].imshow(tensor_to_image(gt_images[:num_show], nrow=num_show))\n",
    "    axes[0].set_title(\"Ground Truth\", fontsize=14)\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Each step count\n",
    "    for idx, (num_steps, data) in enumerate(sorted(results.items())):\n",
    "        samples = data[\"samples\"][:num_show]\n",
    "        metrics = data[\"metrics\"]\n",
    "        \n",
    "        axes[idx + 1].imshow(tensor_to_image(samples, nrow=num_show))\n",
    "        axes[idx + 1].set_title(\n",
    "            f\"Steps={num_steps}: PSNR={metrics['psnr']:.2f} dB\",\n",
    "            fontsize=14\n",
    "        )\n",
    "        axes[idx + 1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"ablation_steps.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot PSNR vs steps\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    steps = sorted(results.keys())\n",
    "    psnrs = [results[s][\"metrics\"][\"psnr\"] for s in steps]\n",
    "    ax.plot(steps, psnrs, \"o-\", markersize=10, linewidth=2)\n",
    "    ax.set_xlabel(\"Sampling Steps\", fontsize=12)\n",
    "    ax.set_ylabel(\"PSNR (dB)\", fontsize=12)\n",
    "    ax.set_title(\"PSNR vs Sampling Steps\", fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"psnr_vs_steps.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if 'steps_results' in dir():\n",
    "    visualize_steps_ablation(steps_results, test_images[:EVAL_BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ablation: Guidance Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_guidance_ablation(model, images, labels, guidance_list=[1.0, 1.5, 2.0, 3.0, 4.0], sparsity=0.2, num_steps=200):\n",
    "    \"\"\"Evaluate effect of guidance scale.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    images = images.to(DEVICE).to(DTYPE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    for guidance in tqdm(guidance_list, desc=\"Guidance\"):\n",
    "        samples, cond_mask = sample_reconstruction(\n",
    "            model, images, labels,\n",
    "            sparsity=sparsity,\n",
    "            num_steps=num_steps,\n",
    "            guidance=guidance,\n",
    "        )\n",
    "        \n",
    "        target_mask = 1.0 - cond_mask\n",
    "        metrics = compute_metrics(samples, images, mask=target_mask)\n",
    "        \n",
    "        results[guidance] = {\n",
    "            \"samples\": samples.cpu(),\n",
    "            \"cond_mask\": cond_mask.cpu(),\n",
    "            \"metrics\": metrics,\n",
    "        }\n",
    "        \n",
    "        print(f\"Guidance={guidance}: PSNR={metrics['psnr']:.2f} dB\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if \"A+B\" in models:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Guidance Scale Ablation (A+B model)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    guidance_results = run_guidance_ablation(\n",
    "        models[\"A+B\"],\n",
    "        test_images[:EVAL_BATCH_SIZE],\n",
    "        test_labels[:EVAL_BATCH_SIZE],\n",
    "        guidance_list=[1.0, 1.5, 2.0, 3.0, 4.0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_guidance_ablation(results, gt_images, num_show=8):\n",
    "    \"\"\"Visualize guidance ablation.\"\"\"\n",
    "    num_variants = len(results)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_variants + 1, 1, figsize=(16, 3 * (num_variants + 1)))\n",
    "    \n",
    "    axes[0].imshow(tensor_to_image(gt_images[:num_show], nrow=num_show))\n",
    "    axes[0].set_title(\"Ground Truth\", fontsize=14)\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    for idx, (guidance, data) in enumerate(sorted(results.items())):\n",
    "        samples = data[\"samples\"][:num_show]\n",
    "        metrics = data[\"metrics\"]\n",
    "        \n",
    "        axes[idx + 1].imshow(tensor_to_image(samples, nrow=num_show))\n",
    "        axes[idx + 1].set_title(\n",
    "            f\"Guidance={guidance}: PSNR={metrics['psnr']:.2f} dB\",\n",
    "            fontsize=14\n",
    "        )\n",
    "        axes[idx + 1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"ablation_guidance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot PSNR vs guidance\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    guidance_vals = sorted(results.keys())\n",
    "    psnrs = [results[g][\"metrics\"][\"psnr\"] for g in guidance_vals]\n",
    "    ax.plot(guidance_vals, psnrs, \"o-\", markersize=10, linewidth=2, color=\"orange\")\n",
    "    ax.set_xlabel(\"Guidance Scale\", fontsize=12)\n",
    "    ax.set_ylabel(\"PSNR (dB)\", fontsize=12)\n",
    "    ax.set_title(\"PSNR vs Guidance Scale\", fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"psnr_vs_guidance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if 'guidance_results' in dir():\n",
    "    visualize_guidance_ablation(guidance_results, test_images[:EVAL_BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ablation: Sparsity Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sparsity_ablation(model, images, labels, sparsity_list=[0.1, 0.2, 0.4, 0.6, 0.8], num_steps=200, guidance=2.0):\n",
    "    \"\"\"Evaluate effect of conditioning sparsity.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    images = images.to(DEVICE).to(DTYPE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    for sparsity in tqdm(sparsity_list, desc=\"Sparsity\"):\n",
    "        samples, cond_mask = sample_reconstruction(\n",
    "            model, images, labels,\n",
    "            sparsity=sparsity,\n",
    "            num_steps=num_steps,\n",
    "            guidance=guidance,\n",
    "        )\n",
    "        \n",
    "        target_mask = 1.0 - cond_mask\n",
    "        metrics = compute_metrics(samples, images, mask=target_mask)\n",
    "        \n",
    "        results[sparsity] = {\n",
    "            \"samples\": samples.cpu(),\n",
    "            \"cond_mask\": cond_mask.cpu(),\n",
    "            \"metrics\": metrics,\n",
    "        }\n",
    "        \n",
    "        print(f\"Sparsity={sparsity:.0%}: PSNR={metrics['psnr']:.2f} dB\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if \"A+B\" in models:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sparsity Rate Ablation (A+B model, trained with 20% input)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sparsity_results = run_sparsity_ablation(\n",
    "        models[\"A+B\"],\n",
    "        test_images[:EVAL_BATCH_SIZE],\n",
    "        test_labels[:EVAL_BATCH_SIZE],\n",
    "        sparsity_list=[0.05, 0.1, 0.2, 0.4, 0.6, 0.8],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sparsity_ablation(results, gt_images, num_show=8):\n",
    "    \"\"\"Visualize sparsity ablation with conditioning masks.\"\"\"\n",
    "    num_variants = len(results)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_variants + 1, 2, figsize=(20, 3 * (num_variants + 1)))\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[0, 0].imshow(tensor_to_image(gt_images[:num_show], nrow=num_show))\n",
    "    axes[0, 0].set_title(\"Ground Truth\", fontsize=14)\n",
    "    axes[0, 0].axis(\"off\")\n",
    "    axes[0, 1].axis(\"off\")\n",
    "    \n",
    "    # Each sparsity level\n",
    "    for idx, (sparsity, data) in enumerate(sorted(results.items())):\n",
    "        samples = data[\"samples\"][:num_show]\n",
    "        cond_mask = data[\"cond_mask\"][:num_show]\n",
    "        metrics = data[\"metrics\"]\n",
    "        \n",
    "        # Show conditioning visualization\n",
    "        cond_vis = gt_images[:num_show].clone()\n",
    "        cond_vis = cond_vis * cond_mask + (1 - cond_mask) * 0.5\n",
    "        axes[idx + 1, 0].imshow(tensor_to_image(cond_vis, nrow=num_show))\n",
    "        axes[idx + 1, 0].set_title(f\"Input ({sparsity:.0%} observed)\", fontsize=14)\n",
    "        axes[idx + 1, 0].axis(\"off\")\n",
    "        \n",
    "        # Show reconstruction\n",
    "        axes[idx + 1, 1].imshow(tensor_to_image(samples, nrow=num_show))\n",
    "        axes[idx + 1, 1].set_title(f\"Output: PSNR={metrics['psnr']:.2f} dB\", fontsize=14)\n",
    "        axes[idx + 1, 1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"ablation_sparsity.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot PSNR vs sparsity\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sparsity_vals = sorted(results.keys())\n",
    "    psnrs = [results[s][\"metrics\"][\"psnr\"] for s in sparsity_vals]\n",
    "    ax.plot([s * 100 for s in sparsity_vals], psnrs, \"o-\", markersize=10, linewidth=2, color=\"green\")\n",
    "    ax.axvline(x=20, color=\"red\", linestyle=\"--\", label=\"Training sparsity (20%)\")\n",
    "    ax.set_xlabel(\"Conditioning Sparsity (%)\", fontsize=12)\n",
    "    ax.set_ylabel(\"PSNR (dB)\", fontsize=12)\n",
    "    ax.set_title(\"PSNR vs Conditioning Sparsity\\n(Model trained with 20% input)\", fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"psnr_vs_sparsity.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if 'sparsity_results' in dir():\n",
    "    visualize_sparsity_ablation(sparsity_results, test_images[:EVAL_BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Super-Resolution Evaluation (4x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_superres(\n",
    "    model,\n",
    "    images,\n",
    "    labels,\n",
    "    scale=4,\n",
    "    sparsity=0.2,\n",
    "    num_steps=200,\n",
    "    guidance=2.0,\n",
    "    disable_spatial_bias=False,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run super-resolution: upsample conditioning mask and sample at higher resolution.\n",
    "    \"\"\"\n",
    "    B, C, H, W = images.shape\n",
    "    device = images.device\n",
    "    dtype = images.dtype\n",
    "    \n",
    "    H_hr, W_hr = H * scale, W * scale\n",
    "    \n",
    "    # Generate LR conditioning mask\n",
    "    cond_mask_lr = generate_sparsity_mask(B, H, W, sparsity, device, dtype, seed=seed)\n",
    "    \n",
    "    # Lift to HR (place LR pixels on HR grid)\n",
    "    cond_mask_hr = torch.zeros((B, 1, H_hr, W_hr), device=device, dtype=dtype)\n",
    "    cond_mask_hr[:, :, ::scale, ::scale] = cond_mask_lr\n",
    "    \n",
    "    x_cond_hr = torch.zeros((B, C, H_hr, W_hr), device=device, dtype=dtype)\n",
    "    x_cond_hr[:, :, ::scale, ::scale] = images\n",
    "    \n",
    "    # Set decoder scale\n",
    "    old_scale_h = model.decoder_patch_scaling_h\n",
    "    old_scale_w = model.decoder_patch_scaling_w\n",
    "    model.decoder_patch_scaling_h = scale\n",
    "    model.decoder_patch_scaling_w = scale\n",
    "    \n",
    "    # Get conditioning\n",
    "    condition, uncondition = conditioner(labels)\n",
    "    \n",
    "    # Build sampler\n",
    "    sampler = build_sampler(scheduler, num_steps=num_steps, guidance=guidance)\n",
    "    \n",
    "    # Sample\n",
    "    noise = torch.randn((B, C, H_hr, W_hr), device=device, dtype=dtype)\n",
    "    \n",
    "    def model_fn(x, t, y, cond_mask=None):\n",
    "        return model(x, t, y, cond_mask=cond_mask, disable_spatial_bias=disable_spatial_bias)\n",
    "    \n",
    "    samples = sampler(\n",
    "        model_fn,\n",
    "        noise,\n",
    "        condition,\n",
    "        uncondition,\n",
    "        cond_mask=cond_mask_hr,\n",
    "        x_cond=x_cond_hr,\n",
    "    )\n",
    "    \n",
    "    # Restore scale\n",
    "    model.decoder_patch_scaling_h = old_scale_h\n",
    "    model.decoder_patch_scaling_w = old_scale_w\n",
    "    \n",
    "    return samples, cond_mask_hr, x_cond_hr\n",
    "\n",
    "\n",
    "print(\"Super-resolution function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_superres_comparison(models_dict, images, labels, scale=4, sparsity=0.2, num_steps=200, guidance=2.0):\n",
    "    \"\"\"Compare super-resolution across model variants.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    images = images.to(DEVICE).to(DTYPE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    for name, model in tqdm(models_dict.items(), desc=\"SR Models\"):\n",
    "        # Try with and without spatial bias disabled\n",
    "        samples, cond_mask, x_cond = sample_superres(\n",
    "            model, images, labels,\n",
    "            scale=scale,\n",
    "            sparsity=sparsity,\n",
    "            num_steps=num_steps,\n",
    "            guidance=guidance,\n",
    "            disable_spatial_bias=False,  # Use Option B if available\n",
    "        )\n",
    "        \n",
    "        results[name] = {\n",
    "            \"samples\": samples.cpu(),\n",
    "            \"cond_mask\": cond_mask.cpu(),\n",
    "            \"x_cond\": x_cond.cpu(),\n",
    "        }\n",
    "        \n",
    "        # Also try with spatial bias disabled (for B variants)\n",
    "        if \"B\" in name or \"A+B\" in name:\n",
    "            samples_no_bias, _, _ = sample_superres(\n",
    "                model, images, labels,\n",
    "                scale=scale,\n",
    "                sparsity=sparsity,\n",
    "                num_steps=num_steps,\n",
    "                guidance=guidance,\n",
    "                disable_spatial_bias=True,\n",
    "            )\n",
    "            results[f\"{name}_no_bias\"] = {\n",
    "                \"samples\": samples_no_bias.cpu(),\n",
    "                \"cond_mask\": cond_mask.cpu(),\n",
    "                \"x_cond\": x_cond.cpu(),\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if models:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Super-Resolution Comparison (4x upscale)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    sr_results = run_superres_comparison(\n",
    "        models,\n",
    "        test_images[:4],  # Fewer samples for SR (memory)\n",
    "        test_labels[:4],\n",
    "        scale=4,\n",
    "        sparsity=0.2,\n",
    "        num_steps=200,\n",
    "        guidance=2.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_superres(results, gt_images, scale=4, num_show=4):\n",
    "    \"\"\"Visualize super-resolution results.\"\"\"\n",
    "    num_variants = len(results)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_variants + 2, 1, figsize=(16, 4 * (num_variants + 2)))\n",
    "    \n",
    "    # LR input\n",
    "    axes[0].imshow(tensor_to_image(gt_images[:num_show], nrow=num_show))\n",
    "    axes[0].set_title(f\"LR Input (32x32)\", fontsize=14)\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Bicubic upscale for reference\n",
    "    bicubic = F.interpolate(gt_images[:num_show], scale_factor=scale, mode=\"bicubic\", align_corners=False)\n",
    "    axes[1].imshow(tensor_to_image(bicubic, nrow=num_show))\n",
    "    axes[1].set_title(f\"Bicubic Upscale ({32*scale}x{32*scale})\", fontsize=14)\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    # Each variant\n",
    "    for idx, (name, data) in enumerate(results.items()):\n",
    "        samples = data[\"samples\"][:num_show]\n",
    "        \n",
    "        axes[idx + 2].imshow(tensor_to_image(samples, nrow=num_show))\n",
    "        axes[idx + 2].set_title(f\"{name} ({32*scale}x{32*scale})\", fontsize=14)\n",
    "        axes[idx + 2].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"superres_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if 'sr_results' in dir():\n",
    "    visualize_superres(sr_results, test_images[:4], scale=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary():\n",
    "    \"\"\"Print summary of all ablation results.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ABLATION STUDY SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if 'ablation_results' in dir() and ablation_results:\n",
    "        print(\"\\n1. OPTIONS A/B COMPARISON (sparsity=20%, steps=200, guidance=2.0)\")\n",
    "        print(\"-\" * 60)\n",
    "        for name, data in ablation_results.items():\n",
    "            m = data[\"metrics\"]\n",
    "            print(f\"  {name:15s}: PSNR = {m['psnr']:.2f} dB, MSE = {m['mse']:.4f}\")\n",
    "    \n",
    "    if 'steps_results' in dir() and steps_results:\n",
    "        print(\"\\n2. SAMPLING STEPS (A+B model)\")\n",
    "        print(\"-\" * 60)\n",
    "        for steps, data in sorted(steps_results.items()):\n",
    "            m = data[\"metrics\"]\n",
    "            print(f\"  Steps={steps:4d}: PSNR = {m['psnr']:.2f} dB\")\n",
    "    \n",
    "    if 'guidance_results' in dir() and guidance_results:\n",
    "        print(\"\\n3. GUIDANCE SCALE (A+B model)\")\n",
    "        print(\"-\" * 60)\n",
    "        for guidance, data in sorted(guidance_results.items()):\n",
    "            m = data[\"metrics\"]\n",
    "            print(f\"  Guidance={guidance:.1f}: PSNR = {m['psnr']:.2f} dB\")\n",
    "    \n",
    "    if 'sparsity_results' in dir() and sparsity_results:\n",
    "        print(\"\\n4. SPARSITY RATE (A+B model, trained with 20%)\")\n",
    "        print(\"-\" * 60)\n",
    "        for sparsity, data in sorted(sparsity_results.items()):\n",
    "            m = data[\"metrics\"]\n",
    "            marker = \" <-- training\" if abs(sparsity - 0.2) < 0.01 else \"\"\n",
    "            print(f\"  Sparsity={sparsity:5.0%}: PSNR = {m['psnr']:.2f} dB{marker}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Detailed Single-Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_single_image_analysis(model, image, label, sparsities=[0.1, 0.2, 0.4], num_steps=200, guidance=2.0):\n",
    "    \"\"\"\n",
    "    Detailed analysis of a single image across sparsity levels.\n",
    "    Shows: GT, conditioning mask, reconstruction, error map.\n",
    "    \"\"\"\n",
    "    image = image.unsqueeze(0).to(DEVICE).to(DTYPE)\n",
    "    label = label.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(sparsities), 4, figsize=(16, 4 * len(sparsities)))\n",
    "    \n",
    "    for idx, sparsity in enumerate(sparsities):\n",
    "        samples, cond_mask = sample_reconstruction(\n",
    "            model, image, label,\n",
    "            sparsity=sparsity,\n",
    "            num_steps=num_steps,\n",
    "            guidance=guidance,\n",
    "        )\n",
    "        \n",
    "        gt = image[0].cpu()\n",
    "        recon = samples[0].cpu()\n",
    "        mask = cond_mask[0].cpu()\n",
    "        \n",
    "        # Conditioned input visualization\n",
    "        cond_vis = gt * mask + (1 - mask) * 0.5\n",
    "        \n",
    "        # Error map (absolute difference)\n",
    "        error = (recon - gt).abs().mean(dim=0)  # Average over channels\n",
    "        \n",
    "        # Plot\n",
    "        axes[idx, 0].imshow(tensor_to_image(gt.unsqueeze(0), nrow=1))\n",
    "        axes[idx, 0].set_title(\"Ground Truth\")\n",
    "        axes[idx, 0].axis(\"off\")\n",
    "        \n",
    "        axes[idx, 1].imshow(tensor_to_image(cond_vis.unsqueeze(0), nrow=1))\n",
    "        axes[idx, 1].set_title(f\"Input ({sparsity:.0%} observed)\")\n",
    "        axes[idx, 1].axis(\"off\")\n",
    "        \n",
    "        axes[idx, 2].imshow(tensor_to_image(recon.unsqueeze(0), nrow=1))\n",
    "        axes[idx, 2].set_title(\"Reconstruction\")\n",
    "        axes[idx, 2].axis(\"off\")\n",
    "        \n",
    "        im = axes[idx, 3].imshow(error.numpy(), cmap=\"hot\", vmin=0, vmax=0.5)\n",
    "        axes[idx, 3].set_title(f\"Error Map (MAE={error.mean():.3f})\")\n",
    "        axes[idx, 3].axis(\"off\")\n",
    "        plt.colorbar(im, ax=axes[idx, 3], fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"detailed_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if \"A+B\" in models:\n",
    "    print(\"\\nDetailed Single-Image Analysis (A+B model)\")\n",
    "    print(\"=\"*60)\n",
    "    detailed_single_image_analysis(\n",
    "        models[\"A+B\"],\n",
    "        test_images[0],\n",
    "        test_labels[0],\n",
    "        sparsities=[0.1, 0.2, 0.4, 0.6],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluation notebook complete!\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - ablation_options_ab.png\")\n",
    "print(\"  - ablation_steps.png\")\n",
    "print(\"  - psnr_vs_steps.png\")\n",
    "print(\"  - ablation_guidance.png\")\n",
    "print(\"  - psnr_vs_guidance.png\")\n",
    "print(\"  - ablation_sparsity.png\")\n",
    "print(\"  - psnr_vs_sparsity.png\")\n",
    "print(\"  - superres_comparison.png\")\n",
    "print(\"  - detailed_analysis.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}