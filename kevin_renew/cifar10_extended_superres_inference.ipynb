{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Extended Boundaries Inference with Overlap Supervision\n",
    "\n",
    "This notebook demonstrates inference with `PixNerDiTExtended` trained using **proper overlap supervision**.\n",
    "\n",
    "## Key Innovation: Overlap Supervision\n",
    "\n",
    "Unlike simple coordinate rescaling, this model:\n",
    "1. **Predicts extended regions**: Each patch predicts beyond its boundary into neighbor territory\n",
    "2. **Uses real pixels**: Extended predictions supervised against actual neighbor pixel values\n",
    "3. **Enforces consistency**: Overlapping regions must match → adjacent patches predict same values\n",
    "4. **Blends during inference**: Overlapping predictions averaged for smooth output\n",
    "\n",
    "### Example: patch_size=2, margin=0.5\n",
    "- `margin_pixels = 1`, `extended_size = 4`\n",
    "- Each 2×2 core patch predicts a 4×4 region\n",
    "- The extra pixels come from neighboring patches\n",
    "- During inference, overlapping 4×4 predictions are blended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PIXNERD_DIR = NOTEBOOK_DIR / \"PixNerd\"\n",
    "\n",
    "if PIXNERD_DIR.exists():\n",
    "    os.chdir(PIXNERD_DIR)\n",
    "    sys.path.insert(0, str(PIXNERD_DIR))\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"ERROR: PixNerd directory not found at {PIXNERD_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Disable dynamo for compatibility\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Must match training config from `train_cifar10_extended.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# MODEL CONFIGURATION - Must match training!\n# =============================================================================\n\n# Checkpoint path (v2 = fresh training with overlap supervision)\nCHECKPOINT_PATH = str(NOTEBOOK_DIR / \"workdirs/exp_cifar10_extended_v2/checkpoints/last.ckpt\")\n\n# Model architecture (~10M params)\nPATCH_SIZE = 2\nHIDDEN_SIZE = 256\nDECODER_HIDDEN_SIZE = 32\nNUM_ENCODER_BLOCKS = 6\nNUM_DECODER_BLOCKS = 2\nNUM_GROUPS = 4\nNUM_CLASSES = 10\n\n# Extended boundary config - KEY PARAMETERS\nMARGIN = 0.5  # 0.5 = 1 pixel margin for patch_size=2\n\n# Compute extended sizes\nMARGIN_PIXELS = max(1, int(round(PATCH_SIZE * MARGIN)))\nEXTENDED_SIZE = PATCH_SIZE + 2 * MARGIN_PIXELS\nEFFECTIVE_MARGIN = MARGIN_PIXELS / PATCH_SIZE\n\nprint(\"Extended Boundary Configuration:\")\nprint(f\"  patch_size: {PATCH_SIZE}\")\nprint(f\"  margin: {MARGIN} → margin_pixels: {MARGIN_PIXELS}\")\nprint(f\"  extended_size: {EXTENDED_SIZE} (core {PATCH_SIZE} + {MARGIN_PIXELS} on each side)\")\nprint(f\"  effective_margin: {EFFECTIVE_MARGIN:.2f}\")\nprint()\nprint(\"How overlap works:\")\nprint(f\"  • Each {PATCH_SIZE}×{PATCH_SIZE} patch predicts {EXTENDED_SIZE}×{EXTENDED_SIZE} pixels\")\nprint(f\"  • Overlapping predictions are blended during inference\")\nprint(f\"  • Consistency enforced during training → smooth boundaries\")\n\n# Sampling config\nGUIDANCE_SCALE = 2.0\nNUM_SAMPLING_STEPS = 50\n\n# Device\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.autoencoder.pixel import PixelAE\n",
    "from src.models.conditioner.class_label import LabelConditioner\n",
    "from src.models.transformer.pixnerd_c2i_extended import PixNerDiTExtended\n",
    "from src.diffusion.flow_matching.scheduling import LinearScheduler\n",
    "from src.diffusion.flow_matching.sampling import EulerSampler, ode_step_fn\n",
    "from src.diffusion.base.guidance import simple_guidance_fn\n",
    "from src.lightning_model import LightningModel\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Build model with extended boundary architecture.\"\"\"\n",
    "    \n",
    "    scheduler = LinearScheduler()\n",
    "    vae = PixelAE(scale=1.0)\n",
    "    conditioner = LabelConditioner(num_classes=NUM_CLASSES)\n",
    "    \n",
    "    # Extended NerfEmbedder model - no jittering (overlap must be consistent)\n",
    "    denoiser = PixNerDiTExtended(\n",
    "        in_channels=3,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_groups=NUM_GROUPS,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        decoder_hidden_size=DECODER_HIDDEN_SIZE,\n",
    "        num_encoder_blocks=NUM_ENCODER_BLOCKS,\n",
    "        num_decoder_blocks=NUM_DECODER_BLOCKS,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        margin=MARGIN,  # Extended boundaries with overlap supervision\n",
    "    )\n",
    "    \n",
    "    sampler = EulerSampler(\n",
    "        num_steps=NUM_SAMPLING_STEPS,\n",
    "        guidance=GUIDANCE_SCALE,\n",
    "        guidance_interval_min=0.0,\n",
    "        guidance_interval_max=1.0,\n",
    "        scheduler=scheduler,\n",
    "        w_scheduler=LinearScheduler(),\n",
    "        guidance_fn=simple_guidance_fn,\n",
    "        step_fn=ode_step_fn,\n",
    "    )\n",
    "    \n",
    "    model = LightningModel(\n",
    "        vae=vae,\n",
    "        conditioner=conditioner,\n",
    "        denoiser=denoiser,\n",
    "        diffusion_trainer=None,\n",
    "        diffusion_sampler=sampler,\n",
    "        ema_tracker=None,\n",
    "        optimizer=None,\n",
    "        lr_scheduler=None,\n",
    "        eval_original_model=False,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and load\n",
    "print(\"Building model...\")\n",
    "model = build_model()\n",
    "\n",
    "print(f\"\\nLoading checkpoint: {CHECKPOINT_PATH}\")\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    print(\"Checkpoint loaded successfully!\")\n",
    "else:\n",
    "    print(f\"WARNING: Checkpoint not found at {CHECKPOINT_PATH}\")\n",
    "    print(\"Model will use random weights (for testing notebook structure)\")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_CLASSES = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "print(\"CIFAR-10 Classes:\")\n",
    "for i, name in enumerate(CIFAR10_CLASSES):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Function with Super-Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef generate_images(\n    model,\n    class_labels,\n    output_size=32,\n    guidance_scale=None,\n    num_steps=None,\n    device=DEVICE,\n):\n    \"\"\"\n    Generate images with optional super-resolution.\n    \n    The extended boundary model naturally supports arbitrary output sizes\n    because it predicts pixel values for continuous positions.\n    \n    Args:\n        model: PixNerDiTExtended model\n        class_labels: List of class indices [0-9]\n        output_size: Output resolution (32 for native, higher for super-res)\n        guidance_scale: CFG scale (None = use default)\n        num_steps: Sampling steps (None = use default)\n    \n    Returns:\n        images: numpy array [N, H, W, 3] in [0, 1]\n    \"\"\"\n    model.eval()\n    \n    # Prepare labels\n    if isinstance(class_labels, int):\n        class_labels = [class_labels]\n    \n    batch_size = len(class_labels)\n    labels = class_labels  # Keep as list, conditioner handles conversion\n    \n    # Compute super-resolution scaling\n    base_size = 32  # CIFAR-10 native resolution\n    scale_factor = output_size / base_size\n    \n    # Set decoder scaling for super-resolution\n    model.ema_denoiser.decoder_patch_scaling_h = scale_factor\n    model.ema_denoiser.decoder_patch_scaling_w = scale_factor\n    \n    # Update sampler if needed\n    if guidance_scale is not None:\n        model.diffusion_sampler.guidance = guidance_scale\n    if num_steps is not None:\n        model.diffusion_sampler.num_steps = num_steps\n    \n    # Prepare conditioning - returns BOTH condition and uncondition\n    condition, uncondition = model.conditioner(labels)\n    \n    # Create noise tensor (sampler expects tensor, not shape)\n    noise = torch.randn(batch_size, 3, output_size, output_size, device=device)\n    \n    print(f\"Generating {batch_size} images at {output_size}×{output_size} (scale: {scale_factor}x)...\")\n    \n    # Sample\n    samples = model.diffusion_sampler(\n        model.ema_denoiser,\n        noise,\n        condition,\n        uncondition,\n    )\n    \n    # Decode (identity for pixel space)\n    images = model.vae.decode(samples)\n    \n    # Convert to numpy [0, 1]\n    images = images.clamp(0, 1).cpu().numpy()\n    images = images.transpose(0, 2, 3, 1)  # [N, H, W, C]\n    \n    # Reset scaling\n    model.ema_denoiser.decoder_patch_scaling_h = 1.0\n    model.ema_denoiser.decoder_patch_scaling_w = 1.0\n    \n    return images"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Native Resolution (32×32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one image per class at native resolution\n",
    "class_labels = list(range(10))\n",
    "\n",
    "images_32 = generate_images(model, class_labels, output_size=32)\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, (ax, img) in enumerate(zip(axes.flat, images_32)):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{i}: {CIFAR10_CLASSES[i]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Native Resolution (32×32)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super-Resolution Comparison\n",
    "\n",
    "Compare the extended boundary model's super-resolution quality with bilinear upscaling.\n",
    "\n",
    "The overlap supervision should produce smoother boundaries between patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_superres(model, class_idx, scales=[1, 2, 4]):\n",
    "    \"\"\"\n",
    "    Compare super-resolution at different scales.\n",
    "    \n",
    "    Shows:\n",
    "    - Model output at each scale\n",
    "    - Bilinear upscaling of native resolution for comparison\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(len(scales), 3, figsize=(12, 4 * len(scales)))\n",
    "    \n",
    "    # Generate native resolution first\n",
    "    img_native = generate_images(model, [class_idx], output_size=32)[0]\n",
    "    \n",
    "    for row, scale in enumerate(scales):\n",
    "        output_size = 32 * scale\n",
    "        \n",
    "        # Model super-resolution\n",
    "        if scale == 1:\n",
    "            img_model = img_native\n",
    "        else:\n",
    "            img_model = generate_images(model, [class_idx], output_size=output_size)[0]\n",
    "        \n",
    "        # Bilinear upscaling\n",
    "        img_bilinear = np.array(Image.fromarray(\n",
    "            (img_native * 255).astype(np.uint8)\n",
    "        ).resize((output_size, output_size), Image.BILINEAR)) / 255.0\n",
    "        \n",
    "        # Display\n",
    "        axes[row, 0].imshow(img_model)\n",
    "        axes[row, 0].set_title(f\"Extended Model ({output_size}×{output_size})\")\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        axes[row, 1].imshow(img_bilinear)\n",
    "        axes[row, 1].set_title(f\"Bilinear ({output_size}×{output_size})\")\n",
    "        axes[row, 1].axis('off')\n",
    "        \n",
    "        # Difference (amplified for visibility)\n",
    "        if scale > 1:\n",
    "            diff = np.abs(img_model - img_bilinear)\n",
    "            diff_amplified = np.clip(diff * 5, 0, 1)  # Amplify differences\n",
    "            axes[row, 2].imshow(diff_amplified)\n",
    "            axes[row, 2].set_title(f\"Difference (5x amplified)\")\n",
    "        else:\n",
    "            axes[row, 2].text(0.5, 0.5, \"N/A\", ha='center', va='center', transform=axes[row, 2].transAxes)\n",
    "            axes[row, 2].set_title(\"Difference\")\n",
    "        axes[row, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Super-Resolution: {CIFAR10_CLASSES[class_idx]}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare for a few classes\n",
    "for class_idx in [3, 5, 8]:  # cat, dog, ship\n",
    "    compare_superres(model, class_idx, scales=[1, 2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary Analysis\n",
    "\n",
    "Analyze whether the overlap supervision produces smoother patch boundaries.\n",
    "\n",
    "We can visualize potential boundary artifacts by looking at the gradient magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_boundaries(model, class_idx, output_size=128):\n",
    "    \"\"\"\n",
    "    Analyze boundary smoothness using gradient analysis.\n",
    "    \n",
    "    If overlap supervision works well, we should NOT see grid patterns\n",
    "    in the gradient magnitude at patch boundaries.\n",
    "    \"\"\"\n",
    "    # Generate high-res image\n",
    "    img = generate_images(model, [class_idx], output_size=output_size)[0]\n",
    "    \n",
    "    # Convert to grayscale for gradient analysis\n",
    "    gray = np.mean(img, axis=2)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad_x = np.abs(np.diff(gray, axis=1))\n",
    "    grad_y = np.abs(np.diff(gray, axis=0))\n",
    "    \n",
    "    # Gradient magnitude (pad to same size)\n",
    "    grad_x_pad = np.pad(grad_x, ((0, 0), (0, 1)), mode='edge')\n",
    "    grad_y_pad = np.pad(grad_y, ((0, 1), (0, 0)), mode='edge')\n",
    "    grad_mag = np.sqrt(grad_x_pad**2 + grad_y_pad**2)\n",
    "    \n",
    "    # Expected patch boundaries (based on patch_size=2, scaled up)\n",
    "    scale = output_size // 32\n",
    "    patch_size_scaled = PATCH_SIZE * scale\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f\"Generated Image ({output_size}×{output_size})\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Gradient magnitude\n",
    "    im = axes[1].imshow(grad_mag, cmap='hot')\n",
    "    axes[1].set_title(\"Gradient Magnitude\")\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1], fraction=0.046)\n",
    "    \n",
    "    # Gradient with patch grid overlay\n",
    "    axes[2].imshow(grad_mag, cmap='hot')\n",
    "    # Draw patch boundaries\n",
    "    for i in range(0, output_size, patch_size_scaled):\n",
    "        axes[2].axhline(y=i, color='cyan', linewidth=0.5, alpha=0.5)\n",
    "        axes[2].axvline(x=i, color='cyan', linewidth=0.5, alpha=0.5)\n",
    "    axes[2].set_title(f\"Gradient with Patch Grid (patch={patch_size_scaled}px)\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Boundary Analysis: {CIFAR10_CLASSES[class_idx]}\\n(If overlap works, grid lines should NOT align with high gradients)\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quantitative analysis: gradient at boundaries vs non-boundaries\n",
    "    boundary_mask = np.zeros_like(grad_mag, dtype=bool)\n",
    "    for i in range(0, output_size, patch_size_scaled):\n",
    "        if i > 0 and i < output_size:\n",
    "            boundary_mask[max(0,i-1):min(output_size,i+2), :] = True\n",
    "            boundary_mask[:, max(0,i-1):min(output_size,i+2)] = True\n",
    "    \n",
    "    grad_at_boundaries = grad_mag[boundary_mask].mean()\n",
    "    grad_elsewhere = grad_mag[~boundary_mask].mean()\n",
    "    \n",
    "    print(f\"Average gradient at patch boundaries: {grad_at_boundaries:.4f}\")\n",
    "    print(f\"Average gradient elsewhere: {grad_elsewhere:.4f}\")\n",
    "    print(f\"Ratio (lower is better): {grad_at_boundaries / grad_elsewhere:.2f}\")\n",
    "\n",
    "# Analyze boundaries for a few classes\n",
    "for class_idx in [0, 5, 9]:  # airplane, dog, truck\n",
    "    analyze_boundaries(model, class_idx, output_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Super-Resolution Test\n",
    "\n",
    "Test higher resolution outputs to see how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test extreme super-resolution\n",
    "class_idx = 7  # horse\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "scales = [1, 4, 8, 16]\n",
    "\n",
    "for ax, scale in zip(axes, scales):\n",
    "    output_size = 32 * scale\n",
    "    img = generate_images(model, [class_idx], output_size=output_size)[0]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{output_size}×{output_size} ({scale}x)\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f\"Extreme Super-Resolution: {CIFAR10_CLASSES[class_idx]}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generation at High Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all classes at 4x resolution (128×128)\n",
    "images_128 = generate_images(model, list(range(10)), output_size=128)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, (ax, img) in enumerate(zip(axes.flat, images_128)):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{i}: {CIFAR10_CLASSES[i]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"4× Super-Resolution (128×128)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grid(class_idx, num_samples=4, output_size=64, guidance=2.0):\n",
    "    \"\"\"Generate a grid of samples for a single class.\"\"\"\n",
    "    images = generate_images(\n",
    "        model, \n",
    "        [class_idx] * num_samples, \n",
    "        output_size=output_size,\n",
    "        guidance_scale=guidance\n",
    "    )\n",
    "    \n",
    "    cols = min(4, num_samples)\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    if num_samples == 1:\n",
    "        axes = [[axes]]\n",
    "    elif rows == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        ax = axes[i // cols][i % cols]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"{CIFAR10_CLASSES[class_idx]} @ {output_size}×{output_size} (CFG={guidance})\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate samples\n",
    "generate_grid(class_idx=3, num_samples=4, output_size=128, guidance=2.0)  # cat\n",
    "generate_grid(class_idx=1, num_samples=4, output_size=128, guidance=3.0)  # automobile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the **Extended Boundaries with Overlap Supervision** model:\n",
    "\n",
    "### Key Features:\n",
    "1. **Overlap Supervision**: Each patch predicts extended regions, overlapping predictions are supervised against ground truth\n",
    "2. **Consistency**: Adjacent patches must predict same values in shared regions\n",
    "3. **Blending**: During inference, overlapping predictions are averaged for smooth output\n",
    "4. **No Jittering**: Position jittering disabled to enforce exact overlap matching\n",
    "\n",
    "### Configuration:\n",
    "- `patch_size=2`, `margin=0.5`\n",
    "- `margin_pixels=1`, `extended_size=4`\n",
    "- Each 2×2 core patch predicts a 4×4 region\n",
    "\n",
    "### Expected Benefits:\n",
    "- Smoother boundaries between patches during super-resolution\n",
    "- Better consistency in overlapping regions\n",
    "- Reduced checkerboard/grid artifacts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}