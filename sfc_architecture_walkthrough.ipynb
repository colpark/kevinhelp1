{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFC Encoder Architecture Walkthrough\n",
    "\n",
    "This notebook provides a **self-contained, step-by-step walkthrough** of the Space-Filling Curve (SFC) encoder architecture for sparse-conditioned image generation.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Space-Filling Curves: Hilbert vs Z-order](#1-space-filling-curves)\n",
    "2. [SFC Tokenization: Sparse Pixels → Tokens](#2-sfc-tokenization)\n",
    "3. [Coordinate Embeddings (Option A: Unified Coords)](#3-coordinate-embeddings)\n",
    "4. [Cross-Attention with Spatial Bias (Option B)](#4-cross-attention)\n",
    "5. [Full Forward Pass](#5-full-forward-pass)\n",
    "6. [Ablation Comparison](#6-ablation-comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional, Literal\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Space-Filling Curves <a name=\"1-space-filling-curves\"></a>\n",
    "\n",
    "Space-filling curves map 2D coordinates to a 1D index while preserving spatial locality.\n",
    "\n",
    "### Why use SFC for tokenization?\n",
    "- **Locality preservation**: Nearby pixels in 2D remain nearby in the 1D sequence\n",
    "- **Efficient for sparse data**: Only observed pixels need to be tokenized\n",
    "- **Better than raster scan**: Raster ordering has poor locality (jumping across rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SPACE-FILLING CURVE IMPLEMENTATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def xy_to_zorder(x: int, y: int, bits: int = 5) -> int:\n",
    "    \"\"\"\n",
    "    Convert (x, y) to Z-order (Morton code) index.\n",
    "    Z-order interleaves the bits of x and y.\n",
    "    \"\"\"\n",
    "    z = 0\n",
    "    for i in range(bits):\n",
    "        z |= ((x & (1 << i)) << i) | ((y & (1 << i)) << (i + 1))\n",
    "    return z\n",
    "\n",
    "\n",
    "def xy_to_hilbert(x: int, y: int, order: int = 5) -> int:\n",
    "    \"\"\"\n",
    "    Convert (x, y) to Hilbert curve index.\n",
    "    Hilbert has better locality: consecutive indices are always adjacent in 2D.\n",
    "    \"\"\"\n",
    "    n = 1 << order\n",
    "    d = 0\n",
    "    s = n >> 1\n",
    "    while s > 0:\n",
    "        rx = 1 if (x & s) > 0 else 0\n",
    "        ry = 1 if (y & s) > 0 else 0\n",
    "        d += s * s * ((3 * rx) ^ ry)\n",
    "        if ry == 0:\n",
    "            if rx == 1:\n",
    "                x = s - 1 - x\n",
    "                y = s - 1 - y\n",
    "            x, y = y, x\n",
    "        s >>= 1\n",
    "    return d\n",
    "\n",
    "\n",
    "def visualize_sfc(size: int = 8, curve: str = \"hilbert\"):\n",
    "    \"\"\"Visualize the space-filling curve ordering.\"\"\"\n",
    "    bits = int(np.log2(size))\n",
    "    \n",
    "    # Compute SFC index for each pixel\n",
    "    sfc_indices = np.zeros((size, size), dtype=int)\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            if curve == \"hilbert\":\n",
    "                sfc_indices[y, x] = xy_to_hilbert(x, y, order=bits)\n",
    "            else:\n",
    "                sfc_indices[y, x] = xy_to_zorder(x, y, bits=bits)\n",
    "    \n",
    "    # Create path coordinates\n",
    "    path = [(0, 0)] * (size * size)\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            idx = sfc_indices[y, x]\n",
    "            path[idx] = (x + 0.5, y + 0.5)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Left: Index grid\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(sfc_indices, cmap='viridis')\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            ax.text(x, y, str(sfc_indices[y, x]), ha='center', va='center', \n",
    "                   fontsize=8, color='white' if sfc_indices[y, x] < size*size/2 else 'black')\n",
    "    ax.set_title(f'{curve.capitalize()} Curve Index Grid ({size}x{size})')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.colorbar(im, ax=ax, label='SFC Index')\n",
    "    \n",
    "    # Right: Path visualization\n",
    "    ax = axes[1]\n",
    "    path_x, path_y = zip(*path)\n",
    "    ax.plot(path_x, path_y, 'b-', linewidth=1, alpha=0.7)\n",
    "    ax.scatter(path_x, path_y, c=range(len(path)), cmap='viridis', s=50, zorder=5)\n",
    "    ax.set_xlim(0, size)\n",
    "    ax.set_ylim(0, size)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f'{curve.capitalize()} Curve Path')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return sfc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Hilbert curve\n",
    "print(\"HILBERT CURVE - Better locality (consecutive indices always adjacent)\")\n",
    "hilbert_indices = visualize_sfc(size=8, curve=\"hilbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Z-order curve\n",
    "print(\"Z-ORDER CURVE - Simpler but has jumps (e.g., index 7 to 8)\")\n",
    "zorder_indices = visualize_sfc(size=8, curve=\"zorder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observation\n",
    "\n",
    "Notice how:\n",
    "- **Hilbert**: Every consecutive pair of indices is spatially adjacent\n",
    "- **Z-order**: Has \"jumps\" (e.g., 7→8 jumps across the grid)\n",
    "\n",
    "For attention mechanisms, Hilbert's better locality means tokens grouped together are more likely to be spatially related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. SFC Tokenization: Sparse Pixels → Tokens <a name=\"2-sfc-tokenization\"></a>\n",
    "\n",
    "The SFC tokenizer converts sparse pixels into a sequence of tokens:\n",
    "\n",
    "1. **Order all pixels** by SFC (Hilbert/Z-order)\n",
    "2. **Select observed pixels** (where `cond_mask=1`)\n",
    "3. **Group consecutive pixels** into groups of `g` (default 8)\n",
    "4. **Concatenate features** within each group\n",
    "5. **Project to hidden_size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_sfc_order(size: int, curve: str = \"hilbert\"):\n",
    "    \"\"\"\n",
    "    Precompute SFC ordering and coordinate grids.\n",
    "    \n",
    "    Returns:\n",
    "        order_flat: (HW,) indices sorted by SFC order\n",
    "        coords_flat: (HW, 2) normalized coordinates in [-1, 1]\n",
    "        sfc_pos_flat: (HW, 1) normalized SFC position in [0, 1]\n",
    "    \"\"\"\n",
    "    bits = int(np.log2(size))\n",
    "    \n",
    "    # Compute (sfc_idx, flat_idx) pairs\n",
    "    pairs = []\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            if curve == \"hilbert\":\n",
    "                sfc = xy_to_hilbert(x, y, order=bits)\n",
    "            else:\n",
    "                sfc = xy_to_zorder(x, y, bits=bits)\n",
    "            flat = y * size + x\n",
    "            pairs.append((sfc, flat))\n",
    "    \n",
    "    pairs.sort(key=lambda t: t[0])\n",
    "    order_flat = torch.tensor([flat for _, flat in pairs], dtype=torch.long)\n",
    "    \n",
    "    # Normalized coordinates in [-1, 1]\n",
    "    xs = torch.arange(size, dtype=torch.float32)\n",
    "    ys = torch.arange(size, dtype=torch.float32)\n",
    "    yy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n",
    "    x_norm = (xx / (size - 1)) * 2.0 - 1.0\n",
    "    y_norm = (yy / (size - 1)) * 2.0 - 1.0\n",
    "    coords_flat = torch.stack([x_norm, y_norm], dim=-1).reshape(-1, 2)\n",
    "    \n",
    "    # Normalized SFC position in [0, 1]\n",
    "    inv = torch.empty_like(order_flat)\n",
    "    inv[order_flat] = torch.arange(order_flat.numel(), dtype=torch.long)\n",
    "    sfc_pos_flat = inv.float().unsqueeze(-1) / max(order_flat.numel() - 1, 1)\n",
    "    \n",
    "    return order_flat, coords_flat, sfc_pos_flat\n",
    "\n",
    "\n",
    "# Demonstrate SFC ordering\n",
    "size = 8\n",
    "order_flat, coords_flat, sfc_pos_flat = precompute_sfc_order(size, \"hilbert\")\n",
    "\n",
    "print(f\"Image size: {size}x{size} = {size*size} pixels\")\n",
    "print(f\"\\nSFC order (flat indices sorted by Hilbert index):\")\n",
    "print(f\"First 16: {order_flat[:16].tolist()}\")\n",
    "print(f\"\\nCoordinates (first 4 in SFC order):\")\n",
    "for i in range(4):\n",
    "    flat_idx = order_flat[i].item()\n",
    "    x, y = flat_idx % size, flat_idx // size\n",
    "    coord = coords_flat[flat_idx]\n",
    "    print(f\"  SFC idx {i}: pixel ({x}, {y}), normalized coord ({coord[0]:.2f}, {coord[1]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_sparse_tokenization(size=8, sparsity=0.4, group_size=4):\n",
    "    \"\"\"\n",
    "    Demonstrate how sparse pixels are tokenized via SFC.\n",
    "    \"\"\"\n",
    "    # Create random sparse mask\n",
    "    mask = torch.rand(size, size) < sparsity\n",
    "    num_observed = mask.sum().item()\n",
    "    \n",
    "    # Get SFC ordering\n",
    "    order_flat, coords_flat, sfc_pos_flat = precompute_sfc_order(size, \"hilbert\")\n",
    "    \n",
    "    # Order mask by SFC\n",
    "    mask_flat = mask.reshape(-1)\n",
    "    mask_ord = mask_flat[order_flat]\n",
    "    \n",
    "    # Select observed pixels in SFC order\n",
    "    observed_indices = torch.nonzero(mask_ord, as_tuple=False).squeeze(1)\n",
    "    \n",
    "    # Group into tokens\n",
    "    num_tokens = int(np.ceil(len(observed_indices) / group_size))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Left: Sparse mask\n",
    "    ax = axes[0]\n",
    "    ax.imshow(mask.float(), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Sparse Mask ({sparsity*100:.0f}% observed)\\n{num_observed} pixels')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    \n",
    "    # Middle: SFC ordering with observed pixels highlighted\n",
    "    ax = axes[1]\n",
    "    sfc_indices = np.zeros((size, size))\n",
    "    bits = int(np.log2(size))\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            sfc_indices[y, x] = xy_to_hilbert(x, y, order=bits)\n",
    "    \n",
    "    ax.imshow(sfc_indices, cmap='viridis', alpha=0.3)\n",
    "    # Highlight observed pixels\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            if mask[y, x]:\n",
    "                ax.scatter(x, y, c='red', s=100, marker='o', edgecolors='white', linewidths=2)\n",
    "                ax.text(x, y, str(int(sfc_indices[y, x])), ha='center', va='center', \n",
    "                       fontsize=7, color='white', fontweight='bold')\n",
    "    ax.set_title('Observed Pixels with SFC Index')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    \n",
    "    # Right: Token grouping\n",
    "    ax = axes[2]\n",
    "    token_colors = plt.cm.tab10(np.linspace(0, 1, num_tokens))\n",
    "    \n",
    "    for token_idx in range(num_tokens):\n",
    "        start = token_idx * group_size\n",
    "        end = min(start + group_size, len(observed_indices))\n",
    "        \n",
    "        for i in range(start, end):\n",
    "            sfc_order_idx = observed_indices[i].item()\n",
    "            flat_idx = order_flat[sfc_order_idx].item()\n",
    "            x, y = flat_idx % size, flat_idx // size\n",
    "            ax.scatter(x, y, c=[token_colors[token_idx]], s=150, marker='s', \n",
    "                      edgecolors='black', linewidths=1)\n",
    "            ax.text(x, y, f'T{token_idx}', ha='center', va='center', \n",
    "                   fontsize=6, color='white', fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(-0.5, size-0.5)\n",
    "    ax.set_ylim(-0.5, size-0.5)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f'Token Grouping (g={group_size})\\n{num_tokens} tokens')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTokenization Summary:\")\n",
    "    print(f\"  Input: {size}x{size} = {size*size} pixels\")\n",
    "    print(f\"  Observed: {num_observed} pixels ({num_observed/size/size*100:.1f}%)\")\n",
    "    print(f\"  Group size: {group_size}\")\n",
    "    print(f\"  Output: {num_tokens} tokens\")\n",
    "    print(f\"  Compression: {size*size} → {num_tokens} ({num_tokens/size/size*100:.1f}% of dense)\")\n",
    "\n",
    "demonstrate_sparse_tokenization(size=8, sparsity=0.4, group_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "Notice how spatially nearby observed pixels tend to be in the same token (same color). This is because:\n",
    "1. Hilbert curve preserves locality\n",
    "2. Consecutive observed pixels (in SFC order) are grouped together\n",
    "\n",
    "This means each token contains **spatially coherent** information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Coordinate Embeddings (Option A: Unified Coords) <a name=\"3-coordinate-embeddings\"></a>\n",
    "\n",
    "Both tokens and queries need positional information. The question is: should they use the **same** embedding function?\n",
    "\n",
    "### Baseline: Separate Embeddings\n",
    "- **Tokens**: `LearnableFourierMLP` (Fourier features → 2-layer MLP)\n",
    "- **Queries**: `FourierFeatures` (Fourier features → single linear)\n",
    "\n",
    "### Option A: Unified Embeddings\n",
    "- **Both**: Share the same `LearnableFourierMLP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierFeatures(nn.Module):\n",
    "    \"\"\"Fixed Fourier features with single linear projection (used for queries in baseline).\"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, n_bands: int = 16):\n",
    "        super().__init__()\n",
    "        self.n_bands = n_bands\n",
    "        freqs = (2.0 ** torch.arange(n_bands)) * np.pi\n",
    "        self.register_buffer(\"freqs\", freqs)\n",
    "        \n",
    "        fourier_dim = 4 * n_bands  # sin/cos for x and y\n",
    "        self.proj = nn.Linear(fourier_dim, out_features)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        # coords: (N, 2)\n",
    "        x, y = coords[:, 0:1], coords[:, 1:2]  # (N, 1)\n",
    "        xw = x * self.freqs  # (N, n_bands)\n",
    "        yw = y * self.freqs\n",
    "        fourier = torch.cat([torch.sin(xw), torch.cos(xw), \n",
    "                            torch.sin(yw), torch.cos(yw)], dim=-1)  # (N, 4*n_bands)\n",
    "        return self.proj(fourier)\n",
    "\n",
    "\n",
    "class LearnableFourierMLP(nn.Module):\n",
    "    \"\"\"Fourier features with learnable MLP projection (used for tokens, and queries in Option A).\"\"\"\n",
    "    def __init__(self, out_features: int, n_bands: int = 16, hidden_dim: int = None, mlp_layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.n_bands = n_bands\n",
    "        freqs = (2.0 ** torch.arange(n_bands)) * np.pi\n",
    "        self.register_buffer(\"freqs\", freqs)\n",
    "        \n",
    "        fourier_dim = 4 * n_bands\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = max(out_features, fourier_dim)\n",
    "        \n",
    "        # 2-layer MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(fourier_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, out_features),\n",
    "        )\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        # coords: (N, 2)\n",
    "        proj = coords.unsqueeze(-1) * self.freqs  # (N, 2, n_bands)\n",
    "        fourier = torch.cat([torch.sin(proj), torch.cos(proj)], dim=-1)  # (N, 2, 2*n_bands)\n",
    "        fourier = fourier.reshape(coords.shape[0], -1)  # (N, 4*n_bands)\n",
    "        return self.mlp(fourier)\n",
    "\n",
    "\n",
    "# Compare the two embedding types\n",
    "hidden_size = 64\n",
    "n_bands = 8\n",
    "\n",
    "fourier_simple = FourierFeatures(2, hidden_size, n_bands=n_bands)\n",
    "fourier_mlp = LearnableFourierMLP(hidden_size, n_bands=n_bands)\n",
    "\n",
    "# Test coordinates\n",
    "test_coords = torch.tensor([\n",
    "    [-1.0, -1.0],  # top-left\n",
    "    [0.0, 0.0],    # center\n",
    "    [1.0, 1.0],    # bottom-right\n",
    "    [0.5, -0.5],   # somewhere\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    embed_simple = fourier_simple(test_coords)\n",
    "    embed_mlp = fourier_mlp(test_coords)\n",
    "\n",
    "print(\"Coordinate Embedding Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nInput coordinates shape: {test_coords.shape}\")\n",
    "print(f\"FourierFeatures output shape: {embed_simple.shape}\")\n",
    "print(f\"LearnableFourierMLP output shape: {embed_mlp.shape}\")\n",
    "\n",
    "# Parameter count\n",
    "params_simple = sum(p.numel() for p in fourier_simple.parameters())\n",
    "params_mlp = sum(p.numel() for p in fourier_mlp.parameters())\n",
    "print(f\"\\nFourierFeatures parameters: {params_simple:,}\")\n",
    "print(f\"LearnableFourierMLP parameters: {params_mlp:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embedding_similarity(embed_fn, title, grid_size=16):\n",
    "    \"\"\"\n",
    "    Visualize how coordinate embeddings relate to each other.\n",
    "    Shows cosine similarity between center point and all other points.\n",
    "    \"\"\"\n",
    "    # Create coordinate grid\n",
    "    xs = torch.linspace(-1, 1, grid_size)\n",
    "    ys = torch.linspace(-1, 1, grid_size)\n",
    "    yy, xx = torch.meshgrid(ys, xs, indexing='ij')\n",
    "    coords = torch.stack([xx, yy], dim=-1).reshape(-1, 2)  # (grid_size^2, 2)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = embed_fn(coords)  # (grid_size^2, hidden_size)\n",
    "        embeddings = F.normalize(embeddings, dim=-1)  # Normalize for cosine similarity\n",
    "    \n",
    "    # Compute similarity to center point\n",
    "    center_idx = grid_size * grid_size // 2 + grid_size // 2\n",
    "    center_embed = embeddings[center_idx:center_idx+1]  # (1, hidden_size)\n",
    "    \n",
    "    similarity = (embeddings @ center_embed.T).squeeze()  # (grid_size^2,)\n",
    "    similarity_grid = similarity.reshape(grid_size, grid_size)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(similarity_grid, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    ax.scatter(grid_size//2, grid_size//2, c='green', s=100, marker='*', \n",
    "              edgecolors='white', linewidths=2, label='Reference point')\n",
    "    ax.set_title(f'{title}\\nCosine Similarity to Center')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.colorbar(im, ax=ax, label='Cosine Similarity')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"FourierFeatures (single linear projection):\")\n",
    "visualize_embedding_similarity(fourier_simple, \"FourierFeatures\")\n",
    "\n",
    "print(\"\\nLearnableFourierMLP (2-layer MLP):\")\n",
    "visualize_embedding_similarity(fourier_mlp, \"LearnableFourierMLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Why Unified Coordinates Matter\n",
    "\n",
    "In the **baseline**, cross-attention computes:\n",
    "```\n",
    "Q = W_q @ FourierFeatures(query_coords)      # Different embedding\n",
    "K = W_k @ LearnableFourierMLP(token_coords)  # Different embedding\n",
    "attention = softmax(Q @ K.T)\n",
    "```\n",
    "\n",
    "The Q/K projections must learn to **align two different coordinate representations**.\n",
    "\n",
    "With **Option A** (unified):\n",
    "```\n",
    "shared_embed = LearnableFourierMLP\n",
    "Q = W_q @ shared_embed(query_coords)   # Same embedding\n",
    "K = W_k @ shared_embed(token_coords)   # Same embedding\n",
    "```\n",
    "\n",
    "Now both operate in the **same representation space**, making alignment easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Cross-Attention with Spatial Bias (Option B) <a name=\"4-cross-attention\"></a>\n",
    "\n",
    "Cross-attention allows patch queries to gather information from sparse tokens.\n",
    "\n",
    "### Baseline Cross-Attention\n",
    "```\n",
    "attention_scores = Q @ K.T / sqrt(d)\n",
    "attention_weights = softmax(attention_scores)\n",
    "output = attention_weights @ V\n",
    "```\n",
    "\n",
    "The model must **learn** that spatially nearby tokens are more relevant.\n",
    "\n",
    "### Option B: Spatial Bias\n",
    "```\n",
    "dist = cdist(query_coords, token_coords)  # Pairwise distances\n",
    "spatial_bias = -scale * dist + offset     # Learnable per head\n",
    "attention_scores = Q @ K.T / sqrt(d) + spatial_bias  # Add bias!\n",
    "```\n",
    "\n",
    "Now **closer tokens automatically get higher attention scores**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_spatial_bias(num_queries=4, num_tokens=12, num_heads=4):\n",
    "    \"\"\"\n",
    "    Demonstrate how spatial bias affects attention patterns.\n",
    "    \"\"\"\n",
    "    hidden_size = 64\n",
    "    head_dim = hidden_size // num_heads\n",
    "    \n",
    "    # Random query positions (patch centers)\n",
    "    query_coords = torch.tensor([\n",
    "        [-0.5, -0.5],  # top-left patch\n",
    "        [0.5, -0.5],   # top-right patch\n",
    "        [-0.5, 0.5],   # bottom-left patch\n",
    "        [0.5, 0.5],    # bottom-right patch\n",
    "    ])\n",
    "    \n",
    "    # Random token positions (sparse observed pixels)\n",
    "    torch.manual_seed(42)\n",
    "    token_coords = torch.rand(num_tokens, 2) * 2 - 1  # Random in [-1, 1]\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    dist = torch.cdist(query_coords, token_coords, p=2)  # (Q, T)\n",
    "    \n",
    "    # Simulate Q, K with random values\n",
    "    Q = torch.randn(num_queries, num_heads, head_dim)\n",
    "    K = torch.randn(num_tokens, num_heads, head_dim)\n",
    "    \n",
    "    # Baseline attention (no spatial bias)\n",
    "    attn_scores_baseline = torch.einsum('qhd,thd->hqt', Q, K) / np.sqrt(head_dim)\n",
    "    attn_weights_baseline = F.softmax(attn_scores_baseline, dim=-1)\n",
    "    \n",
    "    # Option B: With spatial bias\n",
    "    scale = torch.tensor([1.0, 2.0, 3.0, 5.0])  # Different scales per head\n",
    "    offset = torch.zeros(num_heads)\n",
    "    \n",
    "    # spatial_bias: (num_heads, Q, T)\n",
    "    spatial_bias = -scale.view(-1, 1, 1) * dist.unsqueeze(0) + offset.view(-1, 1, 1)\n",
    "    \n",
    "    attn_scores_with_bias = attn_scores_baseline + spatial_bias\n",
    "    attn_weights_with_bias = F.softmax(attn_scores_with_bias, dim=-1)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Top row: Spatial layout and distance matrix\n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(token_coords[:, 0], token_coords[:, 1], c='blue', s=100, \n",
    "              marker='o', label='Tokens', edgecolors='black')\n",
    "    ax.scatter(query_coords[:, 0], query_coords[:, 1], c='red', s=200, \n",
    "              marker='s', label='Query patches', edgecolors='black')\n",
    "    for i, (x, y) in enumerate(query_coords):\n",
    "        ax.annotate(f'Q{i}', (x, y), fontsize=10, ha='center', va='center', color='white', fontweight='bold')\n",
    "    for i, (x, y) in enumerate(token_coords):\n",
    "        ax.annotate(f'T{i}', (x, y), fontsize=8, ha='center', va='center', color='white')\n",
    "    ax.set_xlim(-1.2, 1.2)\n",
    "    ax.set_ylim(-1.2, 1.2)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('Spatial Layout')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    im = ax.imshow(dist, cmap='viridis')\n",
    "    ax.set_title('Distance Matrix (Q × T)')\n",
    "    ax.set_xlabel('Token')\n",
    "    ax.set_ylabel('Query')\n",
    "    ax.set_xticks(range(num_tokens))\n",
    "    ax.set_yticks(range(num_queries))\n",
    "    ax.set_xticklabels([f'T{i}' for i in range(num_tokens)], fontsize=8)\n",
    "    ax.set_yticklabels([f'Q{i}' for i in range(num_queries)])\n",
    "    plt.colorbar(im, ax=ax, label='Euclidean Distance')\n",
    "    \n",
    "    ax = axes[0, 2]\n",
    "    im = ax.imshow(spatial_bias[2], cmap='RdBu_r')  # Show head 2\n",
    "    ax.set_title(f'Spatial Bias (Head 2, scale={scale[2]:.1f})')\n",
    "    ax.set_xlabel('Token')\n",
    "    ax.set_ylabel('Query')\n",
    "    ax.set_xticks(range(num_tokens))\n",
    "    ax.set_yticks(range(num_queries))\n",
    "    ax.set_xticklabels([f'T{i}' for i in range(num_tokens)], fontsize=8)\n",
    "    ax.set_yticklabels([f'Q{i}' for i in range(num_queries)])\n",
    "    plt.colorbar(im, ax=ax, label='Bias Value')\n",
    "    \n",
    "    # Bottom row: Attention weights comparison\n",
    "    ax = axes[1, 0]\n",
    "    im = ax.imshow(attn_weights_baseline[2], cmap='hot', vmin=0, vmax=0.3)\n",
    "    ax.set_title('Baseline Attention (Head 2)\\nNo Spatial Bias')\n",
    "    ax.set_xlabel('Token')\n",
    "    ax.set_ylabel('Query')\n",
    "    ax.set_xticks(range(num_tokens))\n",
    "    ax.set_yticks(range(num_queries))\n",
    "    ax.set_xticklabels([f'T{i}' for i in range(num_tokens)], fontsize=8)\n",
    "    ax.set_yticklabels([f'Q{i}' for i in range(num_queries)])\n",
    "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    im = ax.imshow(attn_weights_with_bias[2], cmap='hot', vmin=0, vmax=0.3)\n",
    "    ax.set_title('With Spatial Bias (Head 2)\\nCloser tokens get more attention')\n",
    "    ax.set_xlabel('Token')\n",
    "    ax.set_ylabel('Query')\n",
    "    ax.set_xticks(range(num_tokens))\n",
    "    ax.set_yticks(range(num_queries))\n",
    "    ax.set_xticklabels([f'T{i}' for i in range(num_tokens)], fontsize=8)\n",
    "    ax.set_yticklabels([f'Q{i}' for i in range(num_queries)])\n",
    "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "    \n",
    "    # Show difference\n",
    "    ax = axes[1, 2]\n",
    "    diff = attn_weights_with_bias[2] - attn_weights_baseline[2]\n",
    "    im = ax.imshow(diff, cmap='RdBu_r', vmin=-0.2, vmax=0.2)\n",
    "    ax.set_title('Difference (With Bias - Baseline)\\nBlue=decreased, Red=increased')\n",
    "    ax.set_xlabel('Token')\n",
    "    ax.set_ylabel('Query')\n",
    "    ax.set_xticks(range(num_tokens))\n",
    "    ax.set_yticks(range(num_queries))\n",
    "    ax.set_xticklabels([f'T{i}' for i in range(num_tokens)], fontsize=8)\n",
    "    ax.set_yticklabels([f'Q{i}' for i in range(num_queries)])\n",
    "    plt.colorbar(im, ax=ax, label='Weight Difference')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return query_coords, token_coords, dist\n",
    "\n",
    "query_coords, token_coords, dist = demonstrate_spatial_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observation\n",
    "\n",
    "With spatial bias:\n",
    "- Queries attend **more strongly** to nearby tokens (red in difference plot)\n",
    "- Queries attend **less** to distant tokens (blue in difference plot)\n",
    "- This is an **inductive bias** that helps the model learn faster\n",
    "\n",
    "The bias is **learnable per head**, so some heads can focus locally while others attend globally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Full Forward Pass <a name=\"5-full-forward-pass\"></a>\n",
    "\n",
    "Now let's trace through the complete forward pass of the SFC encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSFCEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified SFC encoder for demonstration.\n",
    "    Shows the key components without full model complexity.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        hidden_size: int = 128,\n",
    "        patch_size: int = 4,\n",
    "        group_size: int = 4,\n",
    "        num_heads: int = 4,\n",
    "        # Ablation options\n",
    "        unified_coords: bool = False,\n",
    "        spatial_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.patch_size = patch_size\n",
    "        self.group_size = group_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.unified_coords = unified_coords\n",
    "        self.spatial_bias = spatial_bias\n",
    "        \n",
    "        # Coordinate embeddings\n",
    "        self.token_coord_embed = LearnableFourierMLP(hidden_size, n_bands=8)\n",
    "        \n",
    "        if unified_coords:\n",
    "            # Option A: Share the same embedder\n",
    "            self.query_coord_embed = self.token_coord_embed\n",
    "        else:\n",
    "            # Baseline: Separate embedder\n",
    "            self.query_coord_embed = FourierFeatures(2, hidden_size, n_bands=8)\n",
    "        \n",
    "        # Token projection: (group_size * (in_channels + hidden_size)) -> hidden_size\n",
    "        per_point_dim = in_channels + hidden_size  # pixel values + coord embedding\n",
    "        self.token_proj = nn.Linear(group_size * per_point_dim, hidden_size)\n",
    "        self.token_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Cross-attention\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Option B: Spatial bias parameters\n",
    "        if spatial_bias:\n",
    "            self.spatial_scale = nn.Parameter(torch.ones(num_heads))\n",
    "            self.spatial_offset = nn.Parameter(torch.zeros(num_heads))\n",
    "    \n",
    "    def tokenize(self, x: torch.Tensor, cond_mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Convert sparse pixels to SFC tokens.\n",
    "        \n",
    "        Args:\n",
    "            x: (B, C, H, W) input image\n",
    "            cond_mask: (B, 1, H, W) binary mask of observed pixels\n",
    "        \n",
    "        Returns:\n",
    "            tokens: (B, T, D) token embeddings\n",
    "            token_mask: (B, T) boolean mask (True = real token)\n",
    "            token_coords: (B, T, 2) mean coordinate per token\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Get SFC ordering\n",
    "        order_flat, coords_flat, _ = precompute_sfc_order(H, \"hilbert\")\n",
    "        order_flat = order_flat.to(device)\n",
    "        coords_flat = coords_flat.to(device)\n",
    "        \n",
    "        # Flatten and reorder by SFC\n",
    "        x_flat = x.permute(0, 2, 3, 1).reshape(B, H*W, C)  # (B, HW, C)\n",
    "        m_flat = cond_mask[:, 0].reshape(B, H*W) > 0.5     # (B, HW)\n",
    "        \n",
    "        x_ord = x_flat[:, order_flat, :]  # (B, HW, C)\n",
    "        m_ord = m_flat[:, order_flat]     # (B, HW)\n",
    "        coords_ord = coords_flat[order_flat, :]  # (HW, 2)\n",
    "        \n",
    "        # Process each batch item\n",
    "        tokens_list, masks_list, coords_list = [], [], []\n",
    "        \n",
    "        for b in range(B):\n",
    "            # Select observed pixels\n",
    "            idx = torch.nonzero(m_ord[b], as_tuple=False).squeeze(1)\n",
    "            if idx.numel() == 0:\n",
    "                # No observed pixels - create dummy token\n",
    "                tokens_list.append(torch.zeros(1, self.hidden_size, device=device))\n",
    "                masks_list.append(torch.zeros(1, dtype=torch.bool, device=device))\n",
    "                coords_list.append(torch.zeros(1, 2, device=device))\n",
    "                continue\n",
    "            \n",
    "            vals = x_ord[b, idx, :]  # (K, C)\n",
    "            xy = coords_ord[idx, :].to(device)   # (K, 2)\n",
    "            \n",
    "            # Add coordinate embedding\n",
    "            coord_embed = self.token_coord_embed(xy)  # (K, D)\n",
    "            feats = torch.cat([vals, coord_embed], dim=-1)  # (K, C+D)\n",
    "            \n",
    "            K = feats.shape[0]\n",
    "            \n",
    "            # Pad to multiple of group_size\n",
    "            total = int(np.ceil(K / self.group_size) * self.group_size)\n",
    "            pad = total - K\n",
    "            if pad > 0:\n",
    "                feats = torch.cat([feats, feats.new_zeros(pad, feats.shape[-1])], dim=0)\n",
    "                xy_padded = torch.cat([xy, xy[-1:].expand(pad, -1)], dim=0)\n",
    "            else:\n",
    "                xy_padded = xy\n",
    "            \n",
    "            # Group and flatten\n",
    "            T = total // self.group_size\n",
    "            feats = feats.view(T, self.group_size, -1).reshape(T, -1)  # (T, g*(C+D))\n",
    "            \n",
    "            # Compute mean coordinate per token\n",
    "            tcoords = xy_padded.view(T, self.group_size, 2).mean(dim=1)  # (T, 2)\n",
    "            \n",
    "            # Project to hidden size\n",
    "            tok = self.token_proj(feats)  # (T, D)\n",
    "            tok = self.token_norm(tok)\n",
    "            \n",
    "            # Create mask\n",
    "            real_T = int(np.ceil(K / self.group_size))\n",
    "            tmask = torch.zeros(T, dtype=torch.bool, device=device)\n",
    "            tmask[:real_T] = True\n",
    "            \n",
    "            tokens_list.append(tok)\n",
    "            masks_list.append(tmask)\n",
    "            coords_list.append(tcoords)\n",
    "        \n",
    "        # Pad across batch\n",
    "        T_max = max(t.shape[0] for t in tokens_list)\n",
    "        tokens = torch.zeros(B, T_max, self.hidden_size, device=device)\n",
    "        token_mask = torch.zeros(B, T_max, dtype=torch.bool, device=device)\n",
    "        token_coords = torch.zeros(B, T_max, 2, device=device)\n",
    "        \n",
    "        for b in range(B):\n",
    "            T = tokens_list[b].shape[0]\n",
    "            tokens[b, :T] = tokens_list[b]\n",
    "            token_mask[b, :T] = masks_list[b]\n",
    "            token_coords[b, :T] = coords_list[b]\n",
    "        \n",
    "        return tokens, token_mask, token_coords\n",
    "    \n",
    "    def cross_attention(self, queries, tokens, token_mask, query_coords, token_coords):\n",
    "        \"\"\"\n",
    "        Cross-attention: queries attend to tokens.\n",
    "        \n",
    "        Args:\n",
    "            queries: (B, L, D) patch queries\n",
    "            tokens: (B, T, D) SFC tokens\n",
    "            token_mask: (B, T) boolean mask\n",
    "            query_coords: (B, L, 2) query coordinates\n",
    "            token_coords: (B, T, 2) token coordinates\n",
    "        \"\"\"\n",
    "        B, L, D = queries.shape\n",
    "        T = tokens.shape[1]\n",
    "        \n",
    "        # Project Q, K, V\n",
    "        Q = self.q_proj(queries).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(tokens).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(tokens).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Q, K, V: (B, heads, seq, head_dim)\n",
    "        \n",
    "        # Attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)  # (B, heads, L, T)\n",
    "        \n",
    "        # Option B: Add spatial bias\n",
    "        if self.spatial_bias:\n",
    "            dist = torch.cdist(query_coords.float(), token_coords.float(), p=2)  # (B, L, T)\n",
    "            scale = self.spatial_scale.view(1, -1, 1, 1)  # (1, heads, 1, 1)\n",
    "            offset = self.spatial_offset.view(1, -1, 1, 1)\n",
    "            spatial_bias = -scale * dist.unsqueeze(1) + offset  # (B, heads, L, T)\n",
    "            attn_scores = attn_scores + spatial_bias\n",
    "        \n",
    "        # Apply mask\n",
    "        if token_mask is not None:\n",
    "            mask = ~token_mask.unsqueeze(1).unsqueeze(2)  # (B, 1, 1, T)\n",
    "            attn_scores = attn_scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        # Softmax and apply to values\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        out = torch.matmul(attn_weights, V)  # (B, heads, L, head_dim)\n",
    "        out = out.transpose(1, 2).reshape(B, L, D)  # (B, L, D)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        return out, attn_weights\n",
    "    \n",
    "    def forward(self, x, cond_mask):\n",
    "        \"\"\"\n",
    "        Full forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: (B, C, H, W) input image\n",
    "            cond_mask: (B, 1, H, W) binary mask\n",
    "        \n",
    "        Returns:\n",
    "            patch_embeddings: (B, L, D) where L = (H/patch_size) * (W/patch_size)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Step 1: Tokenize sparse pixels\n",
    "        tokens, token_mask, token_coords = self.tokenize(x, cond_mask)\n",
    "        \n",
    "        # Step 2: Create patch queries\n",
    "        ph, pw = H // self.patch_size, W // self.patch_size\n",
    "        L = ph * pw\n",
    "        \n",
    "        # Patch center coordinates\n",
    "        ys = (torch.arange(ph, device=device).float() + 0.5) * self.patch_size\n",
    "        xs = (torch.arange(pw, device=device).float() + 0.5) * self.patch_size\n",
    "        ys = (ys / H) * 2 - 1  # Normalize to [-1, 1]\n",
    "        xs = (xs / W) * 2 - 1\n",
    "        yy, xx = torch.meshgrid(ys, xs, indexing='ij')\n",
    "        query_coords = torch.stack([xx, yy], dim=-1).view(1, L, 2).expand(B, -1, -1)\n",
    "        \n",
    "        # Embed query coordinates\n",
    "        queries = self.query_coord_embed(query_coords.reshape(-1, 2)).view(B, L, -1)\n",
    "        \n",
    "        # Step 3: Cross-attention\n",
    "        output, attn_weights = self.cross_attention(\n",
    "            queries, tokens, token_mask, query_coords, token_coords\n",
    "        )\n",
    "        \n",
    "        return output, {\n",
    "            'tokens': tokens,\n",
    "            'token_mask': token_mask,\n",
    "            'token_coords': token_coords,\n",
    "            'query_coords': query_coords,\n",
    "            'attn_weights': attn_weights,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_full_forward_pass():\n",
    "    \"\"\"\n",
    "    Walk through the full forward pass with visualizations.\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    B, C, H, W = 1, 3, 16, 16\n",
    "    patch_size = 4\n",
    "    group_size = 4\n",
    "    hidden_size = 64\n",
    "    sparsity = 0.3\n",
    "    \n",
    "    # Create model\n",
    "    model = SimpleSFCEncoder(\n",
    "        in_channels=C,\n",
    "        hidden_size=hidden_size,\n",
    "        patch_size=patch_size,\n",
    "        group_size=group_size,\n",
    "        unified_coords=True,   # Option A\n",
    "        spatial_bias=True,     # Option B\n",
    "    )\n",
    "    \n",
    "    # Create input\n",
    "    torch.manual_seed(123)\n",
    "    x = torch.randn(B, C, H, W)\n",
    "    cond_mask = (torch.rand(B, 1, H, W) < sparsity).float()\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output, info = model(x, cond_mask)\n",
    "    \n",
    "    # Extract info\n",
    "    tokens = info['tokens']\n",
    "    token_mask = info['token_mask']\n",
    "    token_coords = info['token_coords']\n",
    "    query_coords = info['query_coords']\n",
    "    attn_weights = info['attn_weights']\n",
    "    \n",
    "    num_observed = cond_mask.sum().int().item()\n",
    "    num_tokens = token_mask[0].sum().item()\n",
    "    num_patches = output.shape[1]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"FULL FORWARD PASS WALKTHROUGH\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nInput:\")\n",
    "    print(f\"  Image: {tuple(x.shape)} = (B, C, H, W)\")\n",
    "    print(f\"  Mask: {tuple(cond_mask.shape)} with {num_observed} observed pixels ({num_observed/H/W*100:.1f}%)\")\n",
    "    print(f\"\\nTokenization:\")\n",
    "    print(f\"  Group size: {group_size}\")\n",
    "    print(f\"  Tokens: {tuple(tokens.shape)} with {int(num_tokens)} real tokens\")\n",
    "    print(f\"  Compression: {H*W} pixels → {int(num_tokens)} tokens\")\n",
    "    print(f\"\\nQueries:\")\n",
    "    print(f\"  Patch size: {patch_size}x{patch_size}\")\n",
    "    print(f\"  Patches: {H//patch_size}x{W//patch_size} = {num_patches}\")\n",
    "    print(f\"\\nCross-Attention:\")\n",
    "    print(f\"  Q: ({num_patches} queries) × K: ({int(num_tokens)} tokens)\")\n",
    "    print(f\"  Attention weights: {tuple(attn_weights.shape)} = (B, heads, L, T)\")\n",
    "    print(f\"\\nOutput:\")\n",
    "    print(f\"  Patch embeddings: {tuple(output.shape)} = (B, L, D)\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Input image and mask\n",
    "    ax = axes[0, 0]\n",
    "    # Show first channel of input\n",
    "    ax.imshow(x[0, 0].numpy(), cmap='gray')\n",
    "    ax.set_title('Input Image (Channel 0)')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.imshow(cond_mask[0, 0].numpy(), cmap='gray')\n",
    "    ax.set_title(f'Conditioning Mask\\n{num_observed} pixels observed')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Token positions\n",
    "    ax = axes[0, 2]\n",
    "    tc = token_coords[0, :int(num_tokens)].numpy()\n",
    "    qc = query_coords[0].numpy()\n",
    "    \n",
    "    ax.scatter(qc[:, 0], qc[:, 1], c='red', s=200, marker='s', \n",
    "              label=f'Query patches ({num_patches})', alpha=0.7, edgecolors='black')\n",
    "    ax.scatter(tc[:, 0], tc[:, 1], c='blue', s=100, marker='o',\n",
    "              label=f'Tokens ({int(num_tokens)})', alpha=0.7, edgecolors='black')\n",
    "    ax.set_xlim(-1.2, 1.2)\n",
    "    ax.set_ylim(-1.2, 1.2)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title('Spatial Layout')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Attention patterns for different heads\n",
    "    for head_idx, ax in enumerate(axes[1, :3]):\n",
    "        if head_idx < attn_weights.shape[1]:\n",
    "            attn = attn_weights[0, head_idx, :, :int(num_tokens)].numpy()\n",
    "            im = ax.imshow(attn, cmap='hot', aspect='auto')\n",
    "            ax.set_title(f'Attention (Head {head_idx})')\n",
    "            ax.set_xlabel('Token')\n",
    "            ax.set_ylabel('Query Patch')\n",
    "            plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, x, cond_mask, output, info\n",
    "\n",
    "model, x, cond_mask, output, info = demonstrate_full_forward_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Ablation Comparison <a name=\"6-ablation-comparison\"></a>\n",
    "\n",
    "Let's compare the 4 ablation configurations side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ablations():\n",
    "    \"\"\"\n",
    "    Compare attention patterns across the 4 ablation configurations.\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    B, C, H, W = 1, 3, 16, 16\n",
    "    patch_size = 4\n",
    "    \n",
    "    # Create input\n",
    "    torch.manual_seed(42)\n",
    "    x = torch.randn(B, C, H, W)\n",
    "    cond_mask = (torch.rand(B, 1, H, W) < 0.3).float()\n",
    "    \n",
    "    # Create 4 model variants\n",
    "    configs = [\n",
    "        ('Baseline', False, False),\n",
    "        ('Option A (Unified Coords)', True, False),\n",
    "        ('Option B (Spatial Bias)', False, True),\n",
    "        ('Option A+B (Both)', True, True),\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    for col, (name, unified, spatial) in enumerate(configs):\n",
    "        # Create model with specific config\n",
    "        model = SimpleSFCEncoder(\n",
    "            in_channels=C,\n",
    "            hidden_size=64,\n",
    "            patch_size=patch_size,\n",
    "            group_size=4,\n",
    "            unified_coords=unified,\n",
    "            spatial_bias=spatial,\n",
    "        )\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            output, info = model(x, cond_mask)\n",
    "        \n",
    "        attn = info['attn_weights'][0]  # (heads, L, T)\n",
    "        num_tokens = int(info['token_mask'][0].sum().item())\n",
    "        \n",
    "        # Top row: Head 0 attention\n",
    "        ax = axes[0, col]\n",
    "        im = ax.imshow(attn[0, :, :num_tokens].numpy(), cmap='hot', aspect='auto')\n",
    "        ax.set_title(f'{name}\\nHead 0')\n",
    "        ax.set_xlabel('Token')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel('Query')\n",
    "        \n",
    "        # Bottom row: Mean attention across heads\n",
    "        ax = axes[1, col]\n",
    "        mean_attn = attn[:, :, :num_tokens].mean(dim=0).numpy()\n",
    "        im = ax.imshow(mean_attn, cmap='hot', aspect='auto')\n",
    "        ax.set_title('Mean across heads')\n",
    "        ax.set_xlabel('Token')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel('Query')\n",
    "    \n",
    "    plt.suptitle('Attention Patterns Comparison\\n(Same input, different configurations)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nAblation Summary:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Configuration':<35} {'Unified Coords':<15} {'Spatial Bias':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    for name, unified, spatial in configs:\n",
    "        print(f\"{name:<35} {str(unified):<15} {str(spatial):<15}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nKey Differences:\")\n",
    "    print(\"- Baseline: Attention must learn spatial relevance from scratch\")\n",
    "    print(\"- Option A: Tokens and queries use same coordinate representation\")\n",
    "    print(\"- Option B: Closer tokens automatically get higher attention\")\n",
    "    print(\"- Option A+B: Both benefits combined\")\n",
    "\n",
    "compare_ablations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Architecture Flow\n",
    "\n",
    "```\n",
    "Input Image (B, C, H, W) + Sparse Mask (B, 1, H, W)\n",
    "                    │\n",
    "                    ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                   SFC TOKENIZER                              │\n",
    "│  1. Order pixels by Hilbert/Z-order curve                   │\n",
    "│  2. Select observed pixels (mask=1)                         │\n",
    "│  3. Add coordinate embeddings (Option A: unified)           │\n",
    "│  4. Group consecutive g pixels → flatten → project          │\n",
    "│  Output: tokens (B,T,D), token_coords (B,T,2)               │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                    │\n",
    "                    ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                   CROSS-ATTENTION                            │\n",
    "│  Queries: patch center embeddings (B,L,D)                   │\n",
    "│  Keys/Values: SFC tokens (B,T,D)                            │\n",
    "│  Option B: Add spatial bias based on coordinate distance    │\n",
    "│  Output: patch embeddings (B,L,D)                           │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                    │\n",
    "                    ▼\n",
    "              DiT Encoder Blocks\n",
    "                    │\n",
    "                    ▼\n",
    "              Heavy Decoder (Super-Resolution)\n",
    "                    │\n",
    "                    ▼\n",
    "              Output: Denoised/Generated Image\n",
    "```\n",
    "\n",
    "### Ablation Options\n",
    "\n",
    "| Option | What it does | Why it helps |\n",
    "|--------|--------------|---------------|\n",
    "| **A: Unified Coords** | Share coord embedder between tokens & queries | Same representation space, easier alignment |\n",
    "| **B: Spatial Bias** | Add distance-based attention bias | Closer tokens automatically more relevant |\n",
    "\n",
    "### Running Experiments\n",
    "\n",
    "```bash\n",
    "# Baseline\n",
    "python train_cifar10.py --encoder_type sfc\n",
    "\n",
    "# Option A only\n",
    "python train_cifar10.py --encoder_type sfc --sfc_unified_coords\n",
    "\n",
    "# Option B only  \n",
    "python train_cifar10.py --encoder_type sfc --sfc_spatial_bias\n",
    "\n",
    "# Both options\n",
    "python train_cifar10.py --encoder_type sfc --sfc_unified_coords --sfc_spatial_bias\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
