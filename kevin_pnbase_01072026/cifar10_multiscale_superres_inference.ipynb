{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# CIFAR-10 Multi-Scale NerfEmbedder: Super-Resolution Inference\n",
    "\n",
    "This notebook demonstrates the **Multi-Scale NerfEmbedder** architecture which decouples:\n",
    "- **Encoder patch size** → controls global coherence (more tokens = better)\n",
    "- **NerfEmbedder dense_samples** → controls super-resolution quality (more positions = smoother)\n",
    "\n",
    "## Key Innovation\n",
    "\n",
    "| Architecture | patch_size | Encoder Tokens | NF Positions | Global | Super-Res |\n",
    "|--------------|------------|----------------|--------------|--------|----------|\n",
    "| Original (ps=2) | 2 | 256 | 4 | ✅ Excellent | ❌ Poor |\n",
    "| Original (ps=8) | 8 | 16 | 64 | ⚠️ Limited | ✅ Good |\n",
    "| **Multi-Scale** | 2 | 256 | 256 | ✅ Excellent | ✅ Excellent |\n",
    "\n",
    "The multi-scale architecture uses **dense position sampling** (e.g., 16×16=256) regardless of patch size,\n",
    "plus **multi-octave Fourier features** for robust interpolation at arbitrary scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": "## Setup\n\n- Requires GPU\n- Assumes model was trained using `train_cifar10_multiscale.py`\n- Update `CKPT_PATH` to point to your trained checkpoint\n\n### Training command:\n```bash\npython train_cifar10_multiscale.py --patch_size 2 --dense_samples 16 --max_steps 100000\n```\n\n### Model config (must match training - ~10M params):\n| Parameter | Value | Why |\n|-----------|-------|-----|\n| patch_size | 2 | 16×16=256 encoder tokens (excellent global coherence) |\n| dense_samples | 16 | 16×16=256 NF positions (excellent super-resolution) |\n| hidden_size | 256 | Reduced for ~10M params |\n| decoder_hidden_size | 32 | Reduced decoder capacity |\n| num_encoder_blocks | 6 | Fewer blocks |\n| num_groups | 4 | Fewer attention heads |\n| nerf_fusion | concat | Multi-scale feature fusion |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9j0k1l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to PixNerd folder where src/ is located\n",
    "import os\n",
    "import sys\n",
    "\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "print(f\"Starting directory: {NOTEBOOK_DIR}\")\n",
    "\n",
    "# Navigate to PixNerd folder (where src/ lives)\n",
    "PIXNERD_DIR = os.path.join(NOTEBOOK_DIR, \"PixNerd\")\n",
    "if os.path.exists(PIXNERD_DIR):\n",
    "    os.chdir(PIXNERD_DIR)\n",
    "    print(f\"Changed to: {os.getcwd()}\")\n",
    "elif os.path.basename(NOTEBOOK_DIR) == \"PixNerd\":\n",
    "    print(f\"Already in PixNerd directory: {NOTEBOOK_DIR}\")\n",
    "else:\n",
    "    parent = os.path.dirname(NOTEBOOK_DIR)\n",
    "    pixnerd_in_parent = os.path.join(parent, \"PixNerd\")\n",
    "    if os.path.exists(pixnerd_in_parent):\n",
    "        os.chdir(pixnerd_in_parent)\n",
    "        print(f\"Changed to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(f\"WARNING: Could not find PixNerd folder. Current dir: {NOTEBOOK_DIR}\")\n",
    "\n",
    "if os.path.exists(\"src\"):\n",
    "    print(\"Found src/ directory\")\n",
    "else:\n",
    "    print(\"ERROR: src/ directory not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3n4o5p6",
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport math\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Paths\nPIXNERD_ROOT = Path(os.getcwd())\n\n# ============================================================\n# CHECKPOINT PATH - UPDATE THIS TO YOUR TRAINED MODEL\n# ============================================================\nCKPT_PATH = PIXNERD_ROOT / \"workdirs\" / \"exp_cifar10_multiscale_nerf\" / \"checkpoints\" / \"last.ckpt\"\n# ============================================================\n\nOUTPUT_DIR = PIXNERD_ROOT / \"outputs\" / \"cifar10_multiscale_superres\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif DEVICE != \"cuda\":\n    print(\"WARNING: Running on CPU will be very slow\")\n\n# CIFAR-10 class names\nCIFAR10_CLASSES = [\n    'airplane', 'automobile', 'bird', 'cat', 'deer',\n    'dog', 'frog', 'horse', 'ship', 'truck'\n]\n\n# ============================================================\n# MODEL CONFIG - Must match train_cifar10_multiscale.py (~10M params)\n# ============================================================\nNUM_CLASSES = 10\nBASE_RES = 32  # CIFAR-10 native resolution\n\n# KEY INNOVATION: These are INDEPENDENT!\nPATCH_SIZE = 2           # Controls encoder tokens: 32/2 = 16x16 = 256 tokens\nDENSE_SAMPLES = 16       # Controls NF positions: 16x16 = 256 positions\n\n# Smaller model config (~10M parameters)\nHIDDEN_SIZE = 256\nDECODER_HIDDEN_SIZE = 32\nNUM_ENCODER_BLOCKS = 6\nNUM_DECODER_BLOCKS = 2\nNUM_GROUPS = 4\nNERF_FUSION = \"concat\"   # Options: \"concat\", \"add\", \"attention\"\n# ============================================================\n\nprint(f\"PixNerd root: {PIXNERD_ROOT}\")\nprint(f\"Checkpoint path: {CKPT_PATH}\")\nprint(f\"Checkpoint exists: {CKPT_PATH.exists()}\")\nif not CKPT_PATH.exists():\n    print(f\"\\n⚠️  CHECKPOINT NOT FOUND!\")\n    print(f\"   Please train a model first using:\")\n    print(f\"   python train_cifar10_multiscale.py --patch_size 2 --dense_samples 16\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Device: {DEVICE}\")\nprint()\nprint(\"=\" * 60)\nprint(\"MULTI-SCALE ARCHITECTURE (~10M params)\")\nprint(\"patch_size and dense_samples are INDEPENDENT!\")\nprint(\"=\" * 60)\nprint(f\"  Encoder patch_size: {PATCH_SIZE}\")\nprint(f\"    → Encoder tokens: {BASE_RES//PATCH_SIZE}x{BASE_RES//PATCH_SIZE} = {(BASE_RES//PATCH_SIZE)**2}\")\nprint(f\"    → Controls: Global coherence\")\nprint()\nprint(f\"  NerfEmbedder dense_samples: {DENSE_SAMPLES}\")\nprint(f\"    → Position samples: {DENSE_SAMPLES}x{DENSE_SAMPLES} = {DENSE_SAMPLES**2}\")\nprint(f\"    → Controls: Super-resolution quality\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "Using `PixNerDiTMultiScale` with the Multi-Scale NerfEmbedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PixNerd components - NOTE: Using MULTISCALE model!\n",
    "from src.models.autoencoder.pixel import PixelAE\n",
    "from src.models.conditioner.class_label import LabelConditioner\n",
    "from src.models.transformer.pixnerd_c2i_multiscale import PixNerDiTMultiScale  # Multi-scale version!\n",
    "from src.diffusion.flow_matching.scheduling import LinearScheduler\n",
    "from src.diffusion.flow_matching.sampling import EulerSampler, ode_step_fn\n",
    "from src.diffusion.base.guidance import simple_guidance_fn\n",
    "from src.diffusion.flow_matching.training import FlowMatchingTrainer\n",
    "from src.callbacks.simple_ema import SimpleEMA\n",
    "from src.lightning_model import LightningModel\n",
    "from src.models.autoencoder.base import fp2uint8\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(\"Using: PixNerDiTMultiScale (Multi-Scale NerfEmbedder)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5z6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing model components...\")\n",
    "\n",
    "main_scheduler = LinearScheduler()\n",
    "\n",
    "vae = PixelAE(scale=1.0)\n",
    "\n",
    "conditioner = LabelConditioner(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Multi-Scale NerfEmbedder model - KEY DIFFERENCE!\n",
    "denoiser = PixNerDiTMultiScale(\n",
    "    in_channels=3,\n",
    "    patch_size=PATCH_SIZE,           # Controls encoder tokens\n",
    "    dense_samples=DENSE_SAMPLES,     # Controls NF positions (INDEPENDENT!)\n",
    "    num_groups=NUM_GROUPS,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    decoder_hidden_size=DECODER_HIDDEN_SIZE,\n",
    "    num_encoder_blocks=NUM_ENCODER_BLOCKS,\n",
    "    num_decoder_blocks=NUM_DECODER_BLOCKS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    nerf_fusion=NERF_FUSION,\n",
    ")\n",
    "\n",
    "# Sampler with CFG\n",
    "sampler = EulerSampler(\n",
    "    num_steps=50,\n",
    "    guidance=2.0,\n",
    "    guidance_interval_min=0.0,\n",
    "    guidance_interval_max=1.0,\n",
    "    scheduler=main_scheduler,\n",
    "    w_scheduler=LinearScheduler(),\n",
    "    guidance_fn=simple_guidance_fn,\n",
    "    step_fn=ode_step_fn,\n",
    ")\n",
    "\n",
    "# Trainer stub for checkpoint loading\n",
    "trainer_stub = FlowMatchingTrainer(\n",
    "    scheduler=main_scheduler,\n",
    "    lognorm_t=True,\n",
    "    timeshift=1.0,\n",
    ")\n",
    "\n",
    "ema_tracker = SimpleEMA(decay=0.9999)\n",
    "\n",
    "model = LightningModel(\n",
    "    vae=vae,\n",
    "    conditioner=conditioner,\n",
    "    denoiser=denoiser,\n",
    "    diffusion_trainer=trainer_stub,\n",
    "    diffusion_sampler=sampler,\n",
    "    ema_tracker=ema_tracker,\n",
    "    optimizer=None,\n",
    "    lr_scheduler=None,\n",
    "    eval_original_model=False,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "print(f\"Model initialized and moved to {DEVICE}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print()\n",
    "print(f\"Architecture: PixNerDiTMultiScale\")\n",
    "print(f\"  - patch_size={PATCH_SIZE} → {(BASE_RES//PATCH_SIZE)**2} encoder tokens\")\n",
    "print(f\"  - dense_samples={DENSE_SAMPLES} → {DENSE_SAMPLES**2} NF positions\")\n",
    "print(f\"  - nerf_fusion={NERF_FUSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3h4i5j6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading checkpoint from: {CKPT_PATH}\")\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\", weights_only=False)\n",
    "missing, unexpected = model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "print(f\"Missing keys: {len(missing)} | Unexpected keys: {len(unexpected)}\")\n",
    "if missing:\n",
    "    print(f\"  Missing: {missing[:5]}...\" if len(missing) > 5 else f\"  Missing: {missing}\")\n",
    "if unexpected:\n",
    "    print(f\"  Unexpected: {unexpected[:5]}...\" if len(unexpected) > 5 else f\"  Unexpected: {unexpected}\")\n",
    "print(\"Checkpoint loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k7l8m9n0",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1p2q3r4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_decoder_scale(scale: float):\n",
    "    \"\"\"Set NF decoder patch scaling for super-resolution.\"\"\"\n",
    "    for net in [model.denoiser, getattr(model, \"ema_denoiser\", None)]:\n",
    "        if net is None:\n",
    "            continue\n",
    "        net.decoder_patch_scaling_h = scale\n",
    "        net.decoder_patch_scaling_w = scale\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_class_conditional(\n",
    "    class_labels: list,\n",
    "    height: int = 32,\n",
    "    width: int = 32,\n",
    "    seed: int = 42,\n",
    "    num_steps: int = 50,\n",
    "    guidance: float = 2.0,\n",
    "    base_res: int = BASE_RES,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate class-conditional images.\n",
    "    \n",
    "    Args:\n",
    "        class_labels: List of class indices (0-9) or names\n",
    "        height: Output height (32 for native, 128 for 4x super-res)\n",
    "        width: Output width\n",
    "        seed: Random seed\n",
    "        num_steps: ODE solver steps\n",
    "        guidance: CFG guidance scale\n",
    "        base_res: Training resolution\n",
    "    \n",
    "    Returns:\n",
    "        Generated images as uint8 tensor\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Convert class names to indices if needed\n",
    "    labels = []\n",
    "    for label in class_labels:\n",
    "        if isinstance(label, str):\n",
    "            label = CIFAR10_CLASSES.index(label.lower())\n",
    "        labels.append(label)\n",
    "    \n",
    "    batch_size = len(labels)\n",
    "    \n",
    "    # Set decoder scale for super-resolution\n",
    "    if height == base_res and width == base_res:\n",
    "        set_decoder_scale(1.0)\n",
    "        print(f\"Generating at native {base_res}x{base_res}\")\n",
    "    else:\n",
    "        scale_h = height / float(base_res)\n",
    "        scale_w = width / float(base_res)\n",
    "        assert scale_h == scale_w, \"Only square scaling supported\"\n",
    "        set_decoder_scale(scale_h)\n",
    "        print(f\"Generating at {height}x{width} ({scale_h:.0f}x super-resolution)\")\n",
    "    \n",
    "    # Configure sampler\n",
    "    model.diffusion_sampler.guidance = guidance\n",
    "    model.diffusion_sampler.num_steps = num_steps\n",
    "    \n",
    "    # Generate noise\n",
    "    noise = torch.randn(batch_size, 3, height, width, device=DEVICE)\n",
    "    \n",
    "    # Get condition and uncondition\n",
    "    condition, uncondition = model.conditioner(labels)\n",
    "    condition = condition.to(DEVICE)\n",
    "    uncondition = uncondition.to(DEVICE)\n",
    "    \n",
    "    # Sample\n",
    "    samples = model.diffusion_sampler(\n",
    "        model.ema_denoiser,\n",
    "        noise,\n",
    "        condition,\n",
    "        uncondition,\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    images = model.vae.decode(samples)\n",
    "    images = torch.clamp(images, -1.0, 1.0)\n",
    "    images_uint8 = fp2uint8(images)\n",
    "    \n",
    "    return images_uint8.cpu()\n",
    "\n",
    "\n",
    "def show_images(images_uint8, labels=None, title=\"\", cols=None):\n",
    "    \"\"\"Display a batch of images with labels.\"\"\"\n",
    "    if isinstance(images_uint8, torch.Tensor):\n",
    "        imgs_np = images_uint8.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    else:\n",
    "        imgs_np = np.transpose(images_uint8, (0, 2, 3, 1))\n",
    "    \n",
    "    n = len(imgs_np)\n",
    "    if cols is None:\n",
    "        cols = min(n, 5)\n",
    "    rows = math.ceil(n / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs_np)):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if labels is not None:\n",
    "            label = labels[i]\n",
    "            if isinstance(label, int):\n",
    "                label = CIFAR10_CLASSES[label]\n",
    "            ax.set_title(label)\n",
    "    \n",
    "    for ax in axes[n:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_grid(images_uint8, filename, labels=None, cols=None):\n",
    "    \"\"\"Save images as a grid.\"\"\"\n",
    "    if isinstance(images_uint8, torch.Tensor):\n",
    "        imgs_np = images_uint8.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    else:\n",
    "        imgs_np = np.transpose(images_uint8, (0, 2, 3, 1))\n",
    "    \n",
    "    imgs = [Image.fromarray(img) for img in imgs_np]\n",
    "    \n",
    "    n = len(imgs)\n",
    "    if cols is None:\n",
    "        cols = min(n, 5)\n",
    "    rows = math.ceil(n / cols)\n",
    "    \n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", (cols * w, rows * h))\n",
    "    for idx, img in enumerate(imgs):\n",
    "        r, c = divmod(idx, cols)\n",
    "        grid.paste(img, (c * w, r * h))\n",
    "    \n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    grid.save(out_path)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5t6u7v8",
   "metadata": {},
   "source": [
    "## Generate at Native Resolution (32x32)\n",
    "\n",
    "Generate one sample per class at the native CIFAR-10 resolution.\n",
    "\n",
    "With `patch_size=2`, we have **256 encoder tokens** for excellent global coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9x0y1z2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Generating all 10 classes at 32x32\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate one image per class\n",
    "all_classes = list(range(10))\n",
    "\n",
    "images_32 = sample_class_conditional(\n",
    "    class_labels=all_classes,\n",
    "    height=32,\n",
    "    width=32,\n",
    "    seed=42,\n",
    "    num_steps=50,\n",
    "    guidance=2.0,\n",
    ")\n",
    "\n",
    "print(f\"Output shape: {images_32.shape}\")\n",
    "show_images(images_32, labels=CIFAR10_CLASSES, title=\"Multi-Scale NerfEmbedder: CIFAR-10 at 32x32\")\n",
    "save_grid(images_32, \"multiscale_cifar10_32x32_all_classes.png\", cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## Generate at 4x Super-Resolution (128x128)\n",
    "\n",
    "The Multi-Scale NerfEmbedder enables smooth super-resolution because:\n",
    "1. **Dense position sampling** (16×16=256) regardless of patch size\n",
    "2. **Multi-octave Fourier features** for robust interpolation\n",
    "3. **Cross-scale communication** (if using attention fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8g9h0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Generating all 10 classes at 128x128 (4x super-resolution)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "images_128 = sample_class_conditional(\n",
    "    class_labels=all_classes,\n",
    "    height=128,\n",
    "    width=128,\n",
    "    seed=42,  # Same seed for comparison\n",
    "    num_steps=50,\n",
    "    guidance=2.0,\n",
    ")\n",
    "\n",
    "print(f\"Output shape: {images_128.shape}\")\n",
    "show_images(images_128, labels=CIFAR10_CLASSES, title=\"Multi-Scale NerfEmbedder: CIFAR-10 at 128x128 (4x Super-Res)\")\n",
    "save_grid(images_128, \"multiscale_cifar10_128x128_4x_superres.png\", cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1j2k3l4",
   "metadata": {},
   "source": [
    "## Side-by-Side Comparison: 32x32 vs 128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m5n6o7p8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare specific classes\n",
    "comparison_classes = ['cat', 'dog', 'airplane', 'ship']\n",
    "comparison_labels = [CIFAR10_CLASSES.index(c) for c in comparison_classes]\n",
    "\n",
    "print(\"Comparing 32x32 vs Bilinear 128x128 vs Multi-Scale NF 128x128\")\n",
    "\n",
    "# Generate at 32x32\n",
    "imgs_32 = sample_class_conditional(\n",
    "    class_labels=comparison_labels,\n",
    "    height=32, width=32,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "# Generate at 128x128 (4x)\n",
    "imgs_128 = sample_class_conditional(\n",
    "    class_labels=comparison_labels,\n",
    "    height=128, width=128,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "# Bilinear upscale 32->128\n",
    "imgs_32_upscaled = F.interpolate(\n",
    "    imgs_32.float() / 255.0,\n",
    "    size=(128, 128),\n",
    "    mode='bilinear',\n",
    "    align_corners=False,\n",
    ")\n",
    "imgs_32_upscaled = (imgs_32_upscaled * 255).to(torch.uint8)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(len(comparison_classes), 3, figsize=(12, 4 * len(comparison_classes)))\n",
    "\n",
    "for i, class_name in enumerate(comparison_classes):\n",
    "    # 32x32 native\n",
    "    axes[i, 0].imshow(imgs_32[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 0].set_title(f\"{class_name} - 32x32 Native\")\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Bilinear upscale\n",
    "    axes[i, 1].imshow(imgs_32_upscaled[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 1].set_title(f\"{class_name} - Bilinear 128x128\")\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Multi-scale NF super-res\n",
    "    axes[i, 2].imshow(imgs_128[i].permute(1, 2, 0).numpy())\n",
    "    axes[i, 2].set_title(f\"{class_name} - Multi-Scale NF 128x128\")\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.suptitle(\"32x32 Native vs Bilinear Upscale vs Multi-Scale NF Super-Resolution\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q9r0s1t2",
   "metadata": {},
   "source": [
    "## Try Different Super-Resolution Scales\n",
    "\n",
    "The Multi-Scale NerfEmbedder should provide smooth interpolation at any scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scales: 1x, 2x, 4x, 8x\n",
    "print(\"=\" * 50)\n",
    "print(\"Comparing different super-resolution scales\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "scales = [\n",
    "    (32, \"1x (Native)\"),\n",
    "    (64, \"2x\"),\n",
    "    (128, \"4x\"),\n",
    "    (256, \"8x\"),\n",
    "]\n",
    "\n",
    "class_label = 'airplane'\n",
    "seed = 42\n",
    "\n",
    "scale_images = []\n",
    "scale_titles = []\n",
    "\n",
    "for resolution, scale_name in scales:\n",
    "    print(f\"Generating at {resolution}x{resolution}...\")\n",
    "    img = sample_class_conditional(\n",
    "        class_labels=[class_label],\n",
    "        height=resolution, width=resolution,\n",
    "        seed=seed,\n",
    "    )\n",
    "    scale_images.append(img[0])\n",
    "    scale_titles.append(f\"{resolution}x{resolution} ({scale_name})\")\n",
    "\n",
    "# Display at same visual size\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, (img, title) in enumerate(zip(scale_images, scale_titles)):\n",
    "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"{class_label.capitalize()} at Different Resolutions (Multi-Scale NerfEmbedder)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra_scales",
   "metadata": {},
   "source": [
    "## Extreme Super-Resolution Test\n",
    "\n",
    "Let's test the limits of the multi-scale architecture with even higher resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try extreme super-resolution: 16x (512x512)\n",
    "print(\"=\" * 50)\n",
    "print(\"Extreme Super-Resolution Test: 16x (512x512)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    img_512 = sample_class_conditional(\n",
    "        class_labels=['cat'],\n",
    "        height=512, width=512,\n",
    "        seed=42,\n",
    "        num_steps=50,\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_512[0].permute(1, 2, 0).numpy())\n",
    "    plt.title(\"Cat at 512x512 (16x Super-Resolution)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    save_grid(img_512, \"multiscale_cat_512x512_16x_superres.png\")\n",
    "except Exception as e:\n",
    "    print(f\"512x512 generation failed: {e}\")\n",
    "    print(\"This might require more GPU memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5h6i7j8",
   "metadata": {},
   "source": [
    "## Guidance Scale Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9l0m1n2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different guidance scales\n",
    "print(\"=\" * 50)\n",
    "print(\"Comparing different CFG guidance scales at 128x128\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "guidance_scales = [1.0, 1.5, 2.0, 3.0, 5.0]\n",
    "class_label = 'horse'\n",
    "seed = 42\n",
    "\n",
    "guidance_images = []\n",
    "for g in guidance_scales:\n",
    "    print(f\"Guidance = {g}...\")\n",
    "    img = sample_class_conditional(\n",
    "        class_labels=[class_label],\n",
    "        height=128, width=128,\n",
    "        seed=seed,\n",
    "        guidance=g,\n",
    "    )\n",
    "    guidance_images.append(img[0])\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, (img, g) in enumerate(zip(guidance_images, guidance_scales)):\n",
    "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[i].set_title(f\"guidance={g}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"{class_label.capitalize()} with Different CFG Scales (Multi-Scale NerfEmbedder)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variety_samples",
   "metadata": {},
   "source": [
    "## Generate Variety of Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variety_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 5 different samples for each class at 128x128\n",
    "print(\"=\" * 50)\n",
    "print(\"Generating 5 samples per class at 128x128\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "selected_classes = ['cat', 'dog', 'airplane']\n",
    "seeds = [0, 42, 123, 456, 789]\n",
    "\n",
    "for class_name in selected_classes:\n",
    "    print(f\"\\nGenerating {class_name}s...\")\n",
    "    class_images = []\n",
    "    for seed in seeds:\n",
    "        img = sample_class_conditional(\n",
    "            class_labels=[class_name],\n",
    "            height=128, width=128,\n",
    "            seed=seed,\n",
    "        )\n",
    "        class_images.append(img)\n",
    "    \n",
    "    class_images = torch.cat(class_images, dim=0)\n",
    "    show_images(class_images, labels=[class_name] * 5, title=f\"5 {class_name.capitalize()}s at 128x128\")\n",
    "    save_grid(class_images, f\"multiscale_{class_name}s_128x128.png\", cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o3p4q5r6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the **Multi-Scale NerfEmbedder** architecture:\n",
    "\n",
    "### Key Innovation: Decoupled Parameters\n",
    "\n",
    "| Parameter | Controls | Value | Effect |\n",
    "|-----------|----------|-------|--------|\n",
    "| `patch_size` | Encoder tokens | 2 | 256 tokens → excellent global coherence |\n",
    "| `dense_samples` | NF positions | 16 | 256 positions → smooth super-resolution |\n",
    "\n",
    "**These are now INDEPENDENT!**\n",
    "\n",
    "### Multi-Scale Position Encoding\n",
    "- **Global octave**: Low frequencies → coarse image structure\n",
    "- **Region octave**: Mid frequencies → regional patterns\n",
    "- **Local octave**: High frequencies → fine details\n",
    "- **Scale fusion**: Combines information across scales\n",
    "\n",
    "### Comparison with Original Architecture\n",
    "\n",
    "| Architecture | Encoder Tokens | NF Positions | Global | Super-Res |\n",
    "|--------------|----------------|--------------|--------|----------|\n",
    "| Original (ps=2) | 256 | 4 | ✅ | ❌ |\n",
    "| Original (ps=8) | 16 | 64 | ⚠️ | ✅ |\n",
    "| **Multi-Scale** | **256** | **256** | **✅** | **✅** |\n",
    "\n",
    "### Training Command\n",
    "```bash\n",
    "python train_cifar10_multiscale.py \\\n",
    "    --patch_size 2 \\\n",
    "    --dense_samples 16 \\\n",
    "    --nerf_fusion concat \\\n",
    "    --max_steps 100000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7t8u9v0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")\n",
    "print(f\"\\nAll outputs saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}