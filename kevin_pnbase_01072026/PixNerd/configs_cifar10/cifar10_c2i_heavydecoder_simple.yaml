# CIFAR-10 Class-Conditional Training with Heavy Decoder (Simple - No REPA)
# Supports 4x super-resolution (32x32 -> 128x128)
# This config doesn't require DINOv2 encoder
# lightning.pytorch==2.4.0

seed_everything: true
tags:
  exp: &exp cifar10_c2i_heavydecoder_simple

trainer:
  default_root_dir: ./workdirs
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: bf16-mixed
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: pixnerd_cifar10
      name: *exp
  num_sanity_val_steps: 0
  max_steps: 100000
  val_check_interval: 10000
  check_val_every_n_epoch: null
  log_every_n_steps: 50
  deterministic: null
  inference_mode: true
  use_distributed_sampler: false
  callbacks:
    - class_path: src.callbacks.model_checkpoint.CheckpointHook
      init_args:
        every_n_train_steps: 5000
        save_top_k: -1
        save_last: true
    - class_path: src.callbacks.save_images.SaveImagesHook
      init_args:
        save_dir: val
        save_compressed: true

model:
  vae:
    class_path: src.models.autoencoder.pixel.PixelAE
    init_args:
      scale: 1.0

  denoiser:
    class_path: src.models.transformer.pixnerd_c2i_heavydecoder.PixNerDiT
    init_args:
      in_channels: 3
      patch_size: 2  # 32/2 = 16 patches per side
      num_groups: 8
      hidden_size: &hidden_dim 512
      decoder_hidden_size: 64
      num_encoder_blocks: 12
      num_decoder_blocks: 2
      num_classes: &num_classes 10

  conditioner:
    class_path: src.models.conditioner.class_label.LabelConditioner
    init_args:
      num_classes: *num_classes

  diffusion_trainer:
    class_path: src.diffusion.flow_matching.training.FlowMatchingTrainer
    init_args:
      lognorm_t: true
      timeshift: 1.0
      scheduler: &scheduler src.diffusion.flow_matching.scheduling.LinearScheduler

  diffusion_sampler:
    class_path: src.diffusion.flow_matching.sampling.EulerSampler
    init_args:
      num_steps: 50
      guidance: 2.0
      guidance_interval_min: 0.0
      guidance_interval_max: 1.0
      scheduler: *scheduler
      w_scheduler: src.diffusion.flow_matching.scheduling.LinearScheduler
      guidance_fn: src.diffusion.base.guidance.simple_guidance_fn
      step_fn: src.diffusion.flow_matching.sampling.ode_step_fn

  ema_tracker:
    class_path: src.callbacks.simple_ema.SimpleEMA
    init_args:
      decay: 0.9999

  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 1e-4
      weight_decay: 0.0

data:
  train_dataset:
    class_path: src.data.dataset.cifar10.PixCIFAR10
    init_args:
      root: ./data
      train: true
      random_flip: true
      download: true

  eval_dataset:
    class_path: src.data.dataset.cifar10.CIFAR10RandomNDataset
    init_args:
      num_classes: 10
      max_num_instances: 10000
      latent_shape:
        - 3
        - 32
        - 32

  pred_dataset:
    class_path: src.data.dataset.cifar10.CIFAR10RandomNDataset
    init_args:
      num_classes: *num_classes
      max_num_instances: 10000
      latent_shape:
        - 3
        - 32
        - 32

  train_batch_size: 128
  train_num_workers: 4
  pred_batch_size: 64
  pred_num_workers: 2
